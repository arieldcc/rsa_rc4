Luyao Chen, Yong Tang, Jingwen Xia, Siyang Chen, Chengyu Zheng, Hai Lin, Wenyong Wang,
Multi-MEC collaboration for VR video transmission: Architecture and cache algorithm design,
Computer Networks,
Volume 234,
2023,
109864,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109864.
(https://www.sciencedirect.com/science/article/pii/S1389128623003092)
Abstract: Virtual Reality (VR) is becoming an important use case for 5G wireless networks, and Mobile Edge Computing (MEC) servers are being explored as a way to reduce VR video latency. To address the limited cache space of a single MEC server, this paper proposes dividing geographically close MEC servers into collaborative domains. A new VR video transmission architecture is designed after analyzing video compression mechanisms. The proposed architecture splits the VR video into equal-sized tile files to simplify the cache problem and improve caching efficiency. However, the large number of different tile files presents a challenge. To address this, the paper proposes an optimized k-shortest paths (OKSP) algorithm with a time complexity of O((K⋅M+N)⋅M⋅logN), where K is the number of tiles that all M MEC servers can cache in the collaboration domain and N is the number of tile files. For extremely large-scale data cases, a greedy-based approximation algorithm is also proposed. The numerical results demonstrate the OKSP algorithm’s excellent performance in solving large-scale data, outperforming other caching algorithms in experiments.
Keywords: Virtual reality; Mobile edge computing; Multimedia communication; Distributed algorithms

Hyunsuk Nam, Roman Lysecky,
Security-aware multi-objective optimization of distributed reconfigurable embedded systems,
Journal of Parallel and Distributed Computing,
Volume 133,
2019,
Pages 377-390,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.02.015.
(https://www.sciencedirect.com/science/article/pii/S0743731518300923)
Abstract: Distributed embedded systems are increasingly prevalent in numerous applications, and with pervasive network access within these systems, security is also a critical design concern. We present a modeling and optimization framework for distributed embedded systems incorporating heterogeneous resources, including single core processor, asymmetric multicore processors, and FPGAs. A dataflow-based modeling framework for streaming applications integrates models for computational latency, cryptographic security levels, communication latency, and power consumption. We utilize a multi-objective genetic optimization algorithm to optimize security subject to constraints for energy consumption and minimum security level. The presented methodology is evaluated using a video-based object detection and tracking application considering several distributed heterogeneous embedded systems architectures.
Keywords: Distributed embedded systems; Security; Co-design modeling; Dynamic optimization; Design space exploration; Penalty functions

Yinzhi Guo, Xiaolong Xu, Fu Xiao,
MADRLOM: A Computation offloading mechanism for software-defined cloud-edge computing power network,
Computer Networks,
Volume 245,
2024,
110352,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110352.
(https://www.sciencedirect.com/science/article/pii/S1389128624001841)
Abstract: Cloud-edge computing power network often exhibits complex and heterogeneous structures, posing several challenges to computation offloading that significantly impact network performance and the efficient utilization of computation resources. In this paper, we propose a cloud-edge computing power network architecture that efficiently integrates cloud and edge computing resources into a single network system using software-defined networking technology to support upper-layer business applications. In this context, existing computation offloading methods struggle with issues like users' personalized requirements for latency and energy consumption, as well as the inability to adapt to dynamically complex environments. To overcome these challenges, we introduce a computation offloading mechanism, MADRLOM, focusing on the network's heterogeneity and modeling the computation offloading problem at the cloud-edge end. We formalize the offloading problem as a Markov process and employ a multi-agent deep reinforcement learning algorithm based on priority experience replay sampling to address the planning problem of computation offloading, allowing user equipment to continuously optimize offloading strategies in response to environmental changes and achieve rational resource allocation. Through dynamic offloading strategies, computation tasks can be intelligently allocated to appropriate computing nodes, thereby achieving optimal resource utilization. We utilized the Mininet simulation platform to construct the experimental environment for the software-defined cloud-edge computing power network and compared it with several representative computation offloading strategies. The experimental results demonstrate that the MADRLOM significantly reduces the total system overhead in the software-defined cloud-edge computing network and shows excellent adaptability to environmental changes.
Keywords: Computation offloading; Deep reinforcement learning; Software-defined networking; Computing power network

Youmei Song, Tianyu Wo, Renyu Yang, Qi Shen, Jie Xu,
Joint optimization of cache placement and request routing in unreliable networks,
Journal of Parallel and Distributed Computing,
Volume 157,
2021,
Pages 168-178,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.06.006.
(https://www.sciencedirect.com/science/article/pii/S0743731521001350)
Abstract: Edge caching is a prevailing media delivery technology where data is hosted at the edge nodes with computing and storage capability in close proximity to users, in order to expand the backhaul network capacity and enhance users' quality of experience (QoE). The existing work in this area often neglects the fact that large-scale distributed cache networks are not particularly reliable and many edge nodes are prone to failure. In this paper we investigate and develop a novel, cooperative caching mechanism for content placement and request routing. We aim to minimize the content access delay and achieve the optimization in polynomial time, taking into account failures in an unreliable network environment with limited edge storage and bandwidth. We introduce two optimization algorithms: 1) a primal-dual algorithm that is based on the Lagrangian dual decomposition and subgradient method, and 2) a greedy-based approximation algorithm with a proven approximation ratio. Numerical results show that the proposed algorithms outperform other comparative approaches in synthetic and real network environments, and the approximation algorithm is particularly suitable for networking scenarios with sparse node connectivity and resources in short supply.
Keywords: Edge caching; Content placement; Request scheduling; Network unreliability; Optimization

Mohammad Yahya Akhlaqi, Zurina Binti Mohd Hanapi,
Task offloading paradigm in mobile edge computing-current issues, adopted approaches, and future directions,
Journal of Network and Computer Applications,
Volume 212,
2023,
103568,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103568.
(https://www.sciencedirect.com/science/article/pii/S1084804522002090)
Abstract: Many enterprise companies migrate their services and applications to the cloud to benefit from cloud computing advantages. Meanwhile, the rapidly increasing number of connected devices with the massive amount of generated data that use cloud services leads to high workload, congestion, and delay bottleneck in the centralized cloud architecture. Consequently, Mobile Edge Computing (MEC) is introduced as a new paradigm to expand cloud capabilities near the end devices. In addition, new technologies such as the Internet of Things (IoT), Autonomous Vehicles (AV), 5G, and Augmented Reality (AR) bring new demands and opportunities that MEC can make possible. Offloading delay-sensitive and computationally intensive tasks to nearby MEC nodes is an effective way that still is the most common open problem in MEC. The offloading problem in MEC has been widely studied in areas such as Vehicular Edge Computing (VEC), IoT, Radio Access Networks (RAN), and 5G but independently. Due to the high diversity of research areas, targeted issues, and adopted algorithms and techniques, finding the right research path in task offloading is highly demanding. To fill this gap, a comprehensive survey is conducted using the mixed-method systematic literature review involving qualitative and quantitative data from the studied papers. For each journal paper, the detailed information of the work area, targeted issue, formulation technique, optimization approach, adopted algorithms, evaluation techniques, performance matrices, dataset, utilized tools, and framework are extracted and analyzed using manual and automatic coding. Major offloading-related issues in MEC are investigated, and the taxonomy of journal papers based on adopted approaches is presented. For further future exploration, we suggest the potential areas of research, the contribution of the algorithms and technique, and the research direction in MEC. This review will give a quick and overall view of MEC's latest issues and solutions.
Keywords: Computational offloading,; Mobile edge computing; Collaborative edge computing; Multi-access edge computing; Task offloading

Sujie Shao, Lili Su, Qinghang Zhang, Shuang Wu, Shaoyong Guo, Feng Qi,
Multi task dynamic edge–end computing collaboration for urban Internet of Vehicles,
Computer Networks,
Volume 227,
2023,
109690,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109690.
(https://www.sciencedirect.com/science/article/pii/S1389128623001354)
Abstract: As the future trend, more and more vehicles access to the Internet of Vehicles, which means that a huge number of tasks of the vehicle terminals need to be transformed and completed on the network. Edge computing makes the tasks executed on the edge nodes near the terminal, but some vehicle terminals are at a relatively idle state and these additional computing resources are not utilized, causing great waste of resources. What is more, it is hard to highly and comprehensively satisfy the high real-time requirements of some tasks. In order to execute these tasks efficiently, we propose a dynamic edge–end computing collaboration architecture for urban IoV. In this architecture, edge nodes and vehicle terminals can cooperate with each other, which means tasks can be allocated more dynamically and flexibly. We evaluate the completion of the task by considering task latency and overhead, task transmission model, task priority, as well as edge node and vehicle terminal’s capacity when defining task comprehensive utility. Then weformulate the task allocation as an optimization problem and propose an improved quantum particle swarm optimization algorithm to solve the problem. Simulation results show that the proposed strategy have better task allocation utility than other strategies, which can effectively solve the multi task allocation problem.
Keywords: Internet of Vehicles; Edge–end collaboration; Multi task allocation; Quantum particle swarm optimization

Zejun Li, Hao Wu, Yunlong Lu,
Coalition based utility and efficiency optimization for multi-task federated learning in Internet of Vehicles,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 196-208,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.014.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003302)
Abstract: With the emergence of the sixth generation (6G) communication technologies, massive infrastructures will be densely deployed and the number of data will be generated exponentially. Massive data collected by vehicles can be used to train a machine learning model, which is an effective way of implementing intelligent services in the Internet of Vehicles (IoV). However, centralized training leads to communication congestion and user privacy leakage. To address these problems and improve efficiency, federated learning has been proposed. Most current studies on federated learning consider all users performing only a single task at a certain time, which results in low network utility and long task execution time. Because some vehicles may have similar data, we propose a multi-task federated learning model where vehicles simultaneously execute tasks. To jointly maximize the network utility and efficiency of the multi-task federated learning in the 6G-enabled IoV, we design a task-driven vehicular coalition algorithm considering vehicle selection and wireless resource allocation. Moreover, we derive a convex objective function from network utility function and loss function with constraints through a series of mathematical analyses. Finally, we verify that our proposed algorithm with low complexity can improve the utility and efficiency of the multi-task federated learning in the 6G-enabled IoV.
Keywords: Multi-task federated learning; Network utility; Resource optimization; Coalition; 6G-enabled IoV

Bin Xiang, Jocelyne Elias, Fabio Martignon, Elisabetta Di Nitto,
Resource calendaring for Mobile Edge Computing: Centralized and decentralized optimization approaches,
Computer Networks,
Volume 199,
2021,
108426,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108426.
(https://www.sciencedirect.com/science/article/pii/S138912862100390X)
Abstract: Mobile Edge Computing (MEC) is a key technology for the deployment of next generation (5G and beyond) mobile networks. The computational power it provides at the edge could allow providers to fulfill the requirements of use cases in need of ultra-low latency, high bandwidth, as well as real-time access to the radio network. However, this potential needs to be carefully administered as the edge is certainly limited in terms of computation capability, as opposed to the cloud which holds the promise of a virtually infinite power. MEC nodes, though, could still try to exploit not only their local capacity, but also the one that the neighbor MEC nodes could offer. Considering that the 5G scenario assumes an ultra-dense distribution of MEC nodes, this possibility could be feasible, provided that we find an effective way to carefully allocate the resources available at each edge node. In this paper, we provide an optimization framework that considers several key aspects of the resource allocation problem with cooperating MEC nodes. We carefully model and optimize the allocation of resources, including computation and storage capacity available in network nodes as well as link capacity. Specifically, our proposed model jointly optimizes (1) the user requests admission decision (2) their scheduling, also called calendaring (3) and routing as well as (4) the decision of which nodes will serve such user requests and (5) the amount of processing and storage capacity reserved on the chosen nodes. Both an exact optimization model and an effective heuristic, based on sequential fixing, are provided. Furthermore, we propose a distributed approach for our problem, based on the Alternating Direction Method of Multipliers (ADMM), so that resource allocation decisions can be made in a distributed fashion by edge nodes with limited overhead. We perform an extensive numerical analysis in several real-size network scenarios, using real positions for radio access points of a mobile operator in the Milan area. Results demonstrate that the heuristic performs close to the optimum in all considered network scenarios, while exhibiting a low computing time. This provides an evidence that our proposal is an effective framework for optimizing resource allocation in next-generation mobile networks.
Keywords: Edge computing; Network slicing; Network design; Calendaring; Joint optimization; ADMM

Zheng Yao, Huaiyu Wu, Yang Chen,
Multi-objective cooperative computation offloading for MEC in UAVs hybrid networks via integrated optimization framework,
Computer Communications,
Volume 202,
2023,
Pages 124-134,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.01.006.
(https://www.sciencedirect.com/science/article/pii/S0140366423000063)
Abstract: With the continuous expansion of the application field of the Internet of Things (IoT), mobile edge computing (MEC) is regarded as a promising technique to reduce the time-delay and energy-consumption of application. However, the conventional MEC infrastructure is lack of flexibility, and failed to meet the different requires of mobile device. UAVs have the distinct features of high scalability and mobility for communications, which can act as the complement of conventional MEC infrastructure. This paper investigates the issues of multi-objective cooperative computation offloading for MEC in UAVs hybrid Networks. The proposed UAVs hybrid MEC system enables edge-cloud and UAVs cooperation to address the flexible limitations of conventional MEC infrastructure and the efficient computation offloading of computation task. To support good-quality services in a cost-effective manner, we model the computation offloading problem as a multi-objective optimization process, and propose an intelligent computation offloading algorithms based on integrated optimization framework, including mixed integer transformation solving framework, improved multi-adaptive MOEA/D-DE(MOEA/D-MSDE) and Grey Relational Projection (GRP). Evaluation results show that the proposed algorithms outperform in solving multi-objective cooperative computation offloading problem in terms of service time-delay, energy-consumption and server-cost.
Keywords: Mobile edge computing; UAVs hybrid networks; Multi-objective cooperative computation offloading; Integrated optimization framework

Sifeng Zhu, Xiaohua Tian, Hao Chen, Hai Zhu, Rui Qiao,
Edge collaborative caching solution based on improved NSGA II algorithm in Internet of Vehicles,
Computer Networks,
Volume 244,
2024,
110307,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110307.
(https://www.sciencedirect.com/science/article/pii/S1389128624001397)
Abstract: With the rapid development of the Internet of Vehicles technology and the continuous advancement of intelligent transportation, vehicle network traffic is growing exponentially. Traditional caching solutions transmit content through the core network, causing significant latency. A vehicle-cloud collaborative edge caching network architecture was proposed to address this type of problem. Using this architecture, both the on-board terminals and cloud/edge servers could provide computing services. A communication model, caching model, latency model, energy consumption model, load model and multi-objective optimization problem model were designed. The edge collaborative caching solution based on the improved NSGA II algorithm was proposed. By caching some services to edge nodes and nearby vehicles, reducing content access latency and improving resource utilization. The results indicate that the caching solution outperforms the comparison scheme in terms of the comprehensive costs of latency, energy consumption, and load balancing in simulation experiments. It can meet the caching requirements of low latency and low energy consumption for in-vehicle applications, laying a solid foundation for achieving more efficient and reliable connected vehicles and autonomous driving.
Keywords: Edge cache; Internet of Vehicles; NSGA II algorithm; Optimization; Vehicle-road cloud collaboration

Zhanwei Yu, Tao Deng, Yi Zhao, Di Yuan,
Multi-cell content caching: Optimization for cost and information freshness,
Computer Networks,
Volume 247,
2024,
110420,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110420.
(https://www.sciencedirect.com/science/article/pii/S1389128624002524)
Abstract: In multi-access edge computing (MEC) systems, there are multiple local cache servers caching contents to satisfy the users’ requests, instead of letting the users download via the remote cloud server. In this paper, a multi-cell content scheduling problem (MCSP) in MEC systems is considered. Taking into account jointly the freshness of the cached contents and the traffic data costs, we study how to schedule content updates along time in a multi-cell setting. Different from single-cell scenarios, a user may have multiple candidate local cache servers, and thus the caching decisions in all cells must be jointly optimized. We first prove that MCSP is NP-hard, then we formulate MCSP using integer linear programming, by which the optimal scheduling can be obtained for small-scale instances. For problem solving of large scenarios, via a mathematical reformulation, we derive a scalable optimization algorithm based on repeated column generation. Our performance evaluation shows the effectiveness of the proposed algorithm in comparison to an off-the-shelf commercial solver and a popularity-based caching.
Keywords: Age of information; Caching; Multi-cell

Shahid Md. Asif Iqbal,  Asaduzzaman,
Cache-MAB: A reinforcement learning-based hybrid caching scheme in named data networks,
Future Generation Computer Systems,
Volume 147,
2023,
Pages 163-178,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.032.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001723)
Abstract: The presence of content stores in routers enables in-network caching in Named Data Networks (NDN) to improve customer experience, especially when content distribution is considered. Typically, homogeneous caching is assumed in the network where each router operates its cache according to the installed policy. Numerous in-network caching policies that vary in admission logic and caching attributes have been urged. Hence, the policies vary in performance (efficiency) according to unbounded network factors such as caching capacity, users’ request pattern, content popularity distribution, and application type (amongst other contexts). A single scheme may not effectively realize all the network characteristics and vandalize the efficient use of precious cache spaces. Therefore, a new strategy capable of dynamically selecting the optimal caching policy under varying network contexts is inevitable. In this direction, we propose a reinforcement learning (RL)-based hybrid caching strategy, namely Cache-MAB, where the routers work in a distributed manner and learn to pick the most suitable policy for caching a content. The set of policies is formed by picking the policies that assume similar caching attributes for caching decisions. The proposed strategy decouples the caching policy selection from the admission logic used by the selected policy and models the suitable policy selection as a Multi-armed Bandit (MAB) problem. The hybrid strategy equips each router with a diverse set of caching policies and an RL agent that utilizes a reinforcement algorithm to solve the MAB problem and, hence, select the optimal policy in different cases. The optimal policy is the one that maximizes the local performance, for instance, the cache hit rate. Simulation results confirm that the hybrid strategy effectively achieves the optimal policy’s performance, having a performance gap of less than 1%, by adapting to different network scenarios.
Keywords: Optimal policy; Reinforcement learning; Multi-armed bandit; Local performance; Hybrid caching; Learning agent

Alberto del Rio, Javier Serrano, David Jimenez, Luis M. Contreras, Federico Alvarez,
Multisite gaming streaming optimization over virtualized 5G environment using Deep Reinforcement Learning techniques,
Computer Networks,
Volume 244,
2024,
110334,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110334.
(https://www.sciencedirect.com/science/article/pii/S138912862400166X)
Abstract: The massive growth of live streaming, especially gaming-focused content, has led to an overall increase in global bandwidth consumption. Certain services see their quality diminished at times of peak consumption, degrading the quality of the content. This trend generates new research related to optimizing image quality according to network and service conditions. In this work we present a gaming streaming use case optimization on a real multisite 5G environment. The paper outlines the virtualized workflow of the use case and provides a detailed description of the applications and resources deployed for the simulation. This simulation tests the optimization of the service based on the addition of Artificial Intelligence (AI) algorithms, assuring the delivery of content with good Quality of Experience (QoE) under different working conditions. The AI introduced is based on Deep Reinforcement Learning (DRL) algorithms that can adapt, in a flexible way, to the different conditions that the multimedia workflow could face. That is, adapt, through corrective actions, the streaming bitrate, in order to optimize the QoE of the content on a real-time multisite scenario. The results of this work demonstrate how we have been able to minimize content losses, as well as the fact of obtaining high audiovisual multimedia quality results with higher bitrates, compared to a service without an optimizer integrated in the system. In a multi-site environment, we have achieved an improvement of 20 percentage points in terms of blockiness efficiency and also 15 percentage points in block loss.
Keywords: Deep Reinforcement Learning; Multiaccess edge computing; Software-defined networks; 5G; Network function virtualization; Adaptive multimedia; A3C; Quality of Experience

Ehzaz Mustafa, Junaid Shuja, Faisal Rehman, Ahsan Riaz, Mohammed Maray, Muhammad Bilal, Muhammad Khurram Khan,
Deep Neural Networks meet computation offloading in mobile edge networks: Applications, taxonomy, and open issues,
Journal of Network and Computer Applications,
Volume 226,
2024,
103886,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103886.
(https://www.sciencedirect.com/science/article/pii/S1084804524000638)
Abstract: Mobile Edge Computing (MEC) is a modern paradigm that involves moving computing and storage resources closer to the network edge, reducing latency, and enabling innovative, delay-sensitive applications. Within MEC, computation offloading refers to the process of transferring computationally intensive tasks or processes from mobile devices to edge servers, optimizing the performance of mobile applications. Traditional numerical optimization methods for computation offloading often necessitate numerous iterations to attain optimal solutions. In this paper, we provide a tutorial on how Deep Neural Networks (DNNs) resolve the challenges of computation offloading. The article explores various applications of DNNs in computation offloading, encompassing channel estimation, caching, AR and VR applications, resource allocation, mode selection, unmanned aerial vehicles (UAVs), and vehicle management. We present a comprehensive taxonomy that categorizes these applications, and offer an overview of existing schemes, comparing their effectiveness. Additionally, we outline the open research issues that can be addressed through the application of DNNs in MEC offloading. We also highlight specific challenges related to DNN utilization in computation offloading. In conclusion, we affirm that DNNs are widely acknowledged as invaluable tools for optimizing computation offloading in MEC.
Keywords: Mobile edge computing; Computation offloading; Deep Neural Networks; Supervised learning; Unsupervised learning; Reinforcement learning

Anjula Mehto, Shashikala Tapaswi, K.K. Pattanaik,
Multi-objective particle swarm optimization based rendezvous point selection for the energy and delay efficient networked wireless sensor data acquisition,
Journal of Network and Computer Applications,
Volume 195,
2021,
103234,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103234.
(https://www.sciencedirect.com/science/article/pii/S1084804521002320)
Abstract: Traditionally, the non-rendezvous points transmit data towards Rendezvous Points (RPs), and Mobile Sink (MS) visits RPs to collect data. The existing body of research mitigates the problem of data acquisition latency, load on RPs, and energy consumption by regulating the number of RPs. Fewer RPs benefit the data acquisition latency, whereas increased RPs benefit the multi-hop forwarding and load on RPs. This paper takes up these issues and models as multi-objective optimization problem attempting to minimize data collection latency, data load among RPs, and the number of RPs. Particle Swarm Optimization (PSO) is the widely used meta-heuristic method to solve a multi-objective optimization problem with better convergence and minimum overhead. This paper introduces a Multi-Objective Particle Swarm Optimization based RPs Selection (MOPSO-RPS) method for energy and delay efficient data collection. MOPSO-RPS applies a new encoding scheme to generate variable dimension particles that represent each possible set of RPs. Additionally, a new inertia weight tuner is also referred to enhance the convergence speed of multi-objective PSO towards the optimal solution. However, it might happen that after updating the location and velocity in each iteration, the particles become invalid due to the violation of the search space boundary. Thus it adopts a valid particle generator to create valid particles. Moreover, an improved ant colony optimization is also applied to construct the trajectory of MS with fast convergence speed towards the optimal solution. Simulation results depict that the proposed MOPSO-RPS result in 18.61%, 21.11%, and 10.71% average improvement in energy consumption, data load among RPs, and data acquisition latency, respectively, for different number of sensor nodes when compared with the state-of-the-art methods.
Keywords: Wireless sensor networks; Rendezvous points; Mobile sink; Meta-heuristics; Multi-objective optimization; Particle swarm optimization

Ziyan Wu, Zhihui Lu, Patrick C.K. Hung, Shih-Chia Huang, Yu Tong, Zhenfang Wang,
QaMeC: A QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 17-28,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.09.032.
(https://www.sciencedirect.com/science/article/pii/S0167739X18317540)
Abstract: Deploying applications to a centralized cloud for service delivery is infeasible because of the excessive latency and bandwidth limitation of the Internet, such as transporting all IoVs data to big data processing service in a centralized cloud. Therefore, multi-clouds, especially multiple edge clouds is a rising trend for cloud service provision. However, heterogeneity of the cloud service, complex deployment requirements, and large problem space of multi-clouds deployment make how to deploy applications in the multi-clouds environment be a difficult and error-prone decision-making process. Due to these difficulties, current SLA-based solution lacks a unified model to represent functional and non-functional requirements of users. In this background, we propose a QoS-driven IoVs application optimizing deployment scheme in multimedia edge clouds (QaMeC). Our scheme builds a unified QoS model to shield off the inconsistency of QoS calculation. Moreover, we use NSGA-II algorithm to solve the multi-clouds application deployment problem. The implementation and experiments show that our QaMeC scheme can provide optimal and efficient service deployment solutions for a variety of applications with different QoS requirements in CDN multimedia edge clouds environment.
Keywords: IoVs (Internet of Vehicles); IoT(Internet of Things); Optimizing deployment; Cloud computing; Edge computing; Multi-clouds; QoS; CDN (Content Delivery Network, Content Distribution Network)

Xin He, Meixu Lin,
Reliable auxiliary communication of UAV via relay cache optimization,
Computer Communications,
Volume 186,
2022,
Pages 33-44,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.11.024.
(https://www.sciencedirect.com/science/article/pii/S014036642100459X)
Abstract: With the continuous reduction of drone costs and the miniaturization of equipment, many new applications have emerged in the civil and commercial fields. The remote sensing technology of Unmanned Aerial Vehicle (UAV) is used to manage and partition farmland accurately. Compared with the traditional field grid sampling method, UAV remote sensing technology can break through operating conditions, continuously collect data at low altitudes, monitor the area more flexibly, and significantly reduce labor and safety costs. Since UAVs can only provide data transmission services to users through wireless backhaul links established with ground base stations, the need to access the network is easy to lead to the disclosure of user privacy. The capacity of wireless backhaul links is limited, limiting the transmission rate of drones and reducing the user’s quality of service. Therefore, we apply edge caching technology to the assisted relay communication network to study the impact of caching technology on the performance of mobile relay systems. In particular, we propose the horizontal position design method in the buffer auxiliary relay single-user system, and the 3D position design method in the buffer auxiliary relay multi-user system. Position system, which can be reached by the maximum system average and the optimal speed, is designed. Besides, with the help of objective function conversion and classic derivation analysis method, the semi-closed expression of the optimal position of the UAV is obtained, and the intersection of the mobile relay end velocity and the user end velocity is used as the initial value. Meanwhile, the solution formula is substituted after continuous iteration so that the best advantage of local velocity can be obtained. In addition, mobile relay system is deployed to establish a two-hop wireless link to achieve reliable communication between Base Station (BS) and users security trust. The simulation experiment proves that compared with other methods, the method proposed in this paper has considerable performance improvement in power convergence, speed, trajectory path loss, and energy cost, which can provide higher-quality communication services for users in the system and better support for the broad application of drones.
Keywords: UAV control; Agricultural monitoring; Relay cache; Communication privacy; Security trust

Dhruv Gajaria, Tosiron Adegbija,
Evaluating the performance and energy of STT-RAM caches for real-world wearable workloads,
Future Generation Computer Systems,
Volume 136,
2022,
Pages 231-240,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.05.023.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001935)
Abstract: Wearable devices have grown exponentially in popularity in both consumer and industrial applications in recent years. Despite their stringent area and energy constraints, these devices are processing increasingly complex and data-rich workloads, necessitating innovative area- and energy-efficient computing solutions and architectures. This paper explores spin-transfer torque RAM (STT-RAM) as a candidate for designing area- and energy-efficient wearable processor caches. First, we analyze the memory footprints of 16 real-world wearable workloads and compare them to general-purpose benchmarks like SPEC 2017 and MiBench. Then, we analyze the wearable workloads’ memory characteristics to reveal that wearable workloads are highly read-intensive, making them less vulnerable than general-purpose workloads to the write latency/energy overheads inherent in STT-RAM caches. Our analysis also reveals that wearable workloads have low cache variability needs, and their cache blocks exhibit short and stable lifetimes. Against the background of this analysis, we explore the tradeoffs of STT-RAM cache architecture designs for the wearable workloads. Specifically, we explore a simple adaptable design that aims to optimize latency or energy, based on runtime needs, without introducing significant design or area overhead. Our analysis shows that STT-RAM caches offer much promise for energy- and area-efficient wearable computing, without introducing much performance overheads.
Keywords: Wearable computing; Spin-transfer torque RAM (STT-RAM); Low-power embedded systems; Adaptable hardware; Retention time; Internet of Things

S. Vimal, Manju Khari, Nilanjan Dey, Rubén González Crespo, Y. Harold Robinson,
Enhanced resource allocation in mobile edge computing using reinforcement learning based MOACO algorithm for IIOT,
Computer Communications,
Volume 151,
2020,
Pages 355-364,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.018.
(https://www.sciencedirect.com/science/article/pii/S0140366419319255)
Abstract: The Mobile networks deploy and offers a multiaspective approach for various resource allocation paradigms and the service based options in the computing segments with its implication in the Industrial Internet of Things (IIOT) and the virtual reality. The Mobile edge computing (MEC) paradigm runs the virtual source with the edge communication between data terminals and the execution in the core network with a high pressure load. The demand to meet all the customer requirements is a better way for planning the execution with the support of cognitive agent. The user data with its behavioral approach is clubbed together to fulfill the service type for IIOT. The swarm intelligence based and reinforcement learning techniques provide a neural caching for the memory within the task execution, the prediction provides the caching strategy and cache business that delay the execution. The factors affecting this delay are predicted with mobile edge computing resources and to assess the performance in the neighboring user equipment. The effectiveness builds a cognitive agent model to assess the resource allocation and the communication network is established to enhance the quality of service. The Reinforcement Learning techniques Multi Objective Ant Colony Optimization (MOACO) algorithms has been applied to deal with the accurate resource allocation between the end users in the way of creating the cost mapping tables creations and optimal allocation in MEC.
Keywords: Mobile edge computing; Industrial IOT; Reinforcement learning; Multi objective ant colony optimization; Resource allocation; Cognitive agent

Wajid Rafique, Abdelhakim Senhaji Hafid, Soumaya Cherkaoui,
SoftCaching: A framework for caching node selection and routing in Software-Defined Information Centric Internet of Things,
Computer Networks,
Volume 235,
2023,
109966,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109966.
(https://www.sciencedirect.com/science/article/pii/S1389128623004115)
Abstract: A considerable demand for Internet of Things (IoT) services is causing exponential growth in Internet traffic. Software-Defined Networking (SDN) and Information-Centric Networking (ICN) are complementary technologies for IoT services provisioning that reduce network traffic and service provisioning delays by caching content at intermediate nodes in the network. Existing research on Software-Defined Information-Centric Networking (SDICN) for IoT service provisioning suffers from two major challenges: (a) optimal caching node selection and (b) optimal path computation from content consumers to caching nodes based on end-to-end delay and link utilization constraints. We propose a SoftCaching framework that optimizes caching node selection for content publishers and computes the optimal routing path to caching nodes for content consumers. It computes optimal locations for caching nodes and finds optimal routing paths for IoT services to access cached content. SoftCaching analyzes network traffic on all nodes in the network and collects traffic metrics. It estimates the best possible caching nodes by using Waypoint Enforcement (WPE) and solving the traffic matrix using Singular Value Decomposition (SVD) and QR Factorization. WPE and SVD-based QR factorization provide optimal locations to cache the content for IoT services. Subsequently, SoftCaching uses a constraint-based shortest path algorithm to compute the optimal routing path from content consumers to caching nodes. We make sure that the caching node and the routing path selection follow delay and link utilization constraints. We perform extensive experiments for evaluation, and the results show that SoftCaching outperforms state-of-the-art caching schemes in terms of delay, link load, hop count, path stretch, and energy consumption metrics.
Keywords: SDN; ICN; IoT; Services provisioning; Caching; Routing path; Waypoint enforcement

Zewu Li, Chen Xu, Zhanpeng Zhang, Runze Wu,
Deep reinforcement learning based trajectory design and resource allocation for task-aware multi-UAV enabled MEC networks,
Computer Communications,
Volume 213,
2024,
Pages 88-98,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.11.006.
(https://www.sciencedirect.com/science/article/pii/S0140366423003973)
Abstract: Computing tasks in the air is an important form of mobile edge computing (MEC) to improve the quality of service and enhance network coverage. In this paper, we investigate a multi-UAV cooperative computing model and massive devices access scenario in a service area without infrastructure. There are various types of ground devices with different tasks. Moreover, we consider that the UAV executing tasks of devices need to cache the content that task required. Therefore, we propose a multi-UAV enabled MEC network based on task awareness where each UAV caches some programs to execute tasks offloaded from devices. To minimize completion time, a joint UAV trajectory design, access decision and resource allocation problem is formulated. To address this intractable mixed integer non-linear programming problem, a multi-agent trajectory design and resource allocation (MATR) algorithm is proposed, where the multi-agent deep deterministic policy gradient (MADDPG) is applied. Considering the complexity of high-dimensional continuous action space, we introduce the particle swarm optimization (PSO) algorithm to jointly optimize access decisions, and computation resource allocation to reduce action space. In addition, we discuss the impact of the size of UAV cache space and the location of ground devices on the completion time. Simulation results show that the MATR algorithm can significantly reduce the completion time compared to the baselines.
Keywords: UAV; Cooperative computing; Deep reinforcement learning; Trajectory design; Resource allocation

Yanming Fu, Xian Zhang, Xiaoqiong Qin, Qingwen Meng, Bocheng Huang,
Data collection of multi-player cooperative game based on edge computing in mobile crowd sensing,
Computer Networks,
Volume 222,
2023,
109551,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109551.
(https://www.sciencedirect.com/science/article/pii/S1389128622005850)
Abstract: As the number and type of mobile crowd sensing (MCS) data collection increases, more and more computation and processing are required, resulting in higher service cost and service delay, posing a huge challenge to traditional MCS. Currently, edge computing is being introduced to MCS to collect data to reduce service cost and service latency. In the offline mode, there are two issues that need to be addressed with the edge computing-based MCS data collection. First, for large-scale, multi-player, and multiple types of task data, edge servers are limited in computational resources and need to address issues such as task offloading and service cache scheduling. Second, in traditional MCS data collection, workers usually carry in a single type of task data, but it has now been proposed that workers need to carry multiple types of task data. To address the above problems, this paper proposes a joint optimization strategy for edge computing based on multi-player cooperative game and greedy differential evolution algorithm (MCG-GDE) to improve the service rate of edge servers and minimize the service cost and service latency in the data collection. We build a mathematical optimization problem for edge computing based on MCS data collection. The formulation of the optimization problem proves to be a NP-hard problem, so this optimization strategy constructs a task propagation scheme for multi-player cooperative games (MCG), where tasks carried by workers are reassigned to effectively reduce problem complexity and produce sub-optimal solutions to the mathematical model. Then, on the basis of the suboptimal solution, the optimal solution of the problem is obtained by the greedy differential evolution algorithm (GDE). Simulation results demonstrate that MCG-GDE outperforms other baseline strategies.
Keywords: Mobile crowd sensing; Data collection; Edge computing; Multi-player cooperative game; Greedy differential evolution algorithm

Fayez Alqahtani, Mohammed Al-Maitah, Osama Elshakankiry,
A proactive caching and offloading technique using machine learning for mobile edge computing users,
Computer Communications,
Volume 181,
2022,
Pages 224-235,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.10.017.
(https://www.sciencedirect.com/science/article/pii/S0140366421003959)
Abstract: The mobile edge computing (MEC) paradigm provides cloud and application services at the “edge” of user networks for providing ubiquitous access to resources. The heterogeneous services cause varying network traffic that sometimes increases delay. In edge-based services, concurrency in data distribution requires caching and offloading features. This article introduces a proactive caching technique with offloading (PCTO) ability by considering the need for parallel user services. The proposed method performs demand-aware offloading to meet the concurrent service dissemination requirements. Network-level caching and its forecast in concurrent service distribution are performed to reduce the response time. The offloading and caching processes are streamlined using deep recurrent learning for the failing service distribution intervals. In the learning process, the machine is trained for prior failures and for pursuing offloading instances. Based on the learning output, the caching level and offloading rate are determined for the queuing services. The performance of the proposed method is verified using the metrics service ratio, response failures, latency, offloading rate, and caching ratio.
Keywords: Data caching; Deep learning; Mobile edge computing; Service offloading

Zaib Ullah, Fadi Al-Turjman, Uzair Moatasim, Leonardo Mostarda, Roberto Gagliardi,
UAVs joint optimization problems and machine learning to improve the 5G and Beyond communication,
Computer Networks,
Volume 182,
2020,
107478,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107478.
(https://www.sciencedirect.com/science/article/pii/S1389128620311518)
Abstract: Recently, unmanned aerial vehicles (UAVs) have gained notable interest in various applications such as wireless coverage, aerial surveillance, precision agriculture, construction, power lines monitoring and blood delivery, etc. The UAVs implicit attributes e.g., rapid deployment, quick mobility, increase in flight duration, improvements in payload capacities, etc. , place it as an effective candidate for many applications in 5G and Beyond communications. The UAVs-assisted next-generation communications are determined to be highly influenced by various techniques and technologies like artificial intelligence (AI), machine learning (ML), deep reinforcement learning (DRL), mobile edge computing (MEC), and software-defined networks (SDN). In this article, we develop a review to investigate the UAVs joint optimization problems to enhance system efficiency. We classify the joint optimization problems based on the number of parameters used in proposed optimization problems. Moreover, we explore the impact of AI, ML, DRL, MEC, and SDN over UAVs joint optimization problems and present future research challenges and directions.
Keywords: 5G and B5G communication; UAVs; Artificial intelligence; Machine learning; Mobile edge computing; Software-defined networks; Internet of Things; mmWave communication; Smart city; Joint optimization

Jingyao Liu, Guangsheng Feng, Zhibo Zhang, Liying Zheng, Huiqiang Wang, Jyri Hämäläinen,
Joint mixed-timescale optimization of content caching and delivery policy in NOMA-based vehicular networks,
Computer Networks,
Volume 237,
2023,
110075,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110075.
(https://www.sciencedirect.com/science/article/pii/S1389128623005200)
Abstract: Recently, the development of Internet of Vehicles (IoV) and the increasing popularity of video applications have led to the fast-growing in-car video demand causing numerous challenges in wireless networks. Pre-caching and non-orthogonal multiple access (NOMA) have been regarded as two effective techniques to alleviate the mentioned challenge. In this paper, we propose a cache-aided cooperative transmission to maximize the quality of service (QoS) in the NOMA-based vehicular network. A QoS-oriented joint optimization problem is formulated, which incorporates power allocation, content caching, and delivery strategy. Considering, on the one hand, the slow update rate of cache content and, on the other hand, frequent handovers of vehicles between different transmitters, a mixed-timescale optimization is proposed where the serving cache is updated in a long-term phase, while content delivery and power allocation are optimized in a short-term phase. In the proposed approach, content caching is determined based on future user requests, vehicle tracking, and other delivery information. To make this possible, we leverage a substantial number of stochastic samples to approximate content caching in the long-term caching phase. Due to the NOMA-based transmission and integral variables, the setting leads to a Mixed Integer Non-Linear Programming (MINLP) problem, which is NP-hard. To solve this problem, an iterative method based on sample average approximation (SAA) and Successive Convex Approximation (SCA) is applied. Simulations demonstrate that the proposed algorithm can achieve better QoS than other recently proposed transmission schemes.
Keywords: Content caching; Power allocation; Non-orthogonal multiple access (NOMA); Delivery policy; Dense vehicular networks

Mohammed Laroui, Hatem Ibn Khedher, Hassine Moungla, Hossam Afifi,
Service Function Chains multi-resource orchestration in Virtual Mobile Edge Computing,
Computer Networks,
Volume 224,
2023,
109582,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109582.
(https://www.sciencedirect.com/science/article/pii/S1389128623000270)
Abstract: Next-Generation Networks (NGNs) provide efficient services to improve the overall network performance, especially in Internet of Things (IoT) networks. However, service placement requires a significant cost in terms of multi-resource chaining guarantees. Therefore, the balance between improving network performance and optimizing resources orchestration is still a fundamental issue to assure the benefits of NGNs with optimal resources’ utilization. In this paper, we present new approaches for Service Function Chain (SFC) orchestration in Virtual Mobile Edge Computing (VMEC) environments, where IoT devices are used as on-demand virtual edge servers. We propose Optimal SFC Placement in VMEC over AI-IoT (OSPV) algorithm to select the optimal virtual mobile edge (VME) according to heterogeneous computing resources constraints. Moreover, to deal with OSPV complexity issues in dense IoT networks, we propose an Efficient SFC Placement in VMEC over AI-IoT (ESPV) algorithm that selects sufficient VME nodes for services operations and deployments. Furthermore, recent Deep Learning (DL) techniques such LSTM and GRU are implied to predict mobility and energy consumption sequences of IoT devices. These instances introduce feasible sets of IoT nodes, where the optimization algorithms should operate. Results show the efficiency of the DL instances and prove that the prediction awareness prevents the selected VME from failure and disconnection during the communication. Moreover, the proposed optimization algorithms are implemented and evaluated under different computing scenarios. Then, they are compared according to total allocated servers and SFC placement time. Optimization results show that both algorithms are efficient in terms of predetermined key performance indicators compared to the state of the art.
Keywords: Edge Computing (EC); Mobile Edge Computing (MEC); Service Function Chaining (SFC); Deep Learning (DL); Optimization algorithms

Zhongyang Wang, Du Xu,
Online optimization of intelligent reflecting surface-aided energy-efficient IoT-edge computing,
Future Generation Computer Systems,
Volume 141,
2023,
Pages 611-625,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.12.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X22004125)
Abstract: With the tremendous developments of Internet of Things (IoT), IoT devices and applications are imposing progressively higher requirements on computing, communication, and power services. Edge computing in combination with wireless power transmission (WPT) presents a viable solution. By empowering wireless access points (APs) with the capability of computation and WPT, APs can act as both edge computing servers and power stations. To improve the efficiency of data transmission and WPT, the Intelligent Reflective Surface (IRS) technique is adopted to passively beamforming thereby improving the channel condition of the wireless links. In such an IoT-edge system, efficiency and stability are the main concerns in designing a strategy of workload scheduling and resource allocation. In this paper, we propose an online algorithm named LSDR (Lyapunov optimization in combination with semi-definite relaxation) to minimize the total energy consumption while maintaining the system’s stable operation. The system’s stability implies that unprocessed data on the AP is limited and the battery level of each IoT sensor should not be depleted. Without requiring prior knowledge of the statistics of the IoT-edge system, the algorithm can balance the trade-off between system stability and total energy consumption. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate the effectiveness and efficiency of the algorithm.
Keywords: Edge computing; IoT; Online optimization; Intelligent Reflecting Surface; WPT

Shan Xiao, Chunyi Wu,
Explore deep reinforcement learning for efficient task processing based on federated optimization in big data,
Future Generation Computer Systems,
Volume 149,
2023,
Pages 150-161,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.06.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X23002492)
Abstract: In recent years, along with the extensive application of consumer electronics, the task execution with cloud computing for big data has become one of the research focuses. Nevertheless, the traditional theories and algorithms are still employed by existing research work to explore the feasible solutions, which takes a beating from low generalization performance, system load imbalance, more response delay, etc. To solve the matter, a task execution method called DROP (Deep Reinforcement network aided Optimization method aiming at task Processing) has been put forward, which is capable of completing task request allocation through virtual network embedding. The prominence of this method is explained by its effect in reducing load balancing degree, minimizing bandwidth resource overhead, and preserving electric energy as well as meeting customer demands. It makes use of Deep Deterministic Policy Gradient (DDPG) instead of depending on tons of iterations for better path selection schemes in previous methods, through continuous environment interaction and trial-and-error evaluation to get better strategy selection for virtual link embedding. To realize the virtual node embedding in the federated optimization based system architecture, the intentional deep feature learning network is applied. Compared with the cutting edge approaches, the performance benefits of DROP can be verified by the experimental results in terms of bringing down the extra cost on resources and energy of the substrate network during the task execution for big data.
Keywords: Big data; Consumer electronics; Deep reinforcement learning; Federated optimization; Virtual network embedding

Bo Li, Ruizhi Wu,
Joint perception data caching and computation offloading in MEC-enabled vehicular networks,
Computer Communications,
Volume 199,
2023,
Pages 139-152,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.12.021.
(https://www.sciencedirect.com/science/article/pii/S0140366422004716)
Abstract: The accelerated use of intelligent vehicles and the advancement of self-driving technologies have posed significant problems to the provision of real-time vehicular services, such as enormous amounts of computation, long transmission delay, and integration of sensor data. In this context, we propose to solve these problems to guarantee the Quality of Services (QoS) using computation offloading and perception data caching methods, where perception data means combinatorial sensor data, including sensor values and corresponding locations in an area. At first, we present a cooperative vehicular network framework with edge computing and sensor devices. Taking into account vehicle mobility, distributed resources, task properties, and user preferences, we jointly formulate a computation offloading strategy and a perception data caching strategy to minimize the average execution latency, which can be considered a Mixed-Integer Non-Linear Programming (MINLP) problem. We first propose a Genetic Algorithm (GA)-based scheme to solve our formulated problem. Then we propose a heuristic named Correlation-Monte Carlo Fast Search (CMCFS) method to obtain an effective strategy with low complexity. Simulation results reveal that both GA and CMCFS achieve less latency than other baseline schemes.
Keywords: Vehicular networks; Computation offloading; Data caching; Perception data

Yin Wang, Kang’An Gui,
MSEC-D based energy consumption optimization strategy in satellite communication systems,
Computer Communications,
Volume 211,
2023,
Pages 73-82,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.09.006.
(https://www.sciencedirect.com/science/article/pii/S014036642300316X)
Abstract: In this paper, we consider a satellite communication system that consists of a source low earth orbit (SLEO) satellite, several relay low earth orbit (RLEO) satellites, and a destination ground station (GS). Under the assumption that the data can be split into different sizes of data flow while transmitted through the system in different time slots, a joint route selection and time-slot allocation problem are formulated. Considering the constraints on the availability of transmission links, conservation of data flow, and system resources, an optimization problem that minimizes the total system energy consumption is formulated. As the original optimization problem is NP-hard, which cannot be solved conveniently, we transform the problem to the maximum transmission rate route selection subproblem and energy-efficient resource allocation subproblem. To solve these subproblems, we propose two algorithms, namely the multi-time-expanding graph-based maximum transmission rate route selection (MTEG-R) algorithm and the minimum system energy consumption - Dijkstra (MSEC-D) algorithm. The proposed algorithms enable the determination of routes selection and time slots allocation for data transmission. Simulation results demonstrate the effectiveness of the proposed scheme.
Keywords: Satellite systems; Route selection; Time-slot allocation; MTEG-R; MSEC-D; Energy consumption

Yanqi Gong, Kun Bian, Fei Hao, Yifei Sun, Yulei Wu,
Dependent tasks offloading in mobile edge computing: A multi-objective evolutionary optimization strategy,
Future Generation Computer Systems,
Volume 148,
2023,
Pages 314-325,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.06.015.
(https://www.sciencedirect.com/science/article/pii/S0167739X23002364)
Abstract: Due to the proliferation of applications such as virtual reality and online games with high real-time requirements, Mobile Edge Computing (MEC) has become a promising computing paradigm that can improve user experience and reduce the task offloading latency. The cloud–edge-end collaborative offloading further addresses the problem of insufficient computing resources of edge servers owing to large-scale computing-intensive applications in MEC. However, existing offloading solutions often ignore the important factor of economic cost, making it hard for these solutions to achieve a sustainable cloud–edge-end collaborative computation. To this end, this paper considers a multi-user multi-server, cloud–edge-end collaborative offloading scenario in the presence of dependent offloading tasks for the sake of maximizing rewards and minimizing latency. Each user issues a computing-intensive application consisting of multiple dependent tasks, which are offloaded collaboratively by various computational resources. With the goal of maximizing the yield of offloading for users and server providers, a multi-objective optimization problem of joint task offloading and execution rewards is studied. Technically, a multivariate multi-objective optimization problem with three objectives is modeled. An efficient multi-objective evolutionary optimization algorithm based on MOEA/D is then developed to solve the latency minimization and reward maximization problems. Extensive simulation results verify the effectiveness of the algorithm and illustrate that the proposed algorithm can significantly improve user offloading benefits. In addition, a scalability evaluations of our proposed algorithm is conducted for demonstrating its feasibility in large-scale task offloading scenarios.
Keywords: Dependent task offloading; Mobile edge computing; Multi-objective optimization; Evolutionary computation; Cloud–edge-end collaborative computing

Chao Zeng, Xingwei Wang, Rongfei Zeng, Ying Li, Jianzhi Shi, Min Huang,
Joint optimization of multi-dimensional resource allocation and task offloading for QoE enhancement in Cloud-Edge-End collaboration,
Future Generation Computer Systems,
Volume 155,
2024,
Pages 121-131,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.025.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000311)
Abstract: Cloud-Edge-End Collaboration (CEEC) computing architecture inherits many merits from both edge computing and cloud computing and thus is considered as a promising candidate for future network services. In CEEC, user’s QoE is impacted by offload performance which should consider offload strategy, computational resources and network status simultaneously. However, previous offload optimization studies neglect the joint consideration of dependent task offloading, computational resources and channel resources, which may not produce potential performance improvement. In this paper, we investigate the joint optimization of dependent task offloading, computational resource allocation, user transmission power control, and channel resource allocation in the CEEC scenario, with the goal of maximizing user’s QoE. Initially, a new QoE metric is defined to capture the impacts of delay and energy consumption on user’s QoE. Following this definition, we formulate the joint optimization problem as a Mixed Integer Nonlinear Programming (MINLP) problem and introduce a method of multi-agent deep reinforcement learning to solve our MINLP problem with high computation complexity. Extensive experiments are performed, and experimental results show that our proposed scheme outperforms baselines in a series of metrics.
Keywords: Cloud–edge–end collaboration; Task offloading; Resources allocation; Quality of experience; Multi-agent reinforcement learning

Greta Vallero, Margot Deruyck, Michela Meo, Wout Joseph,
Base Station switching and edge caching optimisation in high energy-efficiency wireless access network,
Computer Networks,
Volume 192,
2021,
108100,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108100.
(https://www.sciencedirect.com/science/article/pii/S138912862100181X)
Abstract: The improvement of the energy efficiency and the reduction of the latency are two of the main goals of the next generation of Radio Access Networks (RANs). In order to achieve the latter, Multi-access Edge Computing (MEC) is emerging as a promising solution: it consists of the placement of computing and storage servers, directly at each Base Station (BS) of these networks. For the RAN energy efficiency, the dynamic activation of the BSs is considered an effective approach. In this paper, the caching feature of the MEC paradigm is considered in a portion of an heterogeneous RAN, powered by a renewable energy generator system, energy batteries and the power grid, where micro cell BSs are deactivated in case of renewable energy shortage. The performance of the caching in the RAN is analysed through simulations for different traffic characteristics, as well as for different capacity of the caches and different spread of it. New user association policies are proposed, in order to totally exploit the MEC technology and reduce the network energy consumption. Simulation results reveal that, thanks to this technology and the proposed methodologies, the experienced delay and the energy consumption drop, respectively, up to 60% and 40%.
Keywords: Radio Access Network; Multi-access Edge Caching; Multi-access Edge Computing; Renewable energy; Energy efficiency

Pengjie Ai, Fei Wang,
Joint optimization for computation offloading and 3C resource allocations over wireless-powered and NOMA-enabled multi-access MEC,
Computer Networks,
Volume 246,
2024,
110415,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110415.
(https://www.sciencedirect.com/science/article/pii/S1389128624002470)
Abstract: Multi-access mobile edge computing (MEC) enables mobile users (MUs) to offload tasks to proximal multiple MEC servers for fast task processing. Since MUs generally have stringent delay requirements and limited energy and MEC servers have finite communication, computation, and caching (3C) resources, the joint co-design and optimization over computation offloading and 3C resource allocation for wireless-powered multi-access MEC systems have been highly demanded, while having not been well studied yet. We consider a wireless-powered multi-access MEC, where multiple energy harvesting (EH) based MUs and multiple base stations (BSs) each equipped with an MEC server coexist. Each MU first harvests energy from nearby MEC servers, and then offloads its sub-tasks to multiple MEC servers concurrently based on non-orthogonal multiple access (NOMA), so as to reduce data offloading delay. We first formulate a delay minimization problem, by jointly optimizing MUs’ computation offloading, MEC servers’ computation and caching resources allocation, and system communication resources allocation. Then we propose an alternating direction multiplier method (ADMM) based distributed scheme to decompose the formulated optimization problem into several sub-problems, and use the block coordinate descending (BCD) method and the successive convex approximation (SCA) method to transform all sub-problems each corresponding to one MEC server to convex subproblems. Finally, we validate and evaluate our proposed scheme through numerical analyses, which show that our proposed distributed scheme can greatly reduce the min–max delay of all MUs by allowing each MU’s concurrent data offloading to multiple MEC servers using NOMA and by jointly optimizing computation offloading and 3C resource allocations. Also, the delay imposed by our proposed scheme is much smaller than that imposed by the Interior Point method, which shows the effectiveness of our proposed scheme.
Keywords: Wireless powered multi-access MEC; NOMA; Computation offloading; 3C resource allocations; ADMM

Shuang Zhang, Huilong Jin, Pingkang Guo,
IRS-assisted energy efficient communication for UAV mobile edge computing,
Computer Networks,
Volume 246,
2024,
110387,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110387.
(https://www.sciencedirect.com/science/article/pii/S1389128624002196)
Abstract: Unmanned aerial vehicles (UAVs) combined with mobile edge computing (MEC) servers assist ground terminals (GTs) for communication and computation in wireless networks. Intelligent reflecting surface(IRS) can effectively assist the UAV to improve the communication quality between UAV and GTs, at the same time, reducing processing delay in UAV for remote areas. In this paper, we propose a framework where an MEC server is mounted on the UAV and an IRS is used to enhance the communication channel in the uplink. The optimization problem of computation offloading strategy and 3D trajectory planning is formulated for minimizing the system service energy consumption. It is an NP-hard nonconvex optimization issue with extra difficulty that offloading tasks is highly dynamic. To tackle this challenging problem, we get a convex upper bound. On this basis, an energy consumption minimization algorithm (SAC-Enc) evolved from the deep reinforcement learning (DRL) soft actor critic (SAC) algorithm is proposed. Numerical evaluations confirm that the proposed approach is capable of rapid convergence and achieves superior performance in terms of energy consumption and computation delay compared to other benchmark algorithms.
Keywords: Unmanned aerial vehicles (UAVs); Mobile edge computing (MEC); Intelligent reflecting surface (IRS); Computation offloading strategy; 3D trajectory planning

Rongping Lin, Xuhui Guo, Shan Luo, Yong Xiao, Bill Moran, Moshe Zukerman,
Application-aware computation offloading in edge computing networks,
Future Generation Computer Systems,
Volume 146,
2023,
Pages 86-97,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.009.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001437)
Abstract: Edge computing involves distributive computation resources deployed at the network edge, unlike cloud computing, which has central computation resources in data centers. Edge computing is a complement of cloud computing because edge computing effectively reduces the computing response delay by processing computation tasks and data near terminals. Considering the dramatic increase of terminals connected to networks and data generated by terminals, computation tasks from different applications may require significantly different services with different computation requirements, storage requirements, and response delay requirements. Application-aware computation offloading and resource allocation in edge computation can provide efficient and guaranteed computation services to terminals. In this paper, an application-aware computation offloading and resource allocation problem is investigated in edge computing networks, where computation tasks from different applications have different requirements. A non-convex optimization problem of energy consumption minimization is formulated, where terminals, edge nodes, and a cloud are considered. We convert the original non-convex optimization problem into a lower-bound convex problem and an upper-bound convex problem. Then, an algorithm based on the branch-and-bound method is proposed to force the lower- and upper-bound solutions to approach the optimal solution. Finally, the performance of the algorithm is analyzed where the gap to the optimal solution is provided. Numerical results show that the proposed algorithm can provide guaranteed services for tasks of different application types, with improvements over application-unaware algorithms.
Keywords: Edge computing; Computation offloading; Application-aware; Non-convex; Branch-and-bound; Stochastic gain

Chiara Caiazza, Claudio Cicconetti, Valerio Luconi, Alessio Vecchio,
Measurement-driven design and runtime optimization in edge computing: Methodology and tools,
Computer Networks,
Volume 194,
2021,
108140,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108140.
(https://www.sciencedirect.com/science/article/pii/S1389128621002085)
Abstract: Edge computing is projected to become the dominant form of cloud computing in the future because of the significant advantages it brings to both users (less latency, higher throughput) and telecom operators (less Internet traffic, more local management). However, to fully unlock its potential at scale, system designers and automated optimization systems alike will have to monitor closely the dynamics of both processing and communication facilities. Especially the latter is often neglected in current systems since network performance in cloud computing plays only a minor role. In this paper, we propose the architecture of MECPerf, which is a solution to collect network measurements in a live edge computing domain, to be collected for offline provisioning analysis and simulations, or to be provided in real-time for on-line system optimization. MECPerf has been validated in a realistic testbed funded by the European Commission (Fed4Fire+), and we describe here a summary of the results, which are fully available as open data and through a Python library to expedite their utilization. This is demonstrated via a use case involving the optimization of a system parameter for migrating clients in a federated edge computing system adopting the GSMA platform operator concept.
Keywords: Edge computing; ETSI MEC; Network measurements; GSMA platform operator

Peiyan Yuan, Saike Shao, Lijuan Geng, Xiaoyan Zhao,
Caching hit ratio maximization in mobile edge computing with node cooperation,
Computer Networks,
Volume 200,
2021,
108507,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108507.
(https://www.sciencedirect.com/science/article/pii/S1389128621004424)
Abstract: In this study, we investigate the cooperation problem of edge nodes in mobile edge scenarios. We study two performance metrics: caching hit ratio and cooperation cost. We analyze the relationship of the two metrics for cooperative edge networks in which multiple edge nodes form a forwarding group to serve users. We construct a spanning tree to organize the forwarding group, and maximize the hit ratio subject to a budget constraint on cooperation cost. We first address the optimization problem using the method of Lagrangian multipliers and derive the optimal group parameter. Furthermore, a distributed optimization algorithm with alternating direction method of multipliers (ADMM) via constraint projection and variable splitting is proposed to achieve the desired goal. Finally, we build an edge computing simulation platform consisting of hundreds of edge nodes and users as a case study to verify the effectiveness and efficiency of our proposed method. Numerical results show that it significantly improves the caching hit ratio up to 60%, compared to the classic scheme.
Keywords: Edge node cooperation; Forwarding group; Caching hit ratio; Mobile edge computing; ADMM

Chunlin Li, Jun Liu, Qingchuan Zhang, Youlong Luo,
Efficient cooperative cache management for latency-aware data intelligent processing in edge environment,
Future Generation Computer Systems,
Volume 123,
2021,
Pages 48-67,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.04.012.
(https://www.sciencedirect.com/science/article/pii/S0167739X21001333)
Abstract: Recently, with the rapid development of the fifth-generation (5G) communication technology, various new services such as virtual reality (VR), augmented reality (AR), and video conferencing, etc. have emerged. To achieve optimized quality of service (QoS), high load and low latency, we propose an effective mobile edge caching strategy. Specifically: first, a cache prefetching algorithm is proposed to improve cache hit rate. second, the load balancing algorithm based on the maximum distribution weighted entropy is designed to improve bandwidth utilization and avoid mobile network congestion; third, the optimized cache replacement policy is built based on the content future popularity to reduce access latency and backhaul link pressure. With these proposed cache prefetching and replacement algorithm, the access latency can be reduced significantly, the load balance can be maintained and cache hit ratio can be improved obviously in 5G campus networks. This mobile edge caching policy is an effective improvement of the existing strategy, offering a strong support for the full realization of the 5G network potential.
Keywords: Mobile edge computing; 5G network; Cache prefetching; Cache replacement; Dynamic popularity

Annisa Sarah, Gianfranco Nencioni, Md. Muhidul I. Khan,
Resource Allocation in Multi-access Edge Computing for 5G-and-beyond networks,
Computer Networks,
Volume 227,
2023,
109720,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109720.
(https://www.sciencedirect.com/science/article/pii/S1389128623001652)
Abstract: Innovative services with strict requirements are expected in the fifth generation (5G) of mobile networks and beyond. For example, the Ultra-Reliable Low-Latency Communication (URLLC) requires up to 1 ms latency, end-to-end security, and reliability of up to 99.999%. The Multi-access Edge Computing (MEC) promises to support the delivery of URLLC services by providing computing and storage resources in the proximity of user equipment. The data which previously needed to be processed and stored in the cloud systems can be kept at the edge network, decreasing the total latency and increasing the context-awareness, security, and dependability. Vastly available resources, which are available from cloud to edge, must be appropriately allocated to deliver a service efficiently. The resource allocation problem in MEC for 5G-and-beyond networks can be formulated differently, depending on the nature of the problem. This survey outlines the resource allocation problem as a proper problem formulation, which can be addressed by target, resource type, resource issue, and the considered assumptions. Moreover, this paper also describes the open issues and future directions for MEC resource allocation based on the state of the art on this research topic.
Keywords: Resource allocation; MEC; Edge computing; 5G

Shamim Taimoor, Lilatul Ferdouse, Waleed Ejaz,
Holistic resource management in UAV-assisted wireless networks: An optimization perspective,
Journal of Network and Computer Applications,
Volume 205,
2022,
103439,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103439.
(https://www.sciencedirect.com/science/article/pii/S1084804522000923)
Abstract: Unmanned aerial vehicles (UAVs) are considered as a promising solution to assist terrestrial networks in future wireless networks (i.e., beyond fifth-generation (B5G) and sixth-generation (6G)). The convergence of various technologies requires future wireless networks to provide multiple functionalities, including communication, computing, control, and caching (4Cs), necessary for applications such as connected robotics and autonomous systems. The majority of existing works consider the developments in 4Cs individually, which limits the cooperation among 4Cs for potential gains The limited resources at the network edge call for holistic management of the resources, which requires joint optimization. This survey provides a comprehensive review of holistic resources management in UAV-assisted wireless networks. The integrated resource management considers the challenges associated with aerial networks, such as 3D placement of UAV, trajectory planning, channel modelling, backhaul connectivity, and the challenges related to terrestrial networks, such as limited bandwidth, power, and interference. We briefly present architectures (source–UAV–destination and UAV–destination architecture) and 4Cs in UAV-assisted wireless networks. We then provide a detailed discussion on resource management by categorizing the optimization problems into individual or combinations of two (communication and computation) or three (communication, computation and control). Moreover, solution approaches and performance metrics are discussed and analysed for different objectives and problem types. Finally, the insight about the potential future research areas to address the challenges of holistic resource management in UAV-assisted wireless networks are discussed.
Keywords: Communication; Computing; Caching; Control; Edge computing; Optimization; Resource management; UAVs

Sara A. Elsayed, Sherin Abdelhamid, Hossam S. Hassanein,
Prediction-Based Cooperative Cache Discovery in VANETs for Social Networking,
Computer Communications,
Volume 214,
2024,
Pages 184-200,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.11.030.
(https://www.sciencedirect.com/science/article/pii/S0140366423004279)
Abstract: Social media traffic is increasingly pervading the Internet. Such traffic is mostly generated by mobile devices, which creates immense traffic load on backhaul links in 5G networks. This load can be mitigated by using vehicular networks as a traffic offloading platform, where intelligent vehicles can act as a valuable asset that can bring the data closer to the requester via edge caching. In this paper, we propose the Prediction-Assisted Cooperative Content Discovery (PACD) scheme. PACD exploits the static and mobile nature of parked and moving vehicles, respectively, to promulgate cached content information into the network via bloom filters. PACD is the first scheme that leverages such information to perform cooperative cache discovery to locate closer replicas to the requester by dynamically predicting the location of caching nodes. The Any Relation Clustering Algorithm (ARCA) is employed to cluster trips based on their route similarity using the XXDice similarity coefficient. Each cluster is then trained using the Mixture Transition Distribution-Probit (MTD-Probit) model to predict the remaining trajectory of vehicles. Using these predictions, PACD tracks all possible data providers and ranks them based on their proximity to the requester, as well as their prediction entropy. Extensive evaluations show that PACD yields significant improvements of up to 86%, 30%, 42%, and 16% in terms of delay, packet delivery ratio, cache hit ratio, and prediction accuracy, respectively, compared to prominent caching and prediction schemes in vehicular networks.
Keywords: VANETs; Intelligent vehicles; Caching; Prediction; Cooperative cache discovery

Uchechukwu Awada, Jiankang Zhang, Sheng Chen, Shuangzhi Li, Shouyi Yang,
Resource-aware multi-task offloading and dependency-aware scheduling for integrated edge-enabled IoV,
Journal of Systems Architecture,
Volume 141,
2023,
102923,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102923.
(https://www.sciencedirect.com/science/article/pii/S1383762123001029)
Abstract: Internet of Vehicles (IoV) enables a wealth of modern vehicular applications, such as pedestrian detection, real-time video analytics, etc., that can help to improve traffic efficiency and driving safety. However, these applications impose significant resource demands on the in-vehicle resource-constrained Edge Computing (EC) device installation. In this article, we study the problem of resource-aware offloading of these computation-intensive applications to the Closest roadside units (RSUs) or telecommunication base stations (BSs), where on-site EC devices with larger resource capacities are deployed, and mobility of vehicles are considered at the same time. Specifically, we propose an Integrated EC framework, which can keep edge resources running across various in-vehicles, RSUs and BSs in a single pool, such that these resources can be holistically monitored from a single control plane (CP). Through the CP, individual in-vehicle, RSU or BS edge resource availability can be obtained, hence applications can be offloaded concerning their resource demands. This approach can avoid execution delays due to resource unavailability or insufficient resource availability at any EC deployment. This research further extends the state-of-the-art by providing intelligent multi-task scheduling, by considering both task dependencies and heterogeneous resource demands at the same time. To achieve this, we propose FedEdge, a variant Bin-Packing optimization approach through Gang-Scheduling of multi-dependent tasks that co-schedules and co-locates multi-task tightly on nodes to fully utilize available resources. Extensive experiments on real-world data trace from the recent Alibaba cluster trace, with information on task dependencies and resource demands, show the effectiveness, faster executions, and resource efficiency of our approach compared to the existing approaches.
Keywords: Edge computing; IoV; Dependency-aware; Execution time; Resource efficiency; Co-location

Manoj Kumar Somesula, Rashmi Ranjan Rout, D.V.L.N. Somayajulu,
Contact duration-aware cooperative cache placement using genetic algorithm for mobile edge networks,
Computer Networks,
Volume 193,
2021,
108062,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108062.
(https://www.sciencedirect.com/science/article/pii/S1389128621001559)
Abstract: Caching popular content at the base stations cooperatively is an effective solution to reduce the user-perceived latency and overwhelming data traffic by bringing content close to the user in a cellular network-based Mobile Edge Computing (MEC) architecture. Most of the existing literature assumes static network models where all the users remain static throughout the data transfer time, and the user can download the requested content from the associated base station. Caching content by considering user mobility and randomness of contact duration is an important issue which has been addressed in this work. We consider the cache placement problem in a realistic scenario where users move at different speeds. The moving users that are connected to the multiple base stations intermittently may not download full content because of contact duration. This, in turn, increases the overall delay in downloading the content for mobile users. The cache placement problem is formulated as mixed-integer nonlinear programming to maximize the saved delay with capacity constraint. The user mobility and contact duration are modeled with a Markov renewal process. Further, a greedy algorithm is presented to solve the problem by adopting submodular optimization. For real scenarios that scale to large library sizes, taking into account the computational time, we have proposed a genetic algorithm-based heuristic search mechanism. Extensive simulation results show that the proposed contact duration aware caching scheme significantly improves the performance in terms of hit ratio and acceleration ratio in a real-world scenario as compared with three existing caching mechanisms.
Keywords: Content placement; User mobility; Cooperative caching; Submodular optimization; Mobile edge networks; Genetic algorithm

Yantong Wang, Vasilis Friderikos,
Energy-efficient proactive caching with multipath routing,
Computer Networks,
Volume 216,
2022,
109272,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109272.
(https://www.sciencedirect.com/science/article/pii/S1389128622003346)
Abstract: The ever-continuing explosive growth of on-demand content distribution has imposed great pressure on mobile/wireless network infrastructures. To ease congestion in the network and to increase perceived user experience, caching of popular content closer to the end-users can play a significant role and as such this issue has received significant attention over the last few years. Additionally, energy saving is treated as a fundamental requirement in the design of next-generation mobile networks. However, there has been little attention to the overlapping area between energy saving and network caching especially when considering multipath routing. To this end, this paper proposes an energy-efficient caching with multipath routing support. The proposed scheme provides a joint anchoring of popular content into a set of potential caching nodes with optimized multipath support whilst ensuring a balance between transmission and caching energy cost. The proposed model also considers different content delivery modes, such as multicast and unicast. Two separated Integer-Linear Programming (ILP) models are formulated for each delivery mode. To tackle the curse of dimensionality we then provide a greedy simulated annealing algorithm, which not only reduces the time complexity but also provides a competitive performance. A wide set of numerical investigations reveal that the proposed scheme reduces the energy consumption up to 75% compared with other widely used caching approaches under the premise of network resource limitation. Sensitivity analysis to different parameters is also meticulously discussed in this paper.
Keywords: Wireless networks; Proactive caching; Multipath routing; Energy saving; Integer linear programming; Simulated annealing

Alok Choudhury, Manojit Ghose, Akhirul Islam,  Yogita,
Machine learning-based computation offloading in multi-access edge computing: A survey,
Journal of Systems Architecture,
Volume 148,
2024,
103090,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103090.
(https://www.sciencedirect.com/science/article/pii/S1383762124000274)
Abstract: The advancement of technology towards the realization of the evolving mobile computing paradigm brings a rapid paradigm shift in its usage, especially in the Internet, computation, and communications, that has a profound impact on businesses, services, and users. With the rise in resource-intensive or edge-based mobile applications such as autonomous driving, Amazon Go, virtual and augmented reality, and healthcare-related applications, countless challenges in computation and communication parameters like latency, bandwidth, and energy consumption are evolving. As a result, the multi-access edge computing (MEC) paradigm receives enormous attention where some portions of the user applications are offloaded to powerful machines for their efficient execution to optimize different evaluation metrics or to achieve performance goals. While a few survey works are available in this direction, none of them focuses explicitly on the emerging machine learning (ML) based computation offloading techniques and various associated sub-problems together. This paper aims to provide a detailed but precise overview of the research on using ML techniques for MEC environments. In this survey, we focus on how authors and researchers utilize the ML models in computation offloading problems on MEC architecture. We extend our study by considering several edge architectures, offloading parameters, ML approaches, and problem formulation strategies concerning computation offloading. Additionally, this paper discusses the potential challenges in the direction of computation offloading on MEC architecture.
Keywords: Computation offloading in MEC; Machine learning; Multi-access edge computing; Mobile edge computing; Computation for 5G/6G communication

Shahid Md. Asif Iqbal,  Asaduzzaman,
Cache-MCDM: A hybrid caching scheme in Mobile Named Data Networks based on multi-criteria decision making,
Future Generation Computer Systems,
Volume 154,
2024,
Pages 344-358,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000165)
Abstract: Robust and reliable content retrieval in a Mobile Ad Hoc Network (MANET) is challenged by its intermittent connectivity and mobility characteristics. In-network caching and name-based content retrieval attributes of Named Data Networks (NDN) can counterattack the stated challenges of MANETs. The distance- or hop-based caching schemes, proposed mainly for infrastructure NDN, admit contents either close to the content source, client, or midway, which are trivial to realize since content mainly travels from the network core (content sources) to the edge (clients). In NDN-based MANETs, the mobility attribute causes source and consumer nodes to change their locations frequently. Therefore, the content can travel from the network core to the edge and vice-versa. Under such circumstances, the traditional hop-based schemes that cache at particular points, for instance, either near the source, client, or halfway, may vandalize the efficient content retrieval in MANETs. Thus, proposing a new strategy capable of caching at any of the stated caching points considering varying network contexts, for instance, location change of source and consumer, is inevitable. Such a strategy can scale well in terms of network resource usage and significantly improve PDR and content retrieval latency performances in NDN-based MANET. In this direction, we propose an AHP-based hybrid caching scheme, namely Cache-MCDM, that equips each router with a set of different hop-based caching options and employs AHP-TOPSIS method to pick the most suitable caching option. The proposed method models the suitable option selection as a Multi-Criteria Decision Making (MCDM) problem. Simulation reveals that Cache-MCDM is more effective and efficient than hop-based probabilistic caching (HPC), opportunistic caching (OppProb), probabilistic in-network caching (ProbCache), and SourceCacheProb (SCP), in terms of (i) PDR by approximately 3%, 15%, 10%, and 11%, respectively, (ii) latency by approximately 25%, 145%, 75%, and 77%, respectively, (iii) retransmission ratio by approximately 7%, 30%, 25%, and 25%, respectively, and (iv) cache replacement rate by approximately 8%, 100%, 80%, and 25%, respectively.
Keywords: Local performance indicators; AHP; TOPSIS; Multi-criteria decision making; Hybrid caching; Hop-based caching

Nhu-Ngoc Dao, Duc-Nghia Vu, Woongsoo Na, Trong-Minh Hoang, Dinh-Thuan Do, Sungrae Cho,
Adaptive bitrate streaming in multi-user downlink NOMA edge caching systems with imperfect SIC,
Computer Networks,
Volume 212,
2022,
109064,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109064.
(https://www.sciencedirect.com/science/article/pii/S1389128622002080)
Abstract: Recently, mobile networks have witnessed an increasing dominance of video traffic streams to simultaneously distribute real-time contents to massive number of user devices. In this context, fifth-generation (5G) technologies and beyond expose their advanced new radio access interfaces to facilitate multi-user associations with successive interference cancellation (SIC)-based non-orthogonal multiple access (NOMA) mechanisms. However, the imperfectness of the SIC in reality may result in significant performance decrease in NOMA owing to inter-channel interference existence. Motivated by this observation, this paper studies adaptive bitrate streaming services given such a challenging transmission environment assumed. In this context, our objectives are to maximize the video bitrate (accordingly the video resolution) of the online streams while retaining the playback smoothness. The problem is transformed into a drift-plus-penalty balancing optimization, which is then resolved by an approximation algorithm. Numerical results highlight the outperformance of the proposed approach compared to existing algorithms in terms of video quality and smoothness in various system scenarios.
Keywords: Adaptive bitrate streaming; Edge caching system; Imperfect SIC; Multi-user downlink NOMA

Mohammad Sadegh Aslanpour, Adel N. Toosi, Muhammad Aamir Cheema, Mohan Baruwal Chhetri, Mohsen Amini Salehi,
Load balancing for heterogeneous serverless edge computing: A performance-driven and empirical approach,
Future Generation Computer Systems,
Volume 154,
2024,
Pages 266-280,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.020.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000207)
Abstract: Serverless edge systems simplify the deployment of real-time AI-based Internet of Things (IoT) applications at the edge. However, the heterogeneity of edge computing nodes – in terms of both hardware and software – makes load balancing challenging in these systems. In this paper, we propose a performance-driven, empirical weight-tuning approach to achieve effective load balancing based on the characteristics and capabilities of the nodes. By extensively profiling the nodes, we gather knowledge on performance metrics such as throughput, energy efficiency, response time, AI accuracy, and cost. Using this acquired knowledge, we introduce a weighted round-robin strategy to optimize the performance metrics according to their observed significance. To address multiple objectives, we introduce a multi-objective method that aims to strike a balance between any arbitrary set of performance objectives simultaneously. Additionally, we explore a coordinated distributed approach to overcome the limitations of centralized load balancing. Next, we introduce Hedgi, a heterogeneous serverless edge architecture designed to efficiently configure and utilize the derived load balancing policies, validated empirically. To demonstrate the practicality of Hedgi, we containerize and serverlessize a real-time object detection application. Extensive empirical studies are conducted using Hedgi to evaluate the performance of the proposed load balancing approach. The results provide valuable insights into the design trade-offs of various load balancing policies and system designs in the heterogeneous serverless edge.
Keywords: Heterogeneity; Serverless; Function-as-a-service; Edge computing; Load balancing; Performance

Peizhi Yan, Salimur Choudhury,
Deep Q-learning enabled joint optimization of mobile edge computing multi-level task offloading,
Computer Communications,
Volume 180,
2021,
Pages 271-283,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.09.028.
(https://www.sciencedirect.com/science/article/pii/S0140366421003650)
Abstract: In a mobile edge computing (MEC) network, mobile devices could selectively offload tasks to the edge server(s) to save time and energy. However, we should consider many dynamic factors in task offloading optimization, which increases the complexity of this problem. The traditional optimization approaches could require solving complex models to derive the optimal solution. This type of optimization problems are often NP-hard and will cause a considerable overhead on optimization. In contrast, a well-trained empirical model, such as an artificial neural network could be more efficient in decision making. In this research, considering the potential uneven spatial distribution of mobile devices in a MEC network with multiple wireless edge gateways, we allow an edge gateway to offload tasks to a nearby edge gateway further. We propose a deep Q-learning-based joint optimization approach for both device-level and edge-level task offloading. We also design a centralized mathematical programming solution for exploring the optimal trade-off performance. Simulation results show that the proposed approach achieves a satisfactory task delay performance and a better trade-off between the task delay and the energy consumption on tasks.
Keywords: Mobile edge computing; Task offloading; Optimization; Deep Q-learning

Jingfu Lu, Jiuling Li,
Two-phase sample average approximation for video distribution strategy of edge computing in heterogeneous network,
Computer Communications,
Volume 182,
2022,
Pages 255-267,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.11.007.
(https://www.sciencedirect.com/science/article/pii/S0140366421004345)
Abstract: Cooperative edge computing provides a good platform for edge storage and computing at the same time. When a single edge server cannot efficiently provide video services, multiple edge servers are needed to share resources for collaborative storage and computing. Existing collaboration schemes often ignore the coupling between video caching and distribution, resulting in inefficient caching strategies. Therefore, for the scenario where multiple edge servers provide video services cooperatively, the content access delay in the video service is minimized in this paper based on the video caching strategy on the slow time scale and the video distribution strategy on the fast time scale. The problem is modeled as a random integer linear programming problem on dual time scales. Moreover, an algorithm is proposed based on the sample average approximation technique. The algorithm first designs the video caching strategy of the edge server based on the statistical information of the arrival of video requests on the slow time scale and the expected video distribution strategy. Then, the video distribution strategy is optimized based on the designed caching strategy and the video request situation on fast time scales. Finally, through the cooperation of the two time scales, the content access delay is minimized, and the simulation results verify the advantages of the proposed scheme in reducing content access delay and improving storage hit rate. The storage hit rate is increased by 7.6%; the total content access delay is increased by 17.42%; the number of edge servers and the arrival rate of video requests are ahead of other methods by 5%; and the transcoding time is shortened by 4.56 s. It fully verifies that the design of a more efficient multi-user computing offload strategy in this paper can solve the problems of computing power, bandwidth, delay Energy consumption and other bottlenecks are of practical significance.
Keywords: 5G network; Heterogeneous network; Edge computing; Video distribution; SAATP algorithm

Ali Asghari, Mohammad Karim Sohrabi,
Server placement in mobile cloud computing: A comprehensive survey for edge computing, fog computing and cloudlet,
Computer Science Review,
Volume 51,
2024,
100616,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100616.
(https://www.sciencedirect.com/science/article/pii/S1574013723000837)
Abstract: The growing technology of the fifth generation (5G) of mobile telecommunications has led to the special attention of cloud service providers (CSPs) to mobile cloud computing (MCC). Due to the limitations in processing power, storage space and energy capacity of mobile devices, cloud resources can be moved to the edge of the network to improve the quality of service (QoS). Server placement is a crucial emerging problem in both typical and edge types of MCC, different proposed methods of which are reviewed and evaluated in this paper. Proper placement of servers leads to more efficient utilization of these servers, reduces their response time and optimizes their energy consumption. A variety of techniques and approaches, including machine learning-based techniques, evolutionary models, optimization algorithms, heuristics and meta-heuristics have been employed by different server placement methods of the literature to find the optimal deployment map of servers. This paper provides a comprehensive analysis of these server placement methods in edge computing, fog computing and cloudlet, investigates their various aspects, dimensions and objectives, and evaluates their strengths and weaknesses. Furthermore, open challenges for server placement in MCC are provided, and future research directions are also explained and discussed.
Keywords: Mobile cloud computing; Server placement; Edge computing; Fog computing; Cloudlet

Fuhong Song, Huanlai Xing, Xinhan Wang, Shouxi Luo, Penglin Dai, Ke Li,
Offloading dependent tasks in multi-access edge computing: A multi-objective reinforcement learning approach,
Future Generation Computer Systems,
Volume 128,
2022,
Pages 333-348,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.10.013.
(https://www.sciencedirect.com/science/article/pii/S0167739X21004039)
Abstract: This paper studies the problem of offloading an application consisting of dependent tasks in multi-access edge computing (MEC). This problem is challenging because multiple conflicting objectives exist, e.g., the completion time, energy consumption, and computation overhead should be optimized simultaneously. Recently, some reinforcement learning (RL) based methods have been proposed to address the problem. However, these methods, called single-objective RLs (SORLs), define the user utility as a linear scalarization. The conflict between objectives has been ignored. This paper formulates a multi-objective optimization problem to simultaneously minimize the application completion time, energy consumption of the mobile device, and usage charge for edge computing, subject to dependency constraints. Moreover, the relative importance (preferences) between the objectives may change over time in MEC, making it quite challenging for traditional SORLs to handle. To overcome this, we first model a multi-objective Markov decision process, where the scalar reward is extended to a vector-valued reward. Each element in the reward corresponds to one of the objectives. Then, we propose an improved multi-objective reinforcement learning (MORL) algorithm, where a tournament selection scheme is designed to select important preferences to effectively maintain previously learned policies. The simulation results demonstrate that the proposed algorithm obtains a good tradeoff between three objectives and has significant performance improvement compared with a number of existing algorithms.
Keywords: Computation offloading; Dynamic preferences; Multi-access edge computing; Multi-objective reinforcement learning; Task dependency

Mingzhe Wang, Qiuliang Zhang,
Optimized data storage algorithm of IoT based on cloud computing in distributed system,
Computer Communications,
Volume 157,
2020,
Pages 124-131,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.04.023.
(https://www.sciencedirect.com/science/article/pii/S0140366420304643)
Abstract: The existing Internet of Things(IoT) uses cloud computing data access storage algorithms, that is, the hash algorithm has defects of low data processing efficiency and low fault tolerance rate. Therefore, HDFS is introduced to optimize cloud computing data access storage algorithms. HDFS is first used to optimize the data access storage architecture according to problems of data access storage architecture in the Internet of Things, in which factors of data access storage distribution in the IoT are fully considered, and hash values are used to optimize the configuration of data access information storage locations, so that data access storage distribution strategy can be optimized. Then, the topology of the IoT is optimized, and data block size is also optimized with effect algorithm. Finally, the design of file storage is optimized. Through simulation experiments, it is proved that the optimized cloud storage method has obvious performance advantages in file read and write speed as well as memory usage. Compared with the traditional hash algorithm, optimization algorithm proposed in the paper greatly improves file upload and download efficiency, data processing efficiency and fault tolerance rate, which fully demonstrates that the proposed cloud computing data access storage optimization algorithm is more superior.
Keywords: Cloud computing; Data access; Storage; Hash algorithm; Network topology

Yu Liang, Sheng Zhang,
Mobility-aware multi-user service placement and resource allocation in edge computing,
Computer Networks,
Volume 236,
2023,
110020,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110020.
(https://www.sciencedirect.com/science/article/pii/S1389128623004656)
Abstract: The rise of Mobile Edge Computing (MEC) has brought us its enormous potential and value in mobile service applications. By pushing computing and storage resources from the cloud to the network edge, it reduces transmission latency, and supports applications that require low latency, such as virtual reality and video analytics IoT services. However, many existing works only consider the problem of service placement in MEC, and the problem of computing resource allocation has received less attention. This paper focuses on the joint optimization of Service Placement and computational Resource Allocation (SPRA) in the MEC environment, with the goal of minimizing the total cost of service latency, communication latency, and service migration. For offline cases, we propose an optimal algorithm, D-SPA , based on dynamic programming and an improved algorithm, S-SPA, based on state sampling, which effectively alleviates the state explosion problem in D-SPA. In the online case, due to the unpredictability of future environmental information, we propose the online greedy algorithm OGA, and theoretically show the approximate ratio of OGA. Extensive experiments show that our algorithm is more efficient than other baseline algorithms, and it reduces the total cost by 28.6% on average.
Keywords: Mobile edge computing; Service placement; Resource allocation; State sampling; Approximation

Sehun Jung, Hyang-Won Lee,
Optimization framework for splitting DNN inference jobs over computing networks,
Computer Networks,
Volume 232,
2023,
109814,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109814.
(https://www.sciencedirect.com/science/article/pii/S1389128623002591)
Abstract: Ubiquitous artificial intelligence (AI) is considered one of the key services in 6G systems. AI services typically rely on deep neural network (DNN) requiring heavy computation. Hence, in order to support ubiquitous AI, it is crucial to provide a solution for offloading or distributing computational burden due to DNN, especially at end devices with limited resources. We develop an optimization framework for assigning the tasks of DNN layer computations to computing resources in the network, so as to reduce the inference latency. To this end, we propose a layered graph model with which simple conventional routing jointly solves the problem of selecting nodes for computation and paths for data transfer between nodes. We show that using our model, the existing approaches to splitting DNN layer computations can be equivalently reformulated as a routing problem that possesses better numerical properties. We also apply the proposed framework to derive algorithms for minimizing the end-to-end inference latency. We show through numerical evaluations that our new formulation can find a solution for DNN inference job distribution much faster than the existing formulation, and that our algorithms can select computing nodes and data paths adaptively to the computational attributes of given DNN inference jobs.
Keywords: DNN layers distribution; Computing network; Completion time; Computation path selection

Manoj Kumar Somesula, Rashmi Ranjan Rout, D.V.L.N. Somayajulu,
Cooperative cache update using multi-agent recurrent deep reinforcement learning for mobile edge networks,
Computer Networks,
Volume 209,
2022,
108876,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108876.
(https://www.sciencedirect.com/science/article/pii/S1389128622000809)
Abstract: Caching the most likely to be requested content at the base stations in a cooperative manner can facilitate direct content delivery without fetching content from the remote content server and thus alleviate the user-perceived latency, reduce the burden on backhaul and minimize the duplicated content transmissions. Content popularity plays a vital role, and it drives caching on edge. In the literature, earlier works considered the content popularity either known earlier or obtained on prediction. However, the content popularity is time-varying and unknown in reality, so the above assumption makes it less practical. Therefore, this paper considers the cooperative cache replacement problem in a realistic scenario where the edge nodes are unaware of the content popularity in mobile edge networks. To address this problem, the main contribution of this paper is to design an intelligent content update mechanism using multi-agent deep reinforcement learning in dynamic environments. With the goal of maximizing the saved delay with deadline and capacity constraints, we formulate the cache replacement problem as Integer linear programming problem. Considering the dynamic nature of the content popularity, high dimensional parameters, and for an intelligent caching decision, we model the problem as a partially observable Markov decision process and present an efficient deep reinforcement learning algorithm by embedding the long short-term memory network (LSTM) into a multi-agent deep deterministic policy gradient formalism. The LSTM inclusion reduces the instability produced by partial observability of the environment. Extensive simulation results demonstrate that the proposed cooperative caching mechanism significantly improves the performance in terms of reward, acceleration ratio and hit ratio compared with existing mechanisms.
Keywords: Mobile edge networks; Cooperative caching; Multi-agent deep reinforcement learning; Partially observable Markov decision process; Multi-agent deep deterministic policy gradient

Junna Zhang, Jiawei Chen, Xiang Bao, Chunhong Liu, Peiyan Yuan, Xinglin Zhang, Shangguang Wang,
Dependent task offloading mechanism for cloud–edge-device collaboration,
Journal of Network and Computer Applications,
Volume 216,
2023,
103656,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103656.
(https://www.sciencedirect.com/science/article/pii/S1084804523000759)
Abstract: Edge computing provides abundant computing and storage resources at the edge of a network, aiming to meet the growing requirements of delay-sensitive mobile applications. To fully utilize the advantages of edge computing, it is essential to design an appropriate task offloading strategy to efficiently use edge resources. However, the existing studies of task offloading often ignored the dependencies between tasks and the limited service capabilities of edge servers, resulting in long completion times or even infeasible offloading decisions. Therefore, an efficient dependent task offloading mechanism that can optimize the overall task completion time is proposed in this paper. This mechanism is suitable in scenarios where edge servers have limited service caches and computing power. First, a collaborative offloading model that includes one cloud server, edge servers and local devices is designed. This model uses the cloud server as the offloading node to balance the workload among edge servers. Second, the research problem is formulated as a binary optimization function with the goal of minimizing the application completion time, and a dependent task offloading algorithm for a cloud–edge-device collaborative model is proposed. Finally, extensive experiments are performed to verify the effectiveness of the proposed model and algorithm. The experimental results show that compared with other algorithms, the proposed algorithm can reduce the application completion time by approximately 6% to 30%. In addition, the proposed dependent task offloading mechanism displays better adaptability and scalability for large-scale tasks.
Keywords: Edge computing; Task offloading; Cloud–edge-device collaboration; Dependent task; Service caching

Sekione Reward Jeremiah, Laurence Tianruo Yang, Jong Hyuk Park,
Digital twin-assisted resource allocation framework based on edge collaboration for vehicular edge computing,
Future Generation Computer Systems,
Volume 150,
2024,
Pages 243-254,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.09.001.
(https://www.sciencedirect.com/science/article/pii/S0167739X23003278)
Abstract: Vehicular Edge Computing (VEC) supports latency-sensitive and computation-intensive vehicular applications by providing caching and computing services in vehicle proximity. This reduces congestion and transmission latency. However, VEC faces implementation challenges due to high vehicle mobility and unpredictable network dynamics. These challenges pose difficulties to network resource allocation. Most existing VEC network resource management solutions consider edge–cloud collaboration and ignore collaborative computing between edge nodes. A reasonable collaboration between Roadside Units (RSUs) or small cells eNodeB can improve VEC network performance. Our proposed framework aims to improve VEC network performance by integrating Digital Twin (DT) technology which creates virtual replicas of network nodes to estimate, predict, and evaluate their real-time conditions. A DT is constructed centrally to maintain and simulate VEC network, thus enabling edge nodes collaboration and real-time resources information availability. We employ channel state information (CSI) for RSUs selection, and vehicles communicate with RSUs through a non-orthogonal multiple access (NOMA) protocol. We aim to maximize the VEC system computation rate and minimize task completion delay by jointly optimizing offloading decisions, subchannel allocation, and RSU association. In view of the resulting optimization problem complexity (NP-hard), we model it as a Markov Decision Process (MDP) and apply Advantage Actor–Critic (A2C) algorithm to solve it. Validated via simulations, our scheme shows superiority to the benchmarks in reducing task completion delay and improving VEC system computation rates.
Keywords: Digital twin; Edge cooperation; Resource allocation; Artificial intelligence; Vehicular edge computing; Deep reinforcement learning

Yong Liu, Tianyi Yu, Qian Meng, Quanze Liu,
Flow optimization strategies in data center networks: A survey,
Journal of Network and Computer Applications,
Volume 226,
2024,
103883,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103883.
(https://www.sciencedirect.com/science/article/pii/S1084804524000602)
Abstract: In the era of digitization, Data Center Networks (DCN) have emerged as a critical component supporting infrastructure for cloud computing, big data analytics, online services, and more. With the exponential growth in data transmission and processing demands, the issues related to the performance and efficiency of DCN have become increasingly urgent and complex. However, flow in DCN exhibits distinct characteristics compared to traditional networks, including uneven flow volume distribution, bursty transmission demands, and intricate network topologies. Conventional flow optimization strategies often fall short in this unique context. In recent years, researchers have actively proposed a plethora of innovative flow optimization strategies to address the challenges of enhancing the performance and efficiency of DCN. This survey aims to comprehensively review and analyze flow optimization strategies in DCN, encompassing various key areas of research and development dynamics such as load balancing, congestion control, routing optimization, flow scheduling, and flow security. Furthermore, it provides an in-depth discussion of the strengths and limitations of existing strategies to equip researchers with a comprehensive understanding and framework for this field. Moreover, this article explores future research prospects and trends to guide further innovative work in meeting the ever-growing demands of DCN.
Keywords: Flow optimization; Load balancing; Congestion control; Routing optimization; Flow scheduling

Zhenchun Wei, Yang Zhao, Zengwei Lyu, Xiaohui Yuan, Yu Zhang, Lin Feng,
Cooperative caching algorithm for mobile edge networks based on multi-agent meta reinforcement learning,
Computer Networks,
Volume 242,
2024,
110247,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110247.
(https://www.sciencedirect.com/science/article/pii/S1389128624000793)
Abstract: Edge caching reduces service latency, relieves backhaul link traffic pressure, and enhances the quality of experience by multiplexing network content. Many studies use deep reinforcement learning (DRL) methods to develop edge caching policies. However, these traditional DRL methods have limitations, such as lengthy training time, poor model generalization, and the need to relearn network parameters for new tasks. To overcome these challenges, this paper proposes a multi-agent meta reinforcement learning-based cooperative edge caching algorithm (MAMRC), which consists of an inner and an outer model. The inner model employs the multi-agent deep reinforcement learning (MADRL) algorithm to implement cooperative caching for distributed base stations (BSs). It improves the cache hit rate and reduces service latency. The outer model uses a meta-learning method to learn meta-parameters and initialize the inner model, which enhances the generalization capability of the inner model, allowing it to rapidly adapt to new tasks. Experiment results indicate that, compared to the traditional DRL method and the MADRL-based algorithm, the inner model caching performance considering edge collaboration is improved by 15.35% and 4.55% respectively. Notably, compared to traditional caching algorithms and the inner model without initialization with meta-parameters, MAMRC demonstrates superior average caching performance and stronger generalization ability when facing new tasks.
Keywords: Mobile edge computing (MEC); Cooperative caching; Meta reinforcement learning (MRL); Multi-agent system

Tao Deng, Zhanwei Yu, Di Yuan,
Task offloading optimization in mobile edge computing under uncertain processing cycles and intermittent communications,
Computer Networks,
Volume 245,
2024,
110359,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110359.
(https://www.sciencedirect.com/science/article/pii/S1389128624001919)
Abstract: Mobile edge computing (MEC) has emerged as a promising solution for addressing the growing computational demands by enabling cloud computing capabilities at the network edge. However, existing MEC models typically make assumptions of known processing cycles and uninterrupted communications, which are not practical in real-world scenarios. This paper aims to tackle the challenges posed by uncertainties and intermittent communications in MEC systems in the context of task offloading. We first derive a closed-form expression for the average offloading success probability in a device-to-device (D2D) assisted MEC system. This expression accurately accounts for uncertain computation processing cycles and intermittent communications. Then, we formulate the task offloading maximization problem (TOMP) and prove its NP-hardness. For problem-solving, we propose two algorithms tailored to different scenarios. In instances exhibiting a uniform structure, we introduce a task scheduling algorithm based on dynamic programming (TSDP). When dealing with general scenarios, we reformulate the problem and propose a repeated matching algorithm (RMA). We use Monte Carlo simulations to validate the closed-form expression. The results demonstrate that gap between them is less than 0.55%, confirming the accuracy of the closed-form expression. Furthermore, the proposed algorithms outperform other algorithms by performance comparison.
Keywords: D2D; Dynamic programming; MEC; Intermittent communication; NP-hard; Repeated matching; Uncertain computation processing cycles

Dimitri Belli, Stefano Chessa, Antonio Corradi, Luca Foschini, Michele Girolami,
Optimization strategies for the selection of mobile edges in hybrid crowdsensing architectures,
Computer Communications,
Volume 157,
2020,
Pages 132-142,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.04.006.
(https://www.sciencedirect.com/science/article/pii/S0140366419317700)
Abstract: Communication infrastructures are rapidly evolving to support 5G enabling lower latency, high reliability, and scalability of the network and of the service provisioning. An important element of the 5G vision is Multi-access Edge Computing (MEC), that leverages the availability of powerful and low-cost middle boxes, i.e., MEC nodes, statically deployed at suitable edges of the network to extend the centralized cloud backbone. At the same time, after almost a decade of research, Mobile CrowdSensing (MCS) has established the technology able to collect sensing data on the environment by using personal devices, usually smartphones, as powerful sensing-and-communication platforms. Even though, mutual benefits due to the integration of MEC and Mobile CrowdSensing (MCS) are still largely unexplored. In this paper, we address and analyze the potential of the synergic use of MCS and MEC by thoroughly assessing various strategies for the selection of both traditional Fixed MEC (FMEC) edges as well as human-enabled Mobile MEC (M2EC) edges to support the collection of mobile CrowdSensing data. Collected results quantitatively show the effectiveness of the proposed optimization strategies in elastically scaling the load at edge nodes according to runtime provisioning needs.
Keywords: Mobile CrowdSensing; Multi-access edge computing; Clustering; Sensor data collection

Shijie Zhong, Songtao Guo, Hongyan Yu, Quyuan Wang,
Cooperative service caching and computation offloading in multi-access edge computing,
Computer Networks,
Volume 189,
2021,
107916,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107916.
(https://www.sciencedirect.com/science/article/pii/S1389128621000566)
Abstract: Multi-access edge computing (MEC) as an emerging and promising paradigm can alleviate the physical resource bottlenecks for smart mobile devices, involving storage and processing capacities. In the MEC system, the traffic load and the quality of service (QoS) can be improved through service caching. However, due to the highly coupled relationship between service caching and offloading decisions, it is extremely challenging to flexibly configure edge service cache within limited edge storage capacity to improve system performance. In this paper, we aim to minimize the average task execution time in the edge system by considering the heterogeneity of task requests, the pre-storage of the application data, and the cooperation of the base stations. Firstly, we formulate the problem of joint computation offloading, service caching, and resource allocation as a Mixed Integer Non-Linear Programming (MINLP) problem, which is difficult to solve because of the coupling relationship between optimization variables. Then we solve the MINLP problem by the decomposition theory based on Generalized Benders Decomposition. Moreover, we develop an efficient algorithm of cooperative service caching and computation offloading, called GenCOSCO, to improve QoS while reducing computation complexity. In particular, for special cases when the service cache configuration is fixed, the FixSC algorithm is proposed to derive the offloading strategy by cache replacement. Finally, numerous simulation experiments indicate that our proposed scheme can significantly reduce the average time consumption of task execution.
Keywords: Multi-access edge computing; Computation offloading; Service caching; Resource allocation; Generalized benders decomposition

Everton C. de Lima, Fábio D. Rossi, Marcelo C. Luizelli, Rodrigo N. Calheiros, Arthur F. Lorenzon,
A neural network framework for optimizing parallel computing in cloud servers,
Journal of Systems Architecture,
Volume 150,
2024,
103131,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103131.
(https://www.sciencedirect.com/science/article/pii/S1383762124000687)
Abstract: Energy efficiency has become a major focus in optimizing hardware resource usage for Cloud servers. One approach widely employed to enhance the execution of parallel applications is thread-level parallelism (TLP) exploitation. This technique leverages multiple threads to improve computational efficiency and performance. However, the increasing heterogeneity of resources in cloud environments and the complexity of selecting the optimal configuration for each application pose a significant challenge to cloud users due to the massive number of possible configurations and the need to effectively harness TLP in diverse hardware setups to achieve optimal energy efficiency and performance. To address this challenge, we propose TLP-Allocator, an artificial neural network (ANN) optimization strategy that uses hardware and software metrics to build and train an ANN model. It predicts worker node and thread count combinations that provide optimal energy-delay product (EDP) results. In experiments using ten well-known applications on a private cloud with heterogeneous resources, we show that TLP-Allocator predicts combinations that yield EDP values close to the best achieved by an exhaustive search. It also improves the overall EDP by 38.2% compared to state-of-the-art workloads scheduling on cloud environments.
Keywords: Parallel computing; Energy efficiency; Cloud computing; Artificial neural network

Shadab Mahboob, Koushik Kar, Jacob Chakareski, Md Ibrahim Alam,
CLoSER: Video caching in small-cell edge networks with local content sharing,
Computer Networks,
Volume 237,
2023,
110033,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110033.
(https://www.sciencedirect.com/science/article/pii/S1389128623004784)
Abstract: We consider the problem of video caching in an edge network consisting of a set of small-cell base stations (SBS) that can share content among themselves over a high-capacity short-delay local network, or fetch the videos from a remote server over a long-delay connection. Even though the problem of minimizing the overall video playout delay in our framework is NP-hard, we develop CLoSER, an algorithm that can efficiently compute a solution that is close to the optimal, where the degree of sub-optimality depends on the worst case video-to-cache size ratio. In comparison with related prior work on video caching and streaming, CLoSER specifically focuses on the benefits of local sharing of the initial portion of the video content in reducing the video playout delay, and provides strong optimality guarantees with a low-complexity algorithm. We extend CLoSER to an online setting where the video popularities are not known a priori but are estimated over time through a limited amount of periodic information sharing between SBSs. With such online video popularity estimation, a distributed implementation of CLoSER requires zero explicit coordination between the SBSs and runs in O(NK+KlogK) time, where N is the number of SBSs (caches) and K the maximum number of videos. We carry out simulations using YouTube and Netflix video request traces, as well as synthesized traces with the same marginal distributions as the traces but varying degree of temporal correlations, and demonstrate that our algorithm uses the SBS caches effectively to reduce the video delivery delay and conserve the remote server’s bandwidth. We also show that it outperforms two other reference caching methods adapted to our system setting, over a wide range of remote-to-local bandwidth ratios. Further, we show how CLoSER extends to the scenario where each video may need to be cached in multiple bit-rates, as in Available Bit Rate (ABR) streaming.
Keywords: Collaborative caching; Video streaming; Edge networks

Hayder sabah salih, Mustafa Musa Jaber, Mohammed Hasan Ali, Sura Khalil Abd, Ahmed Alkhayyat, R. Q Malik,
Application of edge computing-based information-centric networking in smart cities,
Computer Communications,
Volume 211,
2023,
Pages 46-58,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.09.003.
(https://www.sciencedirect.com/science/article/pii/S0140366423003122)
Abstract: Many data resources and network availability are needed for smart city applications to execute at their highest efficiency level. Demand for these objects is driving up data traffic, which in turn is placing strain on the network. The 5G-enabled Internet of Things applications address these difficulties in smart city applications. This article proposes an Information-centric Networking System using Multiaccess Edge Computing (ICNMEC) to reduce computation offloading and optimize data traffic. This system's 5G network slicing approaches combine edge computing and software characterization. Internet of Things applications have been used to store and analyze the information gathered. In addition, an algorithm known as OMNM (Optimized Memory Network Management) is created to control network traffic better and better use of storage. With minimal delays, network traffic, and storage ratio, the system's modelling tests demonstrate that it is very efficient. This method can progressively enhance the pace at which one can access and use the system. The performance assessment shows that the proposed method can improve the efficiency ratio of 95.141%, storage utilization ratio of 60.1% and access rate by 0.9, reducing network traffic and delay by 0.6.
Keywords: Edge computing; Networking; Information access; Smart city; Memory

Chunlin Li, Yong Zhang, Xiang Gao, Youlong Luo,
Energy-latency tradeoffs for edge caching and dynamic service migration based on DQN in mobile edge computing,
Journal of Parallel and Distributed Computing,
Volume 166,
2022,
Pages 15-31,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.03.001.
(https://www.sciencedirect.com/science/article/pii/S0743731522000569)
Abstract: Mobile edge computing sinks computing and storage capabilities to the edge of the network to provide reliable and low-latency services. However, the mobility of users and the limited coverage of edge servers can cause service interruptions and reduce service quality. A cooperative edge caching strategy based on energy-latency balance is proposed to solve high power consumption and latency caused by processing computationally intensive applications. In the cache selection phase, the request prediction method based on a deep neural network improves the cache hit rate. In the cache placement stage, the objective function is established by comprehensively considering power consumption and latency, and We use the branch-and-bound algorithm to get the optimal value. We propose an improved service migration method to solve the problem of service interruption caused by user movement. The service migration problem is modeled using a Markov decision process (MDP). The optimization goal is to reduce service latency and improve user experience under the premise of specified cost and computing resources. Finally, the optimal solution of the model is solved by the deep Q-Network (DQN) algorithm. Experiments show that our edge caching algorithm has lower latency and energy consumption than other algorithms in the same conditions. The service migration algorithm proposed in this paper is superior to different service migration algorithms in migration cost and success rate.
Keywords: Edge caching; Energy-latency tradeoffs; Dynamic service migration; Deep Q-Network

Chaowei Wang, Xiaofei Yu, Lexi Xu, Ziye Wang, Weidong Wang,
Multimodal semantic communication accelerated bidirectional caching for 6G MEC,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 225-237,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.036.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003594)
Abstract: Mobile Edge Computing (MEC) enables immersive XR with multimodal data by coordinating communication, computation, and caching (3C) resources upcoming 6G. The traditional communication constrained by Shannon’s theorem cannot accommodate the user demands for ultra-reliability and high throughput. In contrast, the semantic communication improves the quality of service and user experience by exploiting the semantic features. This paper constructs a multi-user MEC structure based on multimodal semantic communication for interactive AR/VR games. We construct a bidirectional caching task model to achieve cache-enhanced computing. To minimize the system cost, including the user latency, energy consumption, and storage size, we propose a content popularity-based DQN (CP-DQN) algorithm to make caching decisions. Then the CP-DQN is extended to the cache–computation coordination optimization algorithm (CCCA) to achieve the 3C resources tradeoff. Simulation results demonstrate that the proposed algorithm outperforms existing algorithms in terms of caching hit ratio, computation cost and edge resource utilization.
Keywords: Multimodal semantic communication; Deep reinforcement learning; Content popularity; Bidirectional caching; Mobile edge computing

Emna Baccour, Aiman Erbad, Kashif Bilal, Amr Mohamed, Mohsen Guizani,
PCCP: Proactive Video Chunks Caching and Processing in edge networks,
Future Generation Computer Systems,
Volume 105,
2020,
Pages 44-60,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.11.006.
(https://www.sciencedirect.com/science/article/pii/S0167739X18331674)
Abstract: Mobile Edge Computing (MEC) networks have been proposed to extend the cloud services and bring the cloud computing capabilities near the end-users at the Mobile Base Stations (MBS). To improve the efficiency of pushing the cloud features to the edge, different MEC servers assist each others to effectively select videos to cache and transcode. In this work, we adopt a joint caching and processing model for Video On Demand (VOD) in MEC networks. Our goal is to proactively cache only the chunks of videos to be watched and instead of caching the whole video content in one edge server (as performed in most of the previous works), neighboring MBSs will collaborate to store different video chunks to optimize the storage resources usage. Then, by coping with the Adaptive BitRate streaming technology (ABR), different representations of each chunk can be generated on the fly and cached in multiple MEC servers. To maximize the caching efficiency, we study the videos viewing pattern and design a Proactive caching Policy (PcP) and a Caching replacement Policy (CrP) to cache only highest probability video chunks. Servers performing caching and transcoding tasks should be thoroughly selected to optimize the storage and computing resources usage. Hence, we formulate this collaborative problem as a NP-hard Integer Linear Program (ILP). In addition to the CrP and PcP policies, we also propose a sub-optimal relaxation and an online heuristic, which are adequate for real-time chunks fetching. The simulation results prove that our model and policies perform more than 20% better than other edge caching approaches in terms of cost, average delay and cache hit ratio for different network configurations.
Keywords: Video chunks; Collaborative chunks caching; ABR; Edge network; Joint processing; Viewing pattern; Proactive caching

Hameeza Ahmed, Muhammad Faraz Hyder, Muhammad Fahim ul Haque, Paulo Cesar Santos,
Exploring compiler optimization space for control flow obfuscation,
Computers & Security,
Volume 139,
2024,
103704,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2024.103704.
(https://www.sciencedirect.com/science/article/pii/S0167404824000051)
Abstract: Code obfuscation is a promising technique for securing software and protecting it from adversaries. The objective is to harden the exploitation of security vulnerabilities for the attacker as well as launching of successful attacks. Obfuscation can be classified into layout, data, and control flow obfuscation. Control flow obfuscation impedes the understanding of the application logic by making it complicated to determine the actual control flows. Although numerous control flow methods exist in the literature, the role of existing compiler optimizations has just been discovered. This paper is the first one that explores the existing optimization space of LLVM compiler for obfuscating code. Our techniques optimally explore the native compiler's optimizations to improve the original code performance and reduce memory space with no disruptive efforts, tools, or extra costs. In the CBench benchmark suite, our work is able to improve 246%, 143%, and 468% in cyclomatic complexity, program length, and implementation effort, respectively, compared to unobfuscated code. Therefore, instead of inventing new obfuscation tools, the existing compiler optimizations can easily be used to obfuscate control flows, saving the overall cost and efforts.
Keywords: Control flow; Obfuscation; Compiler; Reverse engineering

Mande Xie, Xiangquan Su, Hao Sun, Guoping Zhang,
Online task offloading algorithm based on multi-objective optimization caching strategy,
Computer Networks,
Volume 245,
2024,
110400,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110400.
(https://www.sciencedirect.com/science/article/pii/S1389128624002329)
Abstract: Within the realm of Mobile Edge Computing (MEC), task offloading has consistently garnered significant attention. Within the context of intricate caching environments and multi-user scenarios, conventional solutions frequently encounter difficulties in simultaneously fulfilling the demands for latency reduction and energy consumption optimization. This paper presents a novel online task offloading algorithm that leverages a multi-objective optimization caching strategy. This algorithm addresses two challenges: the Online Task Offloading (OTO) problem and the Online Task File Caching (OTFC) problem. The OTO problem is conceptualized as a multi-user game, where Nash equilibrium is employed to effectively characterize and address it. This ensures the determination of the optimal offloading strategy in the presence of various caching scenarios. Meanwhile, the OTFC problem is transformed into a Markov decision process, and through the utilization of Deep Q-Networks, we can forecast the requirements of online tasks and subsequently determine the optimal caching vector. The incorporation of the Multi-Objective Cache Policy (MOCP) algorithm precedes the finalization of the caching vector. Rooted in multi-objective optimization, this algorithm adeptly balances various caching decisions, achieving a Pareto optimal outcome. The proposed offloading model that effectively caters to the requirements of task offloading while incorporating the demands of task file caching. Moreover, the MOCP algorithm ensures optimal caching decisions across a broad range of scenarios. Simulation tests reveal that this enhanced offloading algorithm, grounded in multi-objective optimization, outperforms traditional methods in energy conservation, boasting energy savings of up to 15%.
Keywords: Computation offloading; Online tasks; Pareto optimal; Deep Q-network

Ruidong Zhang, Jiadong Zhang, Wenxiao Shi,
Partial offloading in device-to-device-assisted MEC network: A utility optimization approach,
Computer Communications,
Volume 209,
2023,
Pages 26-37,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.06.022.
(https://www.sciencedirect.com/science/article/pii/S0140366423002207)
Abstract: In order to meet the growing demand for faster and more efficient communication and computing in internet of things networks, the device-to-device (D2D)-assisted multi-access edge computing (MEC) network has emerged as a promising solution. However, most existing literature overlooks the monetary cost associated with computing and communication resources when it comes to computation offloading. Motivated by this, we first formulate a novel utility function which is defined as the tradeoff between the benefit of the delay reduction via partial offloading and the cost of the purchased computing and communication resources. Then, we propose the partial offloading-based utility optimization (POUO) algorithm to solve the utility optimization problem. Afterward, we determine the resource allocation policy and offloading ratios while taking into account the constraints of the utility optimization problem. In addition, the offloading pairing decision in the device-to-edge server (D2E) offloading mode is modeled as a knapsack problem and solved using a greedy algorithm. For D2D offloading mode, we transform the offloading pairing decision into an assignment problem and apply the Hungarian algorithm to solve it. Finally, we validate and evaluate the proposed POUO algorithm across a range of system parameters. Simulation results show that the POUO algorithm improves the completed tasks ratio, the total utility, and the average delay.
Keywords: Partial offloading; Device-to-device communication; Multi-access edge computing; Utility optimization; Resource allocation; Knapsack problem; Hungarian algorithm; Heuristic algorithm

Guowen Wu, Xihang Chen, Zhengjun Gao, Hong Zhang, Shui Yu, Shigen Shen,
Privacy-preserving offloading scheme in multi-access mobile edge computing based on MADRL,
Journal of Parallel and Distributed Computing,
Volume 183,
2024,
104775,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104775.
(https://www.sciencedirect.com/science/article/pii/S0743731523001454)
Abstract: With the development of industrialization and intelligence, the Industrial Internet of Things (IIoT) has gradually become the direction for traditional industries to transform into modern ones. In order to adapt to the emergence of a large number of edge access devices such as sensors, as well as the demand for high-consumption and low-latency computing tasks, Mobile Edge Computing (MEC) has been proposed as an effective paradigm by the academic community. Users can offload tasks to MEC servers, greatly reducing the computing latency and energy consumption when using services. However, traditional single access point edge computing networks cannot meet the usage requirements of a large number of users. In addition, the privacy leakage issues arising from the offloading process are also easily overlooked. In this paper, we propose a privacy-preserving offloading scheme based on stochastic game theory considering multiple access points. We first construct a multi-access point offloading framework and quality of service (QoS) model. In terms of privacy, the privacy risks caused by the offloading preferences of different edge nodes are studied, and the privacy entropy is used to evaluate the privacy protection level. We comprehensively consider the energy consumption, latency requirements, user experience, and privacy protection of the system and formulate the problem as a Markov Decision Process (MDP). Finally, a joint optimal DRL algorithm with privacy preservation (JODRL-PP) is proposed to achieve the optimal offloading scheme of the system. Simulation results verify the effectiveness of our model and algorithm.
Keywords: Mobile edge computing; Computation offloading; Privacy-preservation; Multi-agent DRL; Resource allocation

Mostafa Taghizade Firouzjaee, Kamal Jamshidi, Neda Moghim, Sachin Shetty,
User preference-aware content caching strategy for video delivery in cache-enabled IoT networks,
Computer Networks,
Volume 240,
2024,
110142,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110142.
(https://www.sciencedirect.com/science/article/pii/S138912862300587X)
Abstract: The escalating growth of content-dependent services and applications within the Internet of Things (IoT) platform has led to a surge in traffic, necessitating real-time data processing. Content caching has emerged as an effective solution to counteract this traffic upswing. Caching not only improves network efficiency but also enhances user service quality. Critical to the development of an optimal caching algorithm is the accurate prediction of future content popularity. This prediction hinges on the ability to anticipate users' content preferences, which is a pivotal method for assessing content popularity. In this study, we introduce a novel caching strategy termed User Preference-aware content Caching Strategy (UPCS) tailored for an IoT platform, where users access multimedia services offered by remote Content Providers (CPs). The UPCS encompasses three key algorithms: a content popularity prediction algorithm that utilizes Variational Autoencoders (VAE) to forecast users' future content preferences based on their prior requests, an online algorithm for dynamic cached content replacement, and a cooperative caching algorithm to augment caching efficiency. The proposed content caching strategy outperforms alternative methods, exhibiting superior cache hit rates and reduced Content Retrieval Delays (CRD).
Keywords: Content caching; Cooperative caching; IoT Networks; Popularity prediction; Deep learning; Variational autoencoder

Binbin Huang, Xiao Liu, Yuanyuan Xiang, Dongjin Yu, Shuiguang Deng, Shangguang Wang,
Reinforcement learning for cost-effective IoT service caching at the edge,
Journal of Parallel and Distributed Computing,
Volume 168,
2022,
Pages 120-136,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.06.008.
(https://www.sciencedirect.com/science/article/pii/S0743731522001423)
Abstract: In the edge computing environment, Internet of Things (IoT) application service providers can rent resources from edge servers to cache their service items such as datasets and code libraries, and thus significantly reducing the service request latency and the core network traffic. Since IoT service providers need to pay for the rented edge computing resources, it is essential to find a dynamical service caching strategy to minimize the service cost while optimizing the performance objective such as service latency reduction. However, most of the existing studies either overlooked the problem of collaborative service caching or failed to consider the system's long-term service cost and latency. In this paper, to address such a problem, we coordinate multiple edge servers to cache service items and formulate the collaborative service caching problem using a multi-agent multi-armed bandit model. Furthermore, we propose a utility-aware collaborative service caching (UACSC) scheme based on a multi-agent reinforcement learning. The UACSC scheme can coordinate multiple edge servers to make a dynamic joint caching decision, aiming at maximizing the system's long-term utility. To evaluate the performance of our proposed scheme, we implement four representative baseline algorithms and compare them with six different performance metrics. In addition, a real-world case study is also presented to demonstrate the effectiveness of the UACSC scheme. Comprehensive experimental results show that the UACSC scheme can effectively coordinate multiple edge servers to cache service items, and achieve higher service latency reduction and lower service cost compared with other baseline algorithms.
Keywords: Edge computing; Collaborative service caching; Multi-agent multi-armed bandit model; Multi-agent reinforcement learning

Ziyu Peng, Gaocai Wang, Wang Nong, Yu Qiu, Shuqiang Huang,
Task offloading in Multiple-Services Mobile Edge Computing: A deep reinforcement learning algorithm,
Computer Communications,
Volume 202,
2023,
Pages 1-12,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.02.001.
(https://www.sciencedirect.com/science/article/pii/S0140366423000385)
Abstract: Multiple-Services Mobile Edge Computing enables task-relate services cached in edge server to be dynamically updated, and thus provides great opportunities to offload tasks to edge server for execution. However, the requirements and popularity of services, the computing requirement and the amount of data transferred from users to edge server are dynamic with time. How to adaptively adjust the subset of total service types in the resource-limited edge server and determine the task offloading destination and resource allocation decisions to improve the overall system performance is a challenge problem. To solve this challenge, we firstly convert it into a Markov decision process, then propose a soft actor–critic deep reinforcement learning-based algorithm, called DSOR, to jointly determine not only the discrete decisions of service caching and task offloading but also the continuous allocation of bandwidth and computing resource. To improve the accuracy of our algorithm, we employ an efficient trick of converting the discrete action selection into a continuous space to deal with the key design challenge that arises from continuous-discrete hybrid action space. Additionally, to improve resource utilization, a novel reward function is integrated to our algorithm to speed up the convergence of training while making full use of system resources. Extensive numerical results show that compared with other baseline algorithms, our algorithm can effectively reduce the long-term average completion delay of tasks while accessing excellent performance in terms of stability.
Keywords: Mobile edge computing; Service caching; Task offloading; Resource allocation; Deep reinforcement learning

Hao Hao, Wei Ding, Wei Zhang,
Time-continuous computing offloading algorithm with user fairness guarantee,
Journal of Network and Computer Applications,
Volume 223,
2024,
103826,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103826.
(https://www.sciencedirect.com/science/article/pii/S1084804524000031)
Abstract: Computing offloading is a potential avenue to reduce transmission delay by moving computing tasks from cloud to edge nodes. Due to the limited computing capacity of edge nodes, performing effective computing offloading strategy is challenging. To simplify the problem, it is often assumed that the timeline is discrete and offloading decisions are only made at the end of each time slot. However, the assumption results in decision waiting time and increases service delay. In this paper, we first formulate a time-continuous computing offloading problem without the assumption of discrete timeline. In order to reduce service delay while guaranteeing user fairness, the optimization objective is constructed as the α-fair utility function of average task delay rather than just average task delay. Then, we propose a novel algorithm by adjusting the reward in standard reinforcement learning to solve this problem, and also prove the convergence of our algorithm theoretically. To the best of our knowledge, this paper is the first attempt to study the time-continuous computing offloading problem with fairness. Evaluations show that the proposed algorithm has better performance in terms of service delay and user fairness compared to five baselines.
Keywords: Mobile edge computing; Service offloading; Deep reinforcement learning

Javier Panadero, Mennan Selimi, Laura Calvet, Joan Manuel Marquès, Felix Freitag,
A two-stage Multi-Criteria Optimization method for service placement in decentralized edge micro-clouds,
Future Generation Computer Systems,
Volume 121,
2021,
Pages 90-105,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.03.013.
(https://www.sciencedirect.com/science/article/pii/S0167739X21000935)
Abstract: Community networks are becoming increasingly popular due to the growing demand for network connectivity in both rural and urban areas. Community networks are owned and managed at the edge by volunteers. Their irregular topology, the heterogeneity of resources and their unreliable behavior claim for advanced optimization methods to place services in the network. In particular, an efficient service placement method is key for the performance of these systems. This work presents the Multi-Criteria Optimal Placement method, a novel and fast two-stage multi-objective method to place services in decentralized community network edge micro-clouds. A comprehensive set of computational experiments is carried out using real traces of Guifi.net, which is the largest production community network worldwide. According to the results, the proposed method outperforms both the random placement method used currently in Guifi.net and the Bandwidth-aware Service Placement method, which provides the best known solutions in the literature, by a mean gap in bandwidth gain of about 53% and 10%, respectively, while it also reduces the number of resources used.
Keywords: Service placement; Distributed systems; Community networks; Micro-clouds; Multi-objective optimization algorithms

Mobasshir Mahbub, Raed M. Shubair,
Contemporary advances in multi-access edge computing: A survey of fundamentals, architecture, technologies, deployment cases, security, challenges, and directions,
Journal of Network and Computer Applications,
Volume 219,
2023,
103726,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103726.
(https://www.sciencedirect.com/science/article/pii/S1084804523001455)
Abstract: With advancements of cloud technologies Multi-Access Edge Computing (MEC) emerged as a remarkable edge-cloud technology to provide computing facilities to resource-restrained edge user devices. Utilizing the features of MEC user devices can obtain computational services from the network edge which drastically reduces the transmission latency of evolving low-latency applications such as video analytics, e-healthcare, etc. The objective of the work is to perform a thorough survey of the recent advances relative to the MEC paradigm. In this context, the work overviewed the fundamentals, architecture, state-of-the-art enabling technologies, evolving supporting/assistive technologies, deployment scenarios, security issues, and solutions relative to the MEC technology. The work, moreover, stated the relative challenges and future directions to further improve the features of MEC.
Keywords: MEC; IoT; Digital twin; Open-source MEC; Quantum computing; AI; Security

Weibo Chu, Xiaoyan Zhang, Xinming Jia, John C.S. Lui, Zhiyong Wang,
Online optimal service caching for multi-access edge computing: A constrained Multi-Armed Bandit optimization approach,
Computer Networks,
Volume 246,
2024,
110395,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110395.
(https://www.sciencedirect.com/science/article/pii/S1389128624002275)
Abstract: In order to fully exploit the power of Multi-Access Edge Computing, services need to be cached at the network edge in an adaptive and responsive way to accommodate the high system dynamics and uncertainty. In this paper, we study the online service caching problem in MEC, with the goal to minimize users’ perceived latency while at the same time, ensure the rate of tasks processed by the edge server is no less than a preset threshold. We model the problem with a Constrained stochastic Multi-Armed Bandit formulation, and propose a simple yet effective online caching algorithm called Constrained Confidence Bound (CCB). CCB achieves O(TlnT) bounds on both regret and violation of the constraint, and is able to achieve a good balance between them. We further consider the scenario when there is cost (i.e., delay) due to service switches, and propose two service switch-aware caching algorithms — Explore-First (EF) and Successive Elimination-based (SE) caching, together with a novel sampling scheme. We prove that EF achieves O(T23(lnT)13) bound on regret and violation, whereas SE achieves O(TlnT) and converges significantly faster. Lastly, we conduct extensive simulations to evaluate our algorithms and results demonstrate their superior performance over baselines.
Keywords: Multi-access edge computing; Service selection; Service caching; Constrained multi-armed bandit; Online algorithm

Chuan Feng, Pengchao Han, Xu Zhang, Bowen Yang, Yejun Liu, Lei Guo,
Computation offloading in mobile edge computing networks: A survey,
Journal of Network and Computer Applications,
Volume 202,
2022,
103366,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103366.
(https://www.sciencedirect.com/science/article/pii/S1084804522000327)
Abstract: Computation offloading is one of the key technologies in Mobile Edge Computing (MEC), which makes up for the deficiencies of mobile devices in terms of storage resource, computing capacity, and energy efficiency. On one hand, computation offloading of task requests not only relieves the communication pressure on the core networks but also reduces the delay caused by long-distance data transmission. On the other hand, emerging applications in 5/6G also rely on the computation offloading technology for efficient service provisioning to users. At present, the industry and academia have conducted a lot of researches on the computation offloading methods in MEC networks with a diversity of meaningful techniques and approaches. In this paper, we present a comprehensive survey of the computation offloading in MEC networks including applications, offloading objectives, and offloading approaches. Particularly, we discuss key issues on various offloading objectives, including delay minimization, energy consumption minimization, revenue maximization, and system utility maximization. The approaches to achieve these objectives mainly include mathematical solver, heuristic algorithms, Lyapunov optimization, game theory, and Markov Decision Process (MDP) and Reinforcement Learning (RL). We compare the approaches by characterizing their pros and cons as well as targeting applications. Finally, from the four aspects of subtasks dependency and online task requests, server selection, real-time environment perception, and security, we analyze the current challenges and future directions of computation offloading in MEC networks.
Keywords: Computation offloading; Mobile edge computing; Resource allocation; Networks optimization

Yung-Ting Chuang, Yuan-Tsang Hung,
A real-time and ACO-based offloading algorithm in edge computing,
Journal of Parallel and Distributed Computing,
Volume 179,
2023,
104703,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.04.004.
(https://www.sciencedirect.com/science/article/pii/S0743731523000667)
Abstract: With the increasingly widespread use of networks and end devices, more and more data and computations must be processed. With processing constrained by the limited resources of the end device, edge computing plays an important role. Edge computing offloads computation to surrounding edge nodes with corresponding computing capabilities so that the end device can get a response within a reasonable latency to meet the user's needs. Since these edge nodes are composed of multiple heterogeneous computing units, any system's task-offloading strategy must necessarily affect the system's load balance and execution time. This study proposes a real-time, two-stage ant colony algorithm (RTACO) with the following goals: 1) the algorithm requires low latency; 2) the algorithm minimizes the makespan of all tasks; 3) the algorithm optimizes the system load and reduces the burden of the task-offloading algorithm, thereby providing a stable and high-performance edge computing system. Experiments show that RTACO requires low execution time, and can still effectively achieve good results even when the system has limited resources.
Keywords: Task offloading; Edge computing

Senjiong Zheng, Bo Liu, Weiwei Lin, Xiaoying Ye, Keqin Li,
A package-aware scheduling strategy for edge serverless functions based on multi-stage optimization,
Future Generation Computer Systems,
Volume 144,
2023,
Pages 105-116,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.02.013.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000547)
Abstract: Serverless computing offers a promising deployment model for edge IoT applications. However, serverless functions that rely on large libraries suffer from severe library loading latency when containerized, which is unfriendly to edge latency-sensitive applications. Most function offload strategies in edge environments ignore the impact of this latency. We also found that the measures taken by serverless platforms to reduce loading latency may not work in edge environments. To remedy that, this paper proposes a function offloading strategy to minimize loading latency, a new way to deeply integrate placement optimization with cache optimization. In this way, we first design a package caching policy suitable for edge environments based on the consistency of execution topology. Then a Double Layers Dynamic Programming algorithm (DLDP) is proposed to solve the problem of function offloading considering the dependent packages using a multi-stage progressive optimization approach. The caching policy is embedded in the scheduling algorithm through a phased optimization approach to achieve joint optimization. Extensive experiments on the cluster trace from Alibaba show that DLDP reduces the loading latency of packages by more than 97.84% and significantly outperforms four baselines in the application completion time by more than 55.67%.
Keywords: Serverless function offloading; Dependency package awareness; Package caching strategy

Adyson Maia, Akram Boutouchent, Youcef Kardjadja, Manel Gherari, Ece Gelal Soyak, Muhammad Saqib, Kacem Boussekar, Idil Cilbir, Sama Habibi, Soukaina Ouledsidi Ali, Wessam Ajib, Halima Elbiaze, Ozgur Erçetin, Yacine Ghamri-Doudane, Roch Glitho,
A survey on integrated computing, caching, and communication in the cloud-to-edge continuum,
Computer Communications,
Volume 219,
2024,
Pages 128-152,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.03.005.
(https://www.sciencedirect.com/science/article/pii/S0140366424000847)
Abstract: Cloud and edge computing have proposed different functionalities to enable multiple applications requiring different communication, computing, and caching (3C) resources. The upcoming futuristic applications (e.g., metaverse, holographic, and haptic communication) impose further stringent requirements (e.g., ultra-low latency, ultra-high reliability) on the infrastructure. These requirements call for a paradigm shift in the infrastructure architecture where all resource components and owners collaborate from the cloud up to the edge, creating a cloud-to-edge continuum of integrated resources. Furthermore, we argue that artificial intelligence (AI) and collaborative-based decisions are promising techniques to efficiently manage the highly complex architecture that jointly leverages 3C in the continuum. This article presents a comprehensive survey of existing research, including AI and collaborative-based studies, targeting the effective and seamless provision of 3C resources and services in the cloud-to-edge continuum. Through an extensive analysis of driving use cases, the synergy between these three main services is scrutinized to highlight its crucial role in the next-generation network infrastructures (NGNI). Finally, a discussion on the opportunities and challenges brought by integrating 3C in NGNI from different perspectives, including architectural design as well as the regulatory and business aspects, are presented.
Keywords: Communication; Computing; Caching; Cloud-to-edge continuum; Next-generation network infrastructures

Zhongfu Guo, Xinsheng Ji, Wei You, Mingyan Xu, Yu Zhao, Zhimo Cheng, Deqiang Zhou,
Delay optimal for reliability-guaranteed concurrent transmissions with raptor code in multi-access 6G edge network,
Computer Networks,
Volume 228,
2023,
109716,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109716.
(https://www.sciencedirect.com/science/article/pii/S1389128623001615)
Abstract: This paper proposes RCMEN, a solution for Concurrent Multipath Transfer (CMT) of Raptor-Coded (RC) Protocol Data Units (PDU) in a Multi-Access 6G Edge Network (MEN) to provide low-latency and reliable transmission for 6G User Equipment (UE) and Data Network (DN). While 3GPP Release 16 aims to improve the reliability of a single service based on CMT, replication-based reliability causes high resource and signaling overhead, and asymmetric bandwidth and latency lead to excessive latency. RCMEN addresses these issues by distributing data across multiple PDU sessions and using a queue-aware variable-block-length Raptor encoder to optimize resource utilization under random arrivals. We analyze RCMEN’s delay-reliability performance using the Constrained Markov Decision Process (CMDP), reliability is guaranteed by the reliability function, which takes the amount of data sent as a parameter. Through Linear Programming, we obtain the optimal delay-reliability trade-off and optimize delay metrics for two classical RCMEN. Experiments show that both classical RCMEN optimize the long-tail distribution of latency while ensuring reliability, with the entire system being compatible with the current 5G network infrastructure.
Keywords: 6G core network; Reliable communication; Concurrent multipath transmissions; Multi-access edge network; Raptor codes

Dongjae Kim, Dong-Wook Seo, Minseok Choi,
Edge caching and computing of video chunks in multi-tier wireless networks,
Journal of Network and Computer Applications,
Volume 226,
2024,
103889,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103889.
(https://www.sciencedirect.com/science/article/pii/S1084804524000663)
Abstract: This paper proposes a video caching and transcoding strategy for delay-constrained content delivery in multi-tier wireless networks. Particularly for multimedia services whose content can be encoded into multiple quality versions and consists of multiple chunks, we present an approach of caching chunks of an identical file separately in different network layers with different qualities. Given this caching policy, a complete file can be retrieved by receiving its chunks from different network tiers; therefore, a video transcoding and delivery method is jointly designed with caching to maximize the expected content quality with a strict delay constraint. Motivated by that caching is proactively conducted in advance of many user requests, the joint optimization problem is decomposed into caching and transcoding subproblems. The optimal transcoding policy and its transcoded qualities at different network tiers are analytically found by linear programming for a given cache state. Using the closed-form transcoded qualities, the caching policy is optimized to maximize the expected content quality with respect to randomness of user locations and channel gains. Due to the difficulty of expectation, the caching policy of each network layer is independently obtained by water-filling first, and an alternating optimization method is used to find the joint optimal caching policy satisfying the cache size constraints. Simulation results show that the proposed scheme succeeds in delivering content of 5% to 10% higher expected quality to users than comparison schemes which do not use partitioning for caching. Simultaneously, our approach meets the delay requirement and effectively manages the trade-offs arising from the varying storage and computational capacities within hierarchies, as well as video diversity.
Keywords: Wireless caching; Edge computing; Multi-tier computing networks

Xincao Xu, Kai Liu, Penglin Dai, Feiyu Jin, Hualing Ren, Choujun Zhan, Songtao Guo,
Joint task offloading and resource optimization in NOMA-based vehicular edge computing: A game-theoretic DRL approach,
Journal of Systems Architecture,
Volume 134,
2023,
102780,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102780.
(https://www.sciencedirect.com/science/article/pii/S138376212200265X)
Abstract: Vehicular edge computing (VEC) becomes a promising paradigm for the development of emerging intelligent transportation systems. Nevertheless, the limited resources and massive transmission demands bring great challenges on implementing vehicular applications with stringent deadline requirements. This work presents a non-orthogonal multiple access (NOMA) based architecture in VEC, where heterogeneous edge nodes are cooperated for real-time task processing. We derive a vehicle-to-infrastructure (V2I) transmission model by considering both intra-edge and inter-edge interferences and formulate a cooperative resource optimization (CRO) problem by jointly optimizing the task offloading and resource allocation, aiming at maximizing the service ratio. Further, we decompose the CRO into two subproblems, namely, task offloading and resource allocation. In particular, the task offloading subproblem is modeled as an exact potential game (EPG), and a multi-agent distributed distributional deep deterministic policy gradient (MAD4PG) is proposed to achieve the Nash equilibrium. The resource allocation subproblem is divided into two independent convex optimization problems, and an optimal solution is proposed by using a gradient-based iterative method and KKT condition. Finally, we build the simulation model based on real-world vehicular trajectories and give a comprehensive performance evaluation, which conclusively demonstrates the superiority of the proposed solutions.
Keywords: Vehicular edge computing; Real-time task offloading; Heterogeneous resource allocation; Deep reinforcement learning; Exact potential game

Ying Sai, Dong-zhu Fan, Meng-yang Fan,
Cooperative and efficient content caching and distribution mechanism in 5G network,
Computer Communications,
Volume 161,
2020,
Pages 183-190,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.07.030.
(https://www.sciencedirect.com/science/article/pii/S0140366420318429)
Abstract: This paper considers encoding caching and collaborative distribution issues jointly, and proposes a collaborative and efficient content caching and distribution mechanism; and based on the proposed scheme, an energy consumption model is established from the perspective of content caching and content distribution to optimize Energy efficiency of content caching and content distribution. In addition, this paper proposes a cache placement algorithm based on heuristic greedy algorithm to solve this energy efficiency optimization problem by optimizing cache placement. Finally, the simulation results show the performance of the scheme. This paper proposes a content clustering distribution scheme for the problems of random user distribution and limited cache. This solution uses the user’s media cloud and communication technology to complete the distribution of video content. In addition, the use of an efficient adaptive media cloud clustering mechanism and a minimized hops algorithm has enabled the reasonable deployment of 5G network caches based on the popularity of video content and user distribution. The simulation results show that the system content request response time is shortened by 50%, which effectively reduce the impact of content popularity changes on system performance. This paper analyses the important performance indicators such as cache hit rate, optimal communication radius, and maximum link access in 5G networks for multi-layer popular multimedia content distribution in response to the heterogeneity and transmission interference problems for 5G networks in 5G communications. Under the premise of noise ratio and signal-to-interference ratio threshold, related service link selection and scheduling algorithms is further proposed obtaining simulation results that approximate theoretical analysis, which effectively reduces the number of interference by 40% and improves the system throughput by 10%.
Keywords: 5G network; Content caching; Distribution mechanism; Efficient collaboration

Long Chen, Jigang Wu, Jun Zhang,
Long-term optimization for MEC-enabled HetNets with device–edge–cloud collaboration,
Computer Communications,
Volume 166,
2021,
Pages 66-80,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.11.011.
(https://www.sciencedirect.com/science/article/pii/S0140366420319915)
Abstract: For effective computation offloading with multi-access edge computing (MEC), both communication and computation resources should be properly managed, considering the dynamics of mobile users such as the time-varying demands and user mobility. Most existing works regard the remote cloud server as a special edge server. However, service quality cannot be met when some of the edge servers cannot be connected. Besides, the computation capability of the cloud has not been fully exploited especially when edge servers are congested. We develop an on-line offloading decision and computational resource management algorithm with joint consideration of collaborations between device–cloud, edge–edge and edge–cloud. The objective is to minimize the total energy consumption of the system, subject to computational capability and task buffer stability constraints. Lyapunov optimization technique is used to jointly deal with the delay-energy trade-off optimization and load balancing. The optimal CPU-cycle frequencies, best transmission powers and offloading scheduling policies are jointly handled in the three-layer system. Extensive simulation results demonstrate that, with V varies in [0.1,5]×109, the proposed algorithm can save more than 50% energy and over 120% task processing time than three existing benchmark algorithms averagely.
Keywords: Long-term; Offloading; Edge computing; HetNet; Lyapunov; Collaboration

Shi Yang,
A joint optimization scheme for task offloading and resource allocation based on edge computing in 5G communication networks,
Computer Communications,
Volume 160,
2020,
Pages 759-768,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.07.008.
(https://www.sciencedirect.com/science/article/pii/S0140366420307167)
Abstract: With the development of 5G communication networks and the popularization of intelligent terminals, the computing resource intensive characteristic of various new applications poses a severe challenge to the task processing ability of intelligent terminals. In order to improve the efficiency of task processing, a joint optimization scheme for task offloading and resource allocation based on edge computing in 5G communication networks is proposed. Firstly, combining edge computing and Device-to-Device communication technologies, we propose three modes for processing computationally intensive tasks based on multi-user network system model for 5G edge networks, including local computing, fog node computing, edge node computing. Then, the corresponding time delay model, task execution model and offloading energy consumption computing model are constructed for these three computing modes. Finally, the problem of computing task offloading is transformed into a joint optimization problem of time delay and energy consumption, including optimization problems such as CPU frequency, offloading decision, transmission bandwidth allocation and power allocation of offloading users. Besides, the interior point method is utilized to solve this problem. Simulation platform is used to demonstrate the performance of our proposed scheme. The experimental results show that the scheme can effectively reduce the time delay and energy consumption of terminal tasks, which improves the efficiency of task processing and the experience quality of end users.
Keywords: 5G communication network; Edge computing; Task offloading; Resource allocation; Joint optimization of time delay and energy consumption; Device-to-Device (D2D)

He Xue, Dajiang Chen, Ning Zhang, Hong-Ning Dai, Keping Yu,
Integration of blockchain and edge computing in internet of things: A survey,
Future Generation Computer Systems,
Volume 144,
2023,
Pages 307-326,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.029.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003521)
Abstract: As an important technology to ensure data security, consistency, traceability, etc., blockchain has been increasingly used in Internet of Things (IoT) applications. The integration of blockchain and edge computing (IBEC) can further improve the resource utilization in terms of network, computing, storage, and security. This paper aims to present a survey on the IBEC. In particular, we first give an overview of blockchain and edge computing respectively. We then present a general architecture of an IBEC system. We next study the various applications of the IBEC in IoT. We also discuss the optimizations of the IBEC system and solutions from perspectives of resource management and performance improvement. Finally, we analyze and summarize the existing challenges posed by the IBEC system and the potential solutions in the future.
Keywords: Blockchain; Edge computing; Internet of things; Resource management; Security and privacy; Data management

Luying Wang, Anfeng Liu, Neal N. Xiong, Shaobo Zhang, Tian Wang, Mianxiong Dong,
SD-SRF: An Intelligent Service Deployment Scheme for Serverless-operated Cloud-Edge Computing in 6G Networks,
Future Generation Computer Systems,
Volume 151,
2024,
Pages 242-259,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.09.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X2300359X)
Abstract: With the development of serverless computing, developers can implement and deploy business applications as a combination of stateless functions. Although originally proposed for cloud computing, serverless computing is gradually applied to cloud-edge systems for service deployment to provide users with high-quality, low-latency services. However, optimized service deployment in 6G networks is a very challenging issue because of the vast number of deployable devices in the network, and its permutations are highly exponential. In this paper, we propose optimized service deployment schemes for online and offline, respectively, to minimize the overall latency at a lower cost. (1) First, a SD-SRF algorithm based on the greedy algorithm is proposed to optimize the service deployment for a multi-layer edge network, which consists of two phases: SR and SF. (a) Services are deployed in the nearest ancestor devices in the routing tree of all such service requests with the least cost of deployment. (b) When the deployment cost of moving some service replicas to devices in the lower layer is less than the benefit, the service will fall. (2) However, the offline algorithm relies on the availability of prior information heavily, such as request arrival pattern and number, which is difficult to obtain. Therefore, this paper proposes a PPO-MSD algorithm for optimal deployment online, where a Markov decision process (MDP) is modeled. Extensive simulation results show that PPO-MSD outperforms existing algorithms in terms of overall delay and utility, and its performance is close to the optimal ones obtained by SD-SRF, with the SD-SRF and PPO-MSD algorithms reducing the delay on average by 32.40% and 9.91%.
Keywords: Cloud-Edge computing; Serverless computing; Reinforcement learning; Intelligent service deployment; 6G networks

Shashwat Kumar, Sai Vineeth Doddala, A. Antony Franklin, Jiong Jin,
RAN-aware adaptive video caching in multi-access edge computing networks,
Journal of Network and Computer Applications,
Volume 168,
2020,
102737,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102737.
(https://www.sciencedirect.com/science/article/pii/S1084804520302113)
Abstract: Videos are expected to be a primary contributor to an anticipated massive surge in mobile network data. Caching the videos within the mobile network can significantly reduce the network load and Operational Expenditure (OPEX) for mobile network operators. Multi-access Edge Computing (MEC) can enable the video caching by providing processing and storage capabilities within the network. However, content providers usually employ Dynamic Adaptive Streaming over HTTP (DASH) for video streaming, which contains multiple bit-rate representations of videos. Constrained by its capacity, MEC can not cache all representations of popular videos. Video transcoding mitigates this issue to a certain extent by converting the higher available video bit-rate to a requested lower one; but, it can quickly exhaust the available edge processing power by transcoding a large number of videos in parallel. Therefore, caching appropriate video bit-rates that can serve the maximum number of users in the network is a non-trivial problem. To resolve this problem and to efficiently utilize the resources (processing and storage) at the network edge, we took a non-traditional approach for video caching that utilizes the network information provided by MEC's Radio Network Information (RNI) Application Program Interface (API). In particular, RNI API provides Radio Access Network (RAN) status information that can be employed to estimate the probability distribution of requested video qualities. In this work, we formulate the video caching problem as an Integer Linear Programming (ILP) for the hit-rate maximization. Since the optimization problem requires the knowledge of all future requests, it obviously cannot be used in real-time. Therefore, we develop a RAN-aware Adaptive VidEo cachiNg (RAVEN) method that uses network information to make an informed decision for video bit-rate selection in video caching coupled with transcoding and maximizes the number of served users form the network edge. Simulation results demonstrate that the RAVEN significantly outperforms state-of-the-art algorithms in the domain and performs closer to the optimal solution.
Keywords: Multi-access Edge Computing (MEC); Radio Network Information (RNI); Video caching

Sana Sharif, Sherali Zeadally, Waleed Ejaz,
Space-aerial-ground-sea integrated networks: Resource optimization and challenges in 6G,
Journal of Network and Computer Applications,
Volume 215,
2023,
103647,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103647.
(https://www.sciencedirect.com/science/article/pii/S1084804523000668)
Abstract: Space–air–ground–sea integrated (SAGSI) networks are envisioned to connect satellite, aerial, ground, and sea networks to provide connectivity everywhere and all the time in sixth-generation (6G) networks. However, the success of SAGSI networks is constrained by several challenges including resource optimization when the users have diverse requirements and applications. We present a comprehensive review of SAGSI networks from a resource optimization perspective. We discuss use case scenarios and possible applications of SAGSI networks. The resource optimization discussion considers the challenges associated with SAGSI networks. In our review, we categorized resource optimization techniques based on throughput and capacity maximization, delay minimization, energy consumption, task offloading, task scheduling, resource allocation or utilization, network operation cost, outage probability and the average age-of-information, joint optimization (data rate difference, storage or caching, CPU cycle frequency), the overall performance of network and performance degradation, software-defined networking, and intelligent surveillance and relay communication. Finally, we discuss challenges and future research directions in SAGSI networks.
Keywords: 6G; Aerial networks; Integrated networks; Maritime networks; Resource optimization; SAGSI networks; Terrestrial networks

Xiaoheng Deng, Jun Li, Enlu Liu, Honggang Zhang,
Task allocation algorithm and optimization model on edge collaboration,
Journal of Systems Architecture,
Volume 110,
2020,
101778,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101778.
(https://www.sciencedirect.com/science/article/pii/S1383762120300722)
Abstract: This paper investigates a mobile edge computing environment for video analysis tasks where edge nodes provide their computation capacities to process the computation intensive tasks submitted by end users. First, we introduce a Cloudlet Assisted Cooperative Task Assignment (CACTA) system that organizes edge nodes that are geographically close to an end user into a cluster to collaboratively work on the user’s tasks. It is challenging for the system to find an optimal strategy that assigns workload to edge nodes to meet the user’s optimization goal. To address the challenge, this paper proposes multiple algorithms for different situations. Firstly, considering the situation that historical data cannot be obtained, a multi-round allocation algorithm based on EMA prediction is proposed, and the experimental results prove the efficiency and necessity of multiple rounds of transmission. To address the second case of obtaining historical data, this paper introduces a prediction-based dynamic task assignment algorithm that assigns workload to edge nodes in each time slot based on the prediction of their capacities/costs and an empirical optimal allocation strategy which is learned from an offline optimal solution from historical data. Experimental results demonstrate that our proposed algorithm achieves significantly higher performance than several other algorithms, and especially its performance is very close to that of an offline optimal solution. Finally, we propose an online task assignment algorithm based on Q-learning, which uses the model-free Q-learning algorithm to actively learn the allocation strategy of the system, and the experimental results verify the superiority and effectiveness of this algorithm.
Keywords: Edge computing; Task assignment; Prediction; Q-Learning; Optimization algorithm

Cristiano L. Moreira, Carlos A. Kamienski, Reinaldo A.C. Bianchi,
5G and edge: A reinforcement learning approach for Virtual Network Embedding with cost optimization and improved acceptance rate,
Computer Networks,
Volume 247,
2024,
110434,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110434.
(https://www.sciencedirect.com/science/article/pii/S1389128624002664)
Abstract: 5G technologies are fueling a revolution across numerous industries, including manufacturing, healthcare, and entertainment, by enabling the development and deployment of novel applications at the network’s edge. To meet the demanding service level agreements of these industries, a dynamic and adaptable infrastructure strategy that combines cloud and edge computing models is needed. This hybrid approach offers the benefits of both centralized cloud processing and decentralized edge computing, optimized for responsiveness and efficiency. A key element for success is an orchestration mechanism that dynamically allocates resources to ensure the infrastructure can adapt to fluctuating demands in real time, optimizing resource utilization and meeting SLA requirements. Among these mechanisms, virtual network embedding (VNE) and dynamic resource management (DRM) have emerged as tools for defining where and how edge technology should be used. However, current VNE approaches struggle to adapt to real-time fluctuations in demand across geographically distributed edge resources. This work introduces a novel resource allocation algorithm, the VNE-CRS, which uses an Artificial Intelligence technique called Reinforcement Learning to orchestrate resources across multiple domains. This approach benefits from the strength of Reinforcement Learning: its ability to consider the entire problem from beginning to end while incorporating various aspects of 5G Quality of Service Indicators for optimal decision-making. Experiments were conducted to evaluate the performance of VNE-CRS against state-of-the-art algorithms for multi-domain edge environments. Results have shown that employing Reinforcement Learning techniques for VNE resource allocation yields performance gains of 12.32 percentage points in comparison with the GRC algorithm and 28.80 percentage points in comparison with the base edge environment, presenting an acceptability rate closer to the Public Cloud environment with all benefits of edge environment. In conclusion, VNE-CRS offers an efficient solution for resource allocation in 5G environments, achieving superior performance and transforming the VNE architecture into a comprehensive orchestration system that optimizes infrastructure utilization for strategic long-term benefits.
Keywords: Network virtualization; 5G network slicing; Cloud computing; Edge computing; Virtual Network Embedding; Artificial Intelligence; Reinforcement Learning

Junaid Shuja, Kashif Bilal, Waleed Alasmary, Hassan Sinky, Eisa Alanazi,
Applying machine learning techniques for caching in next-generation edge networks: A comprehensive survey,
Journal of Network and Computer Applications,
Volume 181,
2021,
103005,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103005.
(https://www.sciencedirect.com/science/article/pii/S1084804521000321)
Abstract: Edge networking is a complex and dynamic computing paradigm that aims to push cloud re-sources closer to the end user improving responsiveness and reducing backhaul traffic. User mobility, preferences, and content popularity are the dominant dynamic features of edge networks. Temporal and social features of content, such as the number of views and likes are leveraged to estimate the popularity of content from a global perspective. However, such estimates should not be mapped to an edge network with particular social and geographic characteristics. In next generation edge networks, i.e., 5G and beyond 5G, machine learning techniques can be applied to predict content popularity based on user preferences, cluster users based on similar content interests, and optimize cache placement and replacement strategies provided a set of constraints and predictions about the state of the network. These applications of machine learning can help identify relevant content for an edge network. This article investigates the application of machine learning techniques for in-network caching in edge networks. We survey recent state-of-the-art literature and formulate a comprehensive taxonomy based on (a) machine learning technique (method, objective, and features), (b) caching strategy (policy, location, and replacement), and (c) edge network (type and delivery strategy). A comparative analysis of the state-of-the-art literature is presented with respect to the parameters identified in the taxonomy. Moreover, we debate research challenges and future directions for optimal caching decisions and the application of machine learning in edge networks.
Keywords: Caching; Edge networks; Machine learning; Popularity prediction; 5G

Kaicheng Guo, Yixiao Xu, Zhengwei Qi, Haibing Guan,
Optimum: Runtime optimization for multiple mixed model deployment deep learning inference,
Journal of Systems Architecture,
Volume 141,
2023,
102901,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102901.
(https://www.sciencedirect.com/science/article/pii/S1383762123000802)
Abstract: GPUs used in data centers to perform deep learning inference tasks are underutilized. Previous systems tended to deploy a single model on a GPU to ensure that inference tasks met throughput and latency requirements. The rapid increase in one GPU’s resources, as well as the emergence of scenarios such as small models and small batches, have exacerbated the issue of low GPU utilization. In this case, a mixed model deployment-based solution can significantly improve GPU utilization while also providing greater flexibility to the inference system’s upper layer. The selection of model combinations and optimization strategies in mixed model deployment, however, remain unresolved issues. This paper proposes Optimum, the first model-combination planning and runtime optimization framework for mixed model deployment. Facing enormous search spaces, Optimum uses performance prediction for model combination selection with low search overhead. The predictor is based on a multilayer perceptron. Its input features are the profiling results of the model engine, and the output is the performance degradation. The runtime optimization strategies allow Optimum to perform performance optimization and fine-grained tradeoff. The Optimum prototype is based on CUDA multi-stream and TensorRT. The test results show that we have a flat 10.3% performance improvement over mainstream single-model deployments. We have a performance improvement of up to 7.09% over the state-of-the-art with an order of magnitude reduction in search overhead.
Keywords: Inference system; GPU inference serving; Mixed deployment; Performance prediction; Co-location

Jian Wang, Hongchang Ke, Xuejie Liu, Hui Wang,
Optimization for computational offloading in multi-access edge computing: A deep reinforcement learning scheme,
Computer Networks,
Volume 204,
2022,
108690,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108690.
(https://www.sciencedirect.com/science/article/pii/S1389128621005569)
Abstract: Owing to their limited computing power and battery level, wireless users (WUs) can hardly handle compute-intensive workflows by the local processor. Multi-access edge computing (MEC) servers attached to base stations have ample computing power and communication resources, which can be used to address the computation tasks or workloads of WUs. In this study, we design a framework with multiple static and vehicle-assisted MEC servers to handle the workloads offloaded by WUs. For obtaining the optimal computation offloading scheme to minimize the weighted sum cost, including transmission and execution cost, energy consumption cost, and communication bandwidth cost, we model the offloading decision optimization problem as a Markov decision process (MDP). Then, we propose a partial computation offloading scheme based on reinforcement learning (RL) to address the absence of priori knowledge. The proposed scheme can learn the optimal offloading decision based on stochastic workload arrival, the changing channel state, and the dynamic distance between WUs and the edge servers. Moreover, to avoid the curse of dimensionality caused by the complex state and action spaces, we present an improved computation offloading method based on deep RL (DRL) to learn the optimal offloading policy using deep neural networks. Extensive numerical results illustrate that the proposed algorithms based on RL and DRL can autonomously learn the optimal computation offloading policy with no priori knowledge, and their performance are better than that of four baselines algorithms.
Keywords: Multi-access edge computing; Computation offloading; Markov decision process; Reinforcement learning

Amir Reshadinezhad, Mohammad Reza Khayyambashi, Naser Movahedinia,
An efficient adaptive cache management scheme for named data networks,
Future Generation Computer Systems,
Volume 148,
2023,
Pages 79-92,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.028.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001693)
Abstract: With the advent of modern technologies and applications, NDN networks have been known as a reliable approach to meeting the needs ahead. A prominent feature of these networks is the ability to cache comprehensive content within network nodes. Separating the content from the original location reduces the content retrieval latency, network traffic, and overhead within the applications containing considerable required data volume. This approach also improves the user experience. In this article, first, the idea of intra-network caching with an approach based on the cooperation of adjacent nodes along the path is proposed to form a local virtual cluster with distributed cache space. Second, popular cached data will be shared by exchanging minimal notification messages between them to develop content storage decisions. Two new modules are defined in each node, called RIT and NCT which have offered the possibility of utilizing the resources of nodes adjacent to the delivered route. In the next stage, a novel approach is proposed to determine the content caching threshold in each node by considering the parameters affecting the content, as well as the network topology and real-time status of the nodes. Finally, the decision mechanism regarding the storage of content in each node along the path is presented by using an adaptive approach based on set thresholds. In such a manner, undesirable redundancy of content that wastes network resources will be removed, and more caching space in each area of the network will be provided. The results of simulations using ndnSim revealed that the performance of the proposed algorithm (cache hit ratio, average data delivery latency and path stretch) is better than other existing benchmark strategies. Considering other different parameters confirmed the effectiveness of the presented approach.
Keywords: Information-Centric Network (ICN); Named Data Networking (NDN); In-network caching; Neighbor cooperation; Caching threshold; Cache placement strategy

Quoc-Viet Pham, Dinh C. Nguyen, Seyedali Mirjalili, Dinh Thai Hoang, Diep N. Nguyen, Pubudu N. Pathirana, Won-Joo Hwang,
Swarm intelligence for next-generation networks: Recent advances and applications,
Journal of Network and Computer Applications,
Volume 191,
2021,
103141,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103141.
(https://www.sciencedirect.com/science/article/pii/S1084804521001582)
Abstract: In next-generation networks (NGN), a very large number of devices and applications are emerged, along with the heterogeneity of technologies, architectures, mobile data, etc., and optimizing such a network is of utmost importance. Besides convex optimization and game theory, swarm intelligence (SI) has recently appeared as a promising optimization tool for wireless networks. As a new subdivision of artificial intelligence, SI is inspired by the collective behaviors of societies of biological species. In SI, simple agents with limited capabilities can achieve intelligent strategies for high-dimensional and challenging problems, and thus SI has recently found many applications in NGN. However, SI techniques have still not fully investigated in the literature, especially in the contexts of wireless networks. In this work, our primary focus will be the integration of these two domains, i.e., NGN and SI. Firstly, we provide an overview of SI techniques from fundamental concepts to well-known optimizers. Secondly, we review the applications of SI to settle emerging issues in NGN, including spectrum management and resource allocation, wireless caching and edge computing, network security, and several other miscellaneous issues. Finally, we highlight challenges and issues in the literature, and introduce some interesting directions for future research.
Keywords: 5G and beyond; 6G; Artificial intelligence (AI); Computational intelligence; Swarm intelligence (SI); Next-generation wireless networks

Zichao Xie, Zeyuan Li, Jinsong Gui, Anfeng Liu, Neal N. Xiong, Shaobo Zhang,
UWPEE: Using UAV and wavelet packet energy entropy to predict traffic-based attacks under limited communication, computing and caching for 6G wireless systems,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 238-252,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.013.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003260)
Abstract: The development of 6G has enhanced the communication capabilities of Internet of Things (IoT) devices. However, in wireless system, due to cost constraints, only a few devices have the communication capability of 6G, and the rest can only communicate with the Internet through self-organized networks. Due to simple hardware, these IoT devices have low capacities of communication, computing and caching. And they are vulnerable to all kinds of attacks. One of harmful attacks is traffic-based attack such as on–off attack, Denial of Service (DoS) attack which consumes the limited energy of IoT devices and wreaks havoc on data-based applications. However, there is no effective way to obtain the truth traffic of IoT devices, which makes it difficult for the cloud to secure the communication of IoT devices and manage the state of network. To ensure reliable communication, a novel approach to detect traffic-based attack by Unmanned Aerial Vehicle (UAV) and Wavelet Packet Energy Entropy (UWPEE) is proposed. In UWPEE scheme, UAV is sent to collect the truth traffic from IoT devices. Then wavelet packet energy entropy is innovatively adopted to detect attacks. Finally, the trust of IoT devices is determined according to their entropy. The experimental results show that UWPEE scheme can effectively identify traffic-based attacks with an accuracy rate of 84.47% and an average recognition efficiency of 4.89 for malicious nodes. Meanwhile, compared with the greedy algorithm, the flight path of the UAVs is reduced by 15.44%.
Keywords: Traffic-based attack; Wavelet packet energy entropy; Sensor-cloud systems; 6G; IoT communication; Trust evaluation

Long Luo, Klaus-Tycho Foerster, Stefan Schmid, Hongfang Yu,
Optimizing multicast flows in high-bandwidth reconfigurable datacenter networks,
Journal of Network and Computer Applications,
Volume 203,
2022,
103399,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103399.
(https://www.sciencedirect.com/science/article/pii/S1084804522000583)
Abstract: Modern cloud applications has led to a huge increase in multicast flows, which is becoming one of the primary communication patterns in nowadays datacenter networks. Emerging datacenter technologies enable interesting new opportunities to support such multicast traffic more effectively and flexibly in the physical layer: novel circuit switches offer high-bandwidth and reconfigurable inter-rack multicasting capabilities. However, not much is known today about the algorithmic challenges introduced by this new technology, especially in optimizing the completion times for multicast flows. This paper presents SplitCast, a preemptive multicast scheduling approach that fully exploits emerging high-bandwidth physical-layer multicasting capabilities to reduce flow times. SplitCast dynamically reconfigures the circuit switches to adapt to the multicast traffic, accounting for reconfiguration delays. In particular, SplitCast relies on simple single-hop routing and leverages transfer flexibilities by supporting splittable multicast so that a transfer can already be delivered to just a subset of receivers when the circuit capacity is insufficient. Moreover, SplitCast supports two common forwarding models, the all-stop and the not-all-stop, during circuit reconfiguration. We conduct extensive simulation to evaluate the performance of SplitCast, and the results show that SplitCast can cut down flow times significantly compared to state-of-the-art solutions.
Keywords: Computer networks; Multicast communication; Reconfigurable architectures; Scheduling algorithms

Yaser Mansouri, M. Ali Babar,
A review of edge computing: Features and resource virtualization,
Journal of Parallel and Distributed Computing,
Volume 150,
2021,
Pages 155-183,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.12.015.
(https://www.sciencedirect.com/science/article/pii/S0743731520304317)
Abstract: With the advent of Internet of Things (IoT) connecting billions of mobile and stationary devices to serve real-time applications, cloud computing paradigms face some significant challenges such as high latency and jitter, non-supportive location-awareness and mobility, and non-adaptive communication types. To address these challenges, edge computing paradigms, namely Fog Computing (FC), Mobile Edge Computing (MEC) and Cloudlet, have emerged to shift the digital services from centralized cloud computing to computing at edges. In this article, we analyze cloud and edge computing paradigms from features and pillars perspectives to identify the key motivators of the transitions from one type of virtualized computing paradigm to another one. We then focus on computing and network virtualization techniques as the essence of all these paradigms, and delineate why virtualization features, resource richness and application requirements are the primary factors for the selection of virtualization types in IoT frameworks. Based on these features, we compare the state-of-the-art research studies in the IoT domain. We finally investigate the deployment of virtualized computing and networking resources from performance perspective in an edge-cloud environment, followed by mapping of the existing work to the provided taxonomy for this research domain. The lessons from the reviewed are that the selection of virtualization technique, placement and migration of virtualized resources rely on the requirements of IoT services (i.e., latency, scalability, mobility, multi-tenancy, privacy, and security). As a result, there is a need for prioritizing the requirements, integrating different virtualization techniques, and exploiting a hierarchical edge-cloud architecture.
Keywords: Mobile Edge Computing; Fog Computing; Classical virtualization; Network function Virtualization (NFV)

Yaser Jararweh,
Enabling efficient and secure energy cloud using edge computing and 5G,
Journal of Parallel and Distributed Computing,
Volume 145,
2020,
Pages 42-49,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.06.014.
(https://www.sciencedirect.com/science/article/pii/S074373152030321X)
Abstract: Energy cloud systems continue to shape the future of the energy sector. The complexity of energy cloud systems stems from their widespread and distributed aspects such as renewable energy sources, energy storage, customers engagement, social media and the advancements in communication and computing technologies. The unprecedented large-scale growth of energy cloud systems requires a crucial and dramatic paradigm shift in managing and optimizing the available energy assets in order to satisfy the increasing customers’ requirements. This paper proposes and evaluates an edge computing based framework that aims to efficiently manage and optimize energy cloud systems while increasing their reliability, safety, and security. The proposed framework exploits the current expansion in computing capabilities of the edge computing and the Fifth Communication Generation (5G) technology. The evaluation of the proposed framework shows that an edge computing infrastructure improves the service efficiency by 22.6% compared with a cloud infrastructure. In addition, the latency is reduced by 69.1%. The proposed framework provides threat detection capability by using the edge layer as an extra layer for defense against energy cloud system attacks. However, this defense mechanism incurs 10.9% overhead and 9.6% extra delay per service request on average.
Keywords: Energy cloud; Edge computing; 5G; Energy cloud security; Reliability

EL Hocine Bouzidi, Abdelkader Outtagarts, Rami Langar, Raouf Boutaba,
Deep Q-Network and Traffic Prediction based Routing Optimization in Software Defined Networks,
Journal of Network and Computer Applications,
Volume 192,
2021,
103181,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103181.
(https://www.sciencedirect.com/science/article/pii/S1084804521001909)
Abstract: Software Defined Networking (SDN) is gaining momentum not only in research but also in IT industry representing the drivers of 5G networks, due to its capabilities of increasing the flexibility of a network and address a variety of network challenges, by logically centralizing the intelligence in software-based controllers. Thanks to Machine Learning (ML) techniques, the network performances and utilization can be optimized and enhanced. Neural Networks (NN) and Reinforcement Learning (RL), in particular, have demonstrated great success in cooperating with complex problems arising in network operation and management. To this end, we exploit in this paper, an SDN-based rules placement approach that aims to dynamically predict the traffic congestion by using mainly NN and learn optimal paths and reroute traffic to improve network utilization by deploying a Deep Q-Network (DQN) agent. To this end, we first formulate the Quality-of-Service (QoS)-aware routing problem as a Linear Program (LP), whose objective is to minimize the end-to-end (E2E) delay and link utilization. Then, we propose a simple yet efficient heuristic algorithm to solve it. Numerical results through emulation using ONOS controller and Mininet demonstrate that the proposed approach can significantly improve network performances in terms of decreasing the link utilization, the packet loss and the E2E delay.
Keywords: SDN; Prediction; Neural Networks; QoS; ONOS; DQN; LSTM

Guowen Wu, Zhiqi Xu, Hong Zhang, Shigen Shen, Shui Yu,
Multi-agent DRL for joint completion delay and energy consumption with queuing theory in MEC-based IIoT,
Journal of Parallel and Distributed Computing,
Volume 176,
2023,
Pages 80-94,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.02.008.
(https://www.sciencedirect.com/science/article/pii/S0743731523000291)
Abstract: In the Industrial Internet of Things (IIoT), there exist numerous sensor devices with weak computing power and small energy storage. To meet the real-time and big data computing requirements of industrial production, EIIoT (Edge computing-based IIoT) that combines mobile edge computing with IIoT has emerged. It is necessary to offload computing tasks to nearby edge servers for data storage and processing in EIIoT, thus inevitably causing the edge servers to overload. To this end, we propose a jointly constrained optimization model of delay and energy consumption based on queuing theory; this model can effectively solve the task offloading problem in EIIoT. Subsequently, to satisfy the unique offloading requirements of EIIoT, we improve the MAPPO (multi agent proximal policy optimization) algorithm structure to form a lightweight optimal task offloading algorithm called Multi-Agent Deep Reinforcement Learning based on Queuing theory (MAQDRL), which is more suitable for EIIoT. In the algorithm, we systematically integrate queuing theory and use Multi-Agent Deep Reinforcement Learning (MADRL) to obtain the optimal offloading strategy in dynamic and random multiuser offloading environments. We also improve the structure of neural networks of MADRL by analyzing the structural characteristics of the input data. As a result, the algorithm that we proposed exhibits good convergence and exceptional performance in terms of the task arrival rate, bandwidth, energy consumption, latency and other indicators. The simulation results indicate that compared with other classical algorithms, MAQDRL is effective for solving the EIIoT offloading problem.
Keywords: Task offloading; Queuing theory; Industrial Internet of things; Multi-agent deep reinforcement learning; Mobile edge computing

Qi Li, Zhenyu Shi, Zhaoyu Xue, Zhihua Cui, Yubin Xu,
A many-objective evolutionary algorithm for solving computation offloading problems under uncertain communication conditions,
Computer Communications,
Volume 213,
2024,
Pages 22-32,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.10.020.
(https://www.sciencedirect.com/science/article/pii/S0140366423003845)
Abstract: Computation offloading is a critically important technology in the field of edge computing, enabling improved conditions for task computation. Existing studies on computation offloading largely focus on cooperative offloading and resource allocation. The impact of factors such as uncertainty during task transmission and the different task characteristics on the offloading location is ignored. To address the above issues, this paper designs a many-objective optimization computation offloading model (MaOCO) under uncertainty to cope with offloading requirements. The model formulates channel transmission level strategies and considers five objectives: latency, cost, energy consumption, load balancing, and user satisfaction. A many-objective evolutionary algorithm with deviation selection (MaOEA-DS) is designed to obtain effective offloading strategies. In the environment selection, the deviation is calculated using the concept of variance to choose solutions closer to the origin for the last layer, which further improves the convergence of the algorithm. The simulation results show that MaOEA-DS outperforms other evolutionary algorithms in IGD, GD, SP, and HV.
Keywords: Edge Computing; Many-objective evolutionary algorithm; Task offloading; Uncertainty

Akhirul Islam, Arindam Debnath, Manojit Ghose, Suchetana Chakraborty,
A Survey on Task Offloading in Multi-access Edge Computing,
Journal of Systems Architecture,
Volume 118,
2021,
102225,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102225.
(https://www.sciencedirect.com/science/article/pii/S1383762121001570)
Abstract: With the advent of new technologies in both hardware and software, we are in the need of a new type of application that requires huge computation power and minimal delay. Applications such as face recognition, augmented reality, virtual reality, automated vehicles, industrial IoT, etc. belong to this category. Cloud computing technology is one of the candidates to satisfy the computation requirement of resource-intensive applications running in UEs (User Equipment) as it has ample computational capacity, but the latency requirement for these applications cannot be satisfied by the cloud due to the propagation delay between UEs and the cloud. To solve the latency issues for the delay-sensitive applications a new network paradigm has emerged recently known as Multi-Access Edge Computing (MEC) (also known as mobile edge computing) in which computation can be done at the network edge of UE devices. To execute the resource-intensive tasks of UEs in the MEC servers hosted in the network edge, a UE device has to offload some of the tasks to MEC servers. Few survey papers talk about task offloading in MEC, but most of them do not have in-depth analysis and classification exclusive to MEC task offloading. In this paper, we are providing a comprehensive survey on the task offloading scheme for MEC proposed by many researchers. We will also discuss issues, challenges, and future research direction in the area of task offloading to MEC servers.
Keywords: Multi-access edge computing; Task offloading; Mobile edge computing; Survey

Marzieh Sadat Zahedinia, Mohammad Reza Khayyambashi, Ali Bohlooli,
Fog-based caching mechanism for IoT data in information centric network using prioritization,
Computer Networks,
Volume 213,
2022,
109082,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109082.
(https://www.sciencedirect.com/science/article/pii/S1389128622002195)
Abstract: In-Network caching is one of the most prominent features of Information-Centric Networks (ICN). This feature, designed to reduce the content delivery time, can improve data availability, especially during the sleeping mode of the original data producer. However, the fact that caching mechanism is simultaneously performed with data forwarding, limited processing power, and limited memory capacity of routers has posed challenges in fully profiting from this feature. The synchronization of caching mechanism with data forwarding limits the speed of decision-making in content placement policies. These challenges as well as the limitations of ICN routers have not been taken into consideration by the existing strategies for content placement of IoT data. In this paper, the restrictions for content placement policies in ICN-based IoT are outlined, and a data prioritization-based approach that prevents ICN nodes from filling their Content Store (CS) with inappropriate and inferior data is proposed to improve cache efficiency. Besides, fog computing is used as a middle layer between IoT devices and ICN networks to resolve the speed limits of decision-making and improve cache performance. In the fog layer, prioritization is performed based on the popularity and freshness of data object, and, thus, low-priority data are removed from the caching mechanism of ICN nodes. As the result shows the total number of cached data is reduced. Freshness is an essential property of multiple IoT data that significantly affects caching performance. That's why we prioritized data based on popularity and freshness. Eventually, the simulation results performed by the NDNsim simulator show that decreasing the number of data objects could improve the cache mechanism efficiency in terms of both delay and hit ratio.
Keywords: ICN-based IoT; NDN; Caching mechanism; Priority field; Fog computing

Abbas Dehghani,
A design flow for an optimized congestion-aware application-specific wireless network-on-chip architecture,
Future Generation Computer Systems,
Volume 106,
2020,
Pages 234-249,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.01.001.
(https://www.sciencedirect.com/science/article/pii/S0167739X19306594)
Abstract: Wireless Networks-on-Chip (WiNoC) architecture has emerged as an alternative communication infrastructure for the conventional wire-line NoC to achieve higher performance and low energy dissipation in on-chip communications. However, as realistic applications have different communication requirements, application-specific platforms are attractive to guarantee a specific level of performance. Moreover, the congestion probability over wireless routers is high, because each wireless router is shared by a group of cores. Therefore, we propose a novel methodology for designing an optimized congestion-aware application-specific WiNoC architecture. This methodology utilizes a novel mesh-of-tree (MoT) topology as a communication infrastructure to exploit the benefits of both mesh and tree topologies. Then, for a given application, the long-distance wire/wireless links are inserted into the MoT topology by considering the optimization of system cost and performance simultaneously. Furthermore, a congestion-aware layered routing is presented to reduce the congestion probability. Through cycle-accurate simulations, the performance of the proposed architecture has been evaluated and compared with state-of-the-art works under both synthetic and realistic traffic patterns in terms of network throughput, latency, and energy consumption. Moreover, the area overhead associated with the proposed WiNoC architecture is investigated. The experimental results confirmed that the proposed MoT-based WiNoC architecture is a very competitive architecture among the alternative WiNoC architectures.
Keywords: Emerging on-chip interconnections; Chip multiprocessors; Network-on-chip (NoC); Wireless communication; Congestion

Miaojiang Chen, Wei Liu, Tian Wang, Anfeng Liu, Zhiwen Zeng,
Edge intelligence computing for mobile augmented reality with deep reinforcement learning approach,
Computer Networks,
Volume 195,
2021,
108186,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108186.
(https://www.sciencedirect.com/science/article/pii/S1389128621002425)
Abstract: Convergence of Augmented Reality (AR) and Next Generation Internet-of-Things (NG-IoT) can create new opportunities in many emerging areas, where the real-time data can be visualized on the devices. Integrated NG-IoT network, AR can improve efficiency in many fields such as mobile computing, smart city, intelligent transportation and telemedicine. However, limited by capability of mobile device, the reliability and latency requirements of AR applications is difficult to meet by local processing. To solve this problem, we study a binary offloading scheme for AR edge computing. Based on the proposed model, the parts of AR computing can offload to edge network servers, which is extend the computing capability of mobile AR devices. Moreover, a deep reinforcement learning offloading model is considered to acquire B5G network resource allocation and optimally AR offloading decisions. First, this offloading model does not need to solve combinatorial optimization, which is greatly reduced the computational complexity. Then the wireless channel gains and binary offloading states is modeled as a Markov decision process, and solved by deep reinforcement learning. Numerical results show that our scheme can achieve better performance compared with existing optimization methods.
Keywords: Beyond fifth-generation; Mobile augmented reality; Markov decision process; Deep reinforcement learning; Artificial intelligence

Rong Gu, Yang Qi, Tongyu Wu, Zhaokang Wang, Xiaolong Xu, Chunfeng Yuan, Yihua Huang,
SparkDQ: Efficient generic big data quality management on distributed data-parallel computation,
Journal of Parallel and Distributed Computing,
Volume 156,
2021,
Pages 132-147,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.05.012.
(https://www.sciencedirect.com/science/article/pii/S0743731521001246)
Abstract: In the big data era, large amounts of data are under generation and accumulation in various industries. However, users usually feel hindered by the data quality issues when extracting values from the big data. Thus, data quality issues are gaining more and more attention from data quality management analysts. Cutting-edge solutions like data ETL, data cleaning, and data quality monitoring systems have many deficiencies in capability and efficiency, making it difficult to cope with complicated situations on big data. These problems inspire us to build SparkDQ, a generic distributed data quality management model and framework that provides a series of data quality detection and repair interfaces. Users can quickly build custom tasks of data quality computing for various needs by utilizing these interfaces. In addition, SparkDQ implements a set of algorithms that in a parallel manner with optimizations. These algorithms aim at various data quality goals. We also propose several system-level optimizations, including the job-level optimization with multi-task execution scheduling and the data-level optimization with data state caching. The experimental evaluation shows that the proposed distributed algorithms in SparkDQ run up to 12 times faster compared to the corresponding stand-alone serial and multi-thread algorithms. Compared with the cutting-edge distributed data quality solution Apache Griffin, SparkDQ has more features, and its execution time is only around half of Apache Griffin on average. SparkDQ achieves near-linear data and node scalability.
Keywords: Parallel data quality algorithms; Distributed system; Data quality management system; Multi-tasks scheduling; Big data

Mohammad Nikravan, Mostafa Haghi Kashani,
A review on trust management in fog/edge computing: Techniques, trends, and challenges,
Journal of Network and Computer Applications,
Volume 204,
2022,
103402,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103402.
(https://www.sciencedirect.com/science/article/pii/S1084804522000613)
Abstract: Cloud computing provides software, infrastructure, and platform as services and reduces the cost of usage for cloud customers. Recently, a system architecture called Fog and Edge Computing (FEC) has been introduced that fills the gap between cloud and things toward the continuum of service and optimizes cloud computing resources by processing time-sensitive data near the data generation source at the network edge. Since the FEC environment includes myriad heterogeneous computing nodes, some of the FEC nodes may be un-trustful or even malicious; therefore, these un-trustworthy nodes could disrupt the normal activity of FEC in data storing and processing. Consequently, FEC trust management is crucial to provide trustworthy data processing and improve user privacy. Despite the critical importance of trust management issues in the FEC, any systematic review in this field has not been performed. This paper presents a systematic review of 74 high-quality articles related to FEC trust management published between 2015 and July 2021. To this end, selected FEC trust management approaches are categorized into three main classes: algorithm, architecture, and model/framework. Additionally, this paper discusses and compares the FEC trust management approaches based on merits and demerits, evaluation techniques, tools and simulation environments, and important trust metrics. Finally, some open issues and future trends for the oncoming studies are highlighted.
Keywords: Fog/edge computing; Trust management; Privacy; Internet of things (IoT); Attack; Security

Javad Dogani, Reza Namvar, Farshad Khunjush,
Auto-scaling techniques in container-based cloud and edge/fog computing: Taxonomy and survey,
Computer Communications,
Volume 209,
2023,
Pages 120-150,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.06.010.
(https://www.sciencedirect.com/science/article/pii/S0140366423002086)
Abstract: The long-held dream of computing as a service was realized with the emergence of cloud computing. Recently, fog and edge computing have been introduced as extensions of cloud networks, providing networking, processing, data management, and storage on network nodes near Internet of Things (IoT) devices to bridge the gap between the cloud and IoT devices. As the foundation of distributed computing, virtualization enables more effective use of physical computer hardware. Operating system virtualization through containers has recently been proposed as a promising alternative to virtual machines (VMs). Containers are lightweight packages that contain all application dependencies, system libraries, and third-party software packages. This research aims to review auto-scaling solutions for container-based virtualization in cloud and edge/fog computing applications. Auto-scaling plays a crucial role in the broad adoption of cloud computing by allocating and releasing computing resources in response to fluctuating resource requirements. However, designing and implementing an efficient auto-scaler for container-based applications in cloud and edge/fog computing presents challenges due to diverse application resource requirements and dynamic workload characteristics. Our research presents a comprehensive classification system for articles, covering key parameters such as auto-scaling techniques, experiments, workloads, and metrics, among others. We provide a detailed analysis of the results, offering valuable insights into open challenges and identifying promising directions for future research in this field.
Keywords: Cloud computing; Fog computing; Edge computing; Auto-scaling; Container-based virtualization

Amara Umar, Syed Ali Hassan, Haejoon Jung, Sahil Garg, M. Shamim Hossain, Mohsen Guizani,
Computation offloading in NOMA-MEC-enabled aerial-vehicular networks exploiting mmWave capabilities,
Computer Networks,
Volume 246,
2024,
110335,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110335.
(https://www.sciencedirect.com/science/article/pii/S1389128624001671)
Abstract: In recent years, there has been a significant interest in ubiquitous coverage, high data rate connectivity, and mobile edge computing (MEC) as crucial services within the future sixth-generation (6G) wireless networks. These services are regarded as essential components, exemplifying the advancements anticipated in 6G technology. Nevertheless, the successful implementation of these services in MEC-enabled vehicular networks significantly relies on the availability of robust network coverage and telecommunication framework. Unfortunately, in far-flung and isolated regions, such framework is often lacking, posing significant challenges in achieving uninterrupted connectivity, comprehensive coverage, and efficient computation offloading. Targeting the aforementioned horizon, in this paper, an uplink non-orthogonal multiple access (NOMA)-MEC-enabled aerial-vehicular network operating at millimeter wave (mmWave) is proposed in which the ground vehicles are provided edge computing services by an aerial autonomous vehicle, i.e., a high-altitude platform (HAP). We present a system model in which a HAP equipped with MEC servers offers computation offloading capabilities to a vehicular network. Our main objective is to minimize the transaction time of a NOMA cluster offloading its data to MEC servers located at the HAP. We devise a dual-layer optimization scheme for optimizing the transmission power and computational resource allocation by using the Lagrange multipliers method and then attain convergence by implementing a sub-gradient approach. We further extend our work by proposing a data-aware NOMA clustering scheme. The simulation results demonstrate the efficacy of our proposed approach, showing a notable reduction in the transaction time in comparison to the baseline scheme. The optimal power allocation enhances the data rates which subsequently reduces the transmission time, and the optimal cores assignment effectively minimizes the computation time. Additionally, the data-aware NOMA clustering scheme shows promising results by enhancing the system effective throughput and the spectral efficiency in comparison to the conventional NOMA clustering.
Keywords: Aerial-vehicular networks; Aerial autonomous vehicles; Mobile edge computing; High-altitude platforms; Millimeter waves; Data-aware non-orthogonal multiple access

Xiang He, Zhiying Tu, Xiaofei Xu, Zhongjie Wang,
Programming framework and infrastructure for self-adaptation and optimized evolution method for microservice systems in cloud–edge environments,
Future Generation Computer Systems,
Volume 118,
2021,
Pages 263-281,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.01.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X21000170)
Abstract: Edge computing technologies facilitate the deployment of services on nearby edge servers with a large number of end users and their mobile devices to fulfill personalized demands. Owing to frequent changes in user mobility and demands, service systems deployed in an edge–cloud environment must continuously adapt to ensure that the quality of service (QoS) perceived by the end users is maintained at a stable and satisfactory level. As it is difficult for system operation engineers to manually deal with such frequent and large-scale evolution due to problems of cost and efficiency, self-adaptation of the system is essential. In this paper, we present a programming framework for microservices (EPF4M) and an infrastructure for self-adaptive microservice systems (EI4MS) for the cloud–edge environment based on microservice architecture. Our study follows a “monitoring–analyzing–planning–execution” control loop that empowers the service systems to redeploy the services according to changes in the QoS. A two-phase strategy is adopted to minimize the side effects of the loop on the performance of the service system. A prototype of this framework and infrastructure has been open-sourced and verified through experiments conducted in a real cloud–edge environment. The results demonstrate the usefulness and advantages of our approach.
Keywords: Self-adaptation; Microservice systems; Cloud–edge environment; Quality of Services (QoS); DevOps; Infrastructure

Weimin Liu, Guan Huang, Aiyun Zheng, Jiaxin Liu,
Research on the optimization of IIoT data processing latency,
Computer Communications,
Volume 151,
2020,
Pages 290-298,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.007.
(https://www.sciencedirect.com/science/article/pii/S0140366419310370)
Abstract: As for data processing, data should be sent to cloud, stored and computed. Usually, the amount of data is huge enough for higher latency of processing. In addition, the mobility of cloud service would be much slower. In this paper, an Industrial Internet of Things cloud–fog hybrid network (ITCFN) framework is proposed to solve high latency upon processing industrial data on cloud. In the production equipment area, edge devices such as routers and switches are utilized by framework to construct a fog computing layer between cloud server and production equipment. Since computing power of the edge devices in fog computing is much poor, a distributed computing method for multi-devices is proposed. The aim of minimum task processing delay is achieved by using the constrained particle swarm optimization load balancing algorithm based on simulated annealing method (SAPSO-LB). The experimental results show that the ITCFN based on SAPSO-LB algorithm can effectively reduce the industrial data processing delay. When ten fog computing devices are used and the collected data is between 4GB and 12GB, compared with cloud computing, the latency is improved by 84.1%-29.9%.
Keywords: Task allocation; Fog computing; IIoT; SAPSO-LB algorithm; ITCFN framework

Zhengyu Song, Xintong Qin, Yuanyuan Hao, Tianwei Hou, Jun Wang, Xin Sun,
A comprehensive survey on aerial mobile edge computing: Challenges, state-of-the-art, and future directions,
Computer Communications,
Volume 191,
2022,
Pages 233-256,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.05.004.
(https://www.sciencedirect.com/science/article/pii/S0140366422001566)
Abstract: Driven by the visions of Internet of Things (IoT), there is an ever-increasing demand for computation resources of IoT users to support diverse applications. Mobile edge computing (MEC) has been deemed a promising solution to settle the conflict between the resource-hungry mobile applications and the resource-constrained IoT users. On the other hand, in order to provide ubiquitous and reliable connectivity in wireless networks, unmanned aerial vehicles (UAVs) can be leveraged as efficient aerial platforms by exploiting their inherent attributes, such as the on-demand deployment, high cruising altitude, and controllable maneuverability in three-dimensional (3D) space. Thus, the UAV-enabled aerial MEC is believed as a win-win solution to facilitate cost-effective and energy-saving communication and computation services in various environments. In this paper, we provide a comprehensive survey on the UAV-enabled aerial MEC. Firstly, the related advantages and research challenges for aerial MEC are discussed. Then, we provide a comprehensive review of the recent research advances, which is categorized by different domains, including the joint optimization of UAV trajectory, computation offloading and resource allocation, UAV deployment, task scheduling and load balancing, interplay between aerial MEC and other technologies, as well as the machine-learning (ML)-driven optimization. Finally, some important research directions deserved more efforts in future work are summarized.
Keywords: Computation offloading; Mobile edge computing (MEC); Machine learning (ML); Resource allocation; Trajectory design; Unmanned aerial vehicles (UAVs)

Zheng Wan, Xiaogang Dong,
Computation power maximization for mobile edge computing enabled dense network,
Computer Networks,
Volume 220,
2023,
109458,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109458.
(https://www.sciencedirect.com/science/article/pii/S1389128622004923)
Abstract: High-density connection is among the natures of next-generation wireless communication systems. Meanwhile, various computation-intensive smart applications are becoming growing popularity with the technology boom. Thus, strong computation power will become a crucial requirement for wireless communication systems. Mobile edge computing provides a promising solution to this requirement by pushing a cloud-like computation capacity to the network edge. This study aims to maximize the computation power of a mobile edge computing enabled dense network. To this end, a computation bits maximization problem is formulated by jointly optimizing offloading decision and resource allocation. The problem is a mixed-variable nonlinear programming problem. First, by analyzing the feasibility of computation modes and constructing a user distribution strategy, the original problem is decoupled into two sub-problems, i.e., the assignment of the offloading users and resource allocation. The offloading users’ assignment is modeled as a restricted multiple knapsack problem, and a profit density is defined to maximize the overall profits of multiple knapsacks. Then, an improved differential evolution algorithm is developed to address the knapsack problem, in which mutation and repair operators are designed according to the profit density. Based on the optimal solution to the knapsack problem, the resource allocation sub-problem is solved by the corresponding calculation. Finally, extensive experiments are conducted to evaluate the performance of our scheme. Results show that: (1) our scheme provides superior computation power compared to that of benchmark schemes; (2) the performance gain of our scheme over benchmark schemes expands with growing connection density. Therefore, our scheme is an effective computation power optimization scheme.
Keywords: Mobile edge computing; Computation power maximization; Restricted multiple knapsack problem; Differential evolution

Alem Čolaković,
IoT systems modeling and performance evaluation,
Computer Science Review,
Volume 50,
2023,
100598,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100598.
(https://www.sciencedirect.com/science/article/pii/S1574013723000655)
Abstract: The continuous increase of IoT applications leads to a vast amount of data that needs to be transmitted, stored, and processed. Many IoT applications rely on the Cloud infrastructure to handle these specific application demands. However, the integration of IoT and Cloud poses challenges such as network delays, throughput, energy consumption, reliability, etc. Therefore, a new computing concept is required to support emerging IoT applications. These new concepts include fog computing, edge computing, mobile edge computing, mobile cloud computing, and cloudlets. They use various approaches to distribute resources, processes, and services among IoT system architecture layers. The challenge is to decide which offloading system is the best for a specific use case that emphasizes the IoT system modeling issue. In this paper, a model for the formal description of IoT systems is presented. In addition, an analytical evaluation method was proposed to design these systems using the corresponding architecture, technologies, protocols, and integration model to optimize performance. The proposed approach facilitates and simplifies the selection of the corresponding model for the system architecture. This approach enables an efficient method for performance optimization based on offloading processes (load balancing). Also, this paper provides some insights into specific emerging issues and ideas to be addressed by future research.
Keywords: Internet of Things (IoT); Cloud computing; Fog computing; Edge computing; System modeling; Performance evaluation; Resource allocation; Quality of services (QoS)

Chu-ge Wu, Wei Li, Ling Wang, Albert Y. Zomaya,
An evolutionary fuzzy scheduler for multi-objective resource allocation in fog computing,
Future Generation Computer Systems,
Volume 117,
2021,
Pages 498-509,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.12.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X20330818)
Abstract: With rapid development of the Internet of Things (IoT), a vast amount of raw data produced by IoT devices needs to be processed promptly. Compared to cloud computing, fog computing nodes are closer to data resource for decreasing the end-to-end transmission latency. Considering the limited resource of IoT devices, offloading computationally-intensive tasks to the servers with high computing capability is essential in the IoT–fog–cloud system to complete those tasks on time. In this work, we propose a fuzzy logical offloading strategy for IoT applications characterized by uncertain parameters to optimize both agreement index and robustness. A multi-objective Estimation of Distribution Algorithm (EDA) is designed to learn and optimize the fuzzy offloading strategy from a diversity of the applications. The algorithm partitions applications into independent clusters, so that each cluster can be allocated to the corresponding tier for further processing. Thus, system resources are saved by making scheduling decisions in a reduced search space. Simulation studies on benchmark problems and real-world cases are carried out to verify the efficiency of our proposed algorithm. Pareto sets produced by our algorithm outperformed classic heuristic solutions for 88.3% benchmark cases and dominated Pareto sets of two state-of-art multi-objective algorithms for 92.7% and 94.4% cases correspondingly.
Keywords: Edge computing; Internet of Things; Evolutionary computation; Estimation of distribution algorithm; Fuzzy scheduling; Agreement index; Robustness

Zhufang Kuang, Zhihao Ma, Zhe Li, Xiaoheng Deng,
Cooperative computation offloading and resource allocation for delay minimization in mobile edge computing,
Journal of Systems Architecture,
Volume 118,
2021,
102167,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102167.
(https://www.sciencedirect.com/science/article/pii/S1383762121001181)
Abstract: Mobile edge computing (MEC) is a promising paradigm, which brings computation resources in proximity to mobile devices and allows the tasks of mobile devices to be offloaded to MEC servers with low latency. The joint problem of cooperative computation task offloading and resource allocation is a challenging issue. The joint problem of cooperative computation task offloading scheme and resource assignment in MEC is investigated in this paper, where the vertical cooperation among mobile devices, mobile edge server nodes and mobile cloud server nodes is considered, and the horizontal computation cooperation between edge nodes is considered as well. A computation offloading decision, cooperative selection, power allocation and CPU cycle frequency assignment problem is formulated. The objective is to minimize the latency while guaranteeing the constraint of transmission power, energy consumption and CPU cycle frequency. The formulated latency optimization problem is a nonconvex mixed-integer problem in general, which has binary variables and continuous variables. In order to solve the formulated problem. A joint iterative algorithm based on the Lagrangian dual decomposition, ShengJin Formula method, and monotonic optimization method is proposed. The CPU cycle frequence allocation is handled by the ShengJin Formula method due to the cubic equation of one variable about the CPU frequence allocation. The transmission power assignment is handled by the monotonic optimization method. In the algorithm convergence with different number of tasks, the proposed algorithm can quickly and effectively reach the convergence state and getting the minimum task execution delay. Numerical results demonstrate that the proposed algorithm outperforms the Full MEC, Full Local and Full Cloud three schemes in terms of execution latency.
Keywords: Mobile edge computing; Cooperative computation offloading; Delay minimization; Resource allocation

Sparsh Mittal, Sumanth Umesh,
A survey On hardware accelerators and optimization techniques for RNNs,
Journal of Systems Architecture,
Volume 112,
2021,
101839,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101839.
(https://www.sciencedirect.com/science/article/pii/S1383762120301314)
Abstract: “Recurrent neural networks” (RNNs) are powerful artificial intelligence models that have shown remarkable effectiveness in several tasks such as music generation, speech recognition and machine translation. RNN computations involve both intra-timestep and inter-timestep dependencies. Due to these features, hardware acceleration of RNNs is more challenging than that of CNNs. Recently, several researchers have proposed hardware architectures for RNNs. In this paper, we present a survey of GPU/FPGA/ASIC-based accelerators and optimization techniques for RNNs. We highlight the key ideas of different techniques to bring out their similarities and differences. Improvements in deep-learning algorithms have inevitably gone hand-in-hand with the improvements in the hardware-accelerators. Nevertheless, there is a need and scope of even greater synergy between these two fields. This survey seeks to synergize the efforts of researchers in the area of deep learning, computer architecture, and chip-design.
Keywords: Recurrent neural networks; Deep learning; GPU; FPGA; ASIC; Pruning; Parallelization; Low-precision

Shreshth Tuli, Fatemeh Mirhakimi, Samodha Pallewatta, Syed Zawad, Giuliano Casale, Bahman Javadi, Feng Yan, Rajkumar Buyya, Nicholas R. Jennings,
AI augmented Edge and Fog computing: Trends and challenges,
Journal of Network and Computer Applications,
Volume 216,
2023,
103648,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103648.
(https://www.sciencedirect.com/science/article/pii/S108480452300067X)
Abstract: In recent years, the landscape of computing paradigms has witnessed a gradual yet remarkable shift from monolithic computing to distributed and decentralized paradigms such as Internet of Things (IoT), Edge, Fog, Cloud, and Serverless. The frontiers of these computing technologies have been boosted by shift from manually encoded algorithms to Artificial Intelligence (AI)-driven autonomous systems for optimum and reliable management of distributed computing resources. Prior work focuses on improving existing systems using AI across a wide range of domains, such as efficient resource provisioning, application deployment, task placement, and service management. This survey reviews the evolution of data-driven AI-augmented technologies and their impact on computing systems. We demystify new techniques and draw key insights in Edge, Fog and Cloud resource management-related uses of AI methods and also look at how AI can innovate traditional applications for enhanced Quality of Service (QoS) in the presence of a continuum of resources. We present the latest trends and impact areas such as optimizing AI models that are deployed on or for computing systems. We layout a roadmap for future research directions in areas such as resource management for QoS optimization and service reliability. Finally, we discuss blue-sky ideas and envision this work as an anchor point for future research on AI-driven computing systems.
Keywords: AI; Edge computing; Fog computing; Cloud computing; Deployment; Scheduling; Fault-tolerance

Ke Luo, Tao Ouyang, Zhi Zhou, Xu Chen,
BeeFlow: Behavior tree-based Serverless workflow modeling and scheduling for resource-constrained edge clusters,
Journal of Systems Architecture,
Volume 143,
2023,
102968,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102968.
(https://www.sciencedirect.com/science/article/pii/S1383762123001479)
Abstract: Serverless computing has gained popularity in edge computing due to its flexible features, including the pay-per-use pricing model, auto-scaling capabilities, and multi-tenancy support. Complex Serverless-based applications typically rely on Serverless workflows (also known as Serverless function orchestration) to express task execution logic, and numerous application- and system-level optimization techniques have been developed for Serverless workflow scheduling. However, there has been limited exploration of optimizing Serverless workflow scheduling in edge computing systems, particularly in high-density, resource-constrained environments such as system-on-chip clusters and single-board-computer clusters. In this work, we discover that existing Serverless workflow scheduling techniques typically assume models with limited expressiveness and cause significant resource contention. To address these issues, we propose modeling Serverless workflows using behavior trees, a novel and fundamentally different approach from existing directed-acyclic-graph- and state machine-based models. Behavior tree-based modeling allows for easy analysis without compromising workflow expressiveness. We further present observations derived from the inherent tree structure of behavior trees for contention-free function collections and awareness of exact and empirical concurrent function invocations. Based on these observations, we introduce BeeFlow, a behavior tree-based Serverless workflow system tailored for resource-constrained edge clusters. Experimental results demonstrate that BeeFlow achieves up to 3.2× speedup in a high-density, resource-constrained edge testbed and 2.5× speedup in a high-profile cloud testbed, compared with the state-of-the-art. BeeFlow also demonstrates superior robustness in scenarios with heavy system workloads.
Keywords: Edge computing; Serverless computing; Serverless workflow; Behavior tree; Workflow modeling; Workflow scheduling

Kihan Choi, Hyuck Han, Hyungsoo Jung, Sooyong Kang,
Workload-optimized sensor data store for industrial IoT gateways,
Future Generation Computer Systems,
Volume 135,
2022,
Pages 394-408,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.05.012.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001777)
Abstract: In industrial Internet of Things (IoT) environments, sensor devices continue to generate a stream of sensor data, and the management of an ever-growing amount of data is a vital feature for IoT gateways. However, our preliminary analysis of some popular key–value stores used in IoT gateways revealed that none of the systems exploits the distinctive characteristics of IoT workloads – append-only and immutable – thus the systems show limitations in managing sensor data. To address this issue, in this study, we propose Indexing-of-Indexes (IoI) and LogFlush-and-Append (LFA) to exploit such characteristics. IoI is an indexing and data organization scheme designed to eliminate the notorious compaction-induced write amplification observed in legacy key–value stores, and LFA is a data ingestion scheme intended to remove double data write issues in legacy write-ahead logging implementations. We implement a prototype key–value store, Sen-Store, which incorporates our proposals, and we evaluate its performance using synthetic workloads and the TPCx-IoT benchmark. The evaluation results show that Sen-Store achieves up to 17.6× and 2.1× higher IoTps than industry-leading RocksDB and state-of-the-art IoTDB, respectively.
Keywords: Industrial IoT; IoT gateway; Key–value store; Sensor data store; Index structure; Write amplification

Luís Leira, Miguel Luís, Susana Sargento,
Context-based caching in mobile information-centric networks,
Computer Communications,
Volume 193,
2022,
Pages 214-223,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.07.017.
(https://www.sciencedirect.com/science/article/pii/S014036642200264X)
Abstract: Wireless networking is expected to sustain the direct interaction between personal users’ devices, and to provide connectivity on large-scale resource-constrained devices. However, conventional networking protocols fail in large scale mobile wireless environments, due to node mobility, dynamic topologies, and intermittent connectivity. Information-Centric Networking (ICN) has been considered the most promising candidate to overcome the drawbacks of host-centric architectures where Named Data Networking (NDN) is one of the well-known and studied architectures within the ICN paradigm. The main objective of this work is to improve both content availability and network performance in mobile environments regarding the ICN paradigm. This is provided through a context-based approach for the caching admission policy providing in-network caching and content replication, facilitating the efficient and timely delivery of information. Content popularity, freshness, proximity, source mobility type and network density are some of the metrics considered in the caching decision. We conducted a comparative study between our proposal and the NDN caching strategy by using two different datasets with real mobility and connectivity traces, addressing intermittent communication. According to our results, we observed that using a multi-criteria context-based cache admission policy improves cache hits, cache evictions, and request satisfaction ratios in mobile environments, thus improving content delivery and network efficiency.
Keywords: Mobile networks; Information-Centric Networking; Named Data Networking; In-network caching; Context-based caching

Mengda Yang, Wenzhe Yi, Juan Wang, Hongxin Hu, Xiaoyang Xu, Ziang Li,
Penetralium: Privacy-preserving and memory-efficient neural network inference at the edge,
Future Generation Computer Systems,
Volume 156,
2024,
Pages 30-41,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.03.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000797)
Abstract: The proliferation of artificial intelligence and edge computing has led to an increase in the deployment of proprietary deep learning models on third-party edge servers or devices to power mission-critical applications. However, this trend raises concerns about model privacy, particularly on untrusted edge platforms. Protecting model privacy in such scenarios requires addressing challenges such as untrustworthy model deployment environments, resource-constrained Trusted Execution Environments (TEE), and vulnerability to privacy inference attacks. To address these challenges, this paper proposes Penetralium, a system-algorithm jointly optimized model inference system on edge computing platforms. Penetralium runs models in the TEE by building an underlying computational engine. We propose an adaptive decomposition algorithm that builds a computing pipeline for models, which adapts to the underlying trusted components. Additionally, Penetralium uses a lightweight confidence score perturbation policy to protect against advanced privacy inference attacks on deep learning models. Experimental results demonstrate that Penetralium provides strong security guarantees with reasonable performance. The system not only reduces inference latency and memory consumption overhead but also improves the overall robustness of the system against advanced attacks.
Keywords: Deep learning; Model inference; Privacy protection; Trusted execution environment

Shaohua Cao, Di Liu, Congcong Dai, Chengqi Wang, Yansheng Yang, Weishan Zhang, Danyang Zheng,
Reinforcement learning based tasks offloading in vehicular edge computing networks,
Computer Networks,
Volume 234,
2023,
109894,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109894.
(https://www.sciencedirect.com/science/article/pii/S1389128623003390)
Abstract: With the rapid development of autonomous and intelligent techniques, vehicles are currently equipped with computation and communication modules for satisfying clients’ on-vehicle computing requests. To meet the client’s on-vehicle computation requests such as on-vehicle games and self-driving mechanisms, vehicles have to continuously generate computational tasks. However, due to the limited on-vehicle computation capacities, it is barely possible to handle the above requests by the vehicle itself. These requests are then offloaded to special devices such as roadside units or intelligent vehicles. With the fluid feature of the traffic, more requests are generated during the peak hours than at the low hours. Based on the above facts, two significant challenges arise in vehicular edge computing networks: (i) how to accurately determine whether the vehicular networks are in peak or low hours, and (ii) how to effectively offload the generated requests? In this paper, to tackle the above challenges, we investigate the problem of computational requests offloading under different vehicular networking scenarios. To handle the first challenge, we propose the fuzzy inference-based algorithm to identify the situation of the vehicular network (i.e., whether it is in peak hours or low hours). We employ the reinforcement learning-based algorithm for the second challenge to offload the computational requests effectively. Experiments show that our schemes outperform the benchmark by an average of 24.8% regarding resource utilization when satisfying the interests of both service providers and clients.
Keywords: Mobile edge computing; Reinforcement learning; Task offloading; Fuzzy inference; Internet of vehicles

Hisham Al-Ward, Chee Keong Tan, Wern Han Lim,
Caching transient data in Information-Centric Internet-of-Things (IC-IoT) networks: A survey,
Journal of Network and Computer Applications,
Volume 206,
2022,
103491,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103491.
(https://www.sciencedirect.com/science/article/pii/S1084804522001333)
Abstract: The Information-Centric Internet-of-Things (IC-IoT) will connect billions of devices to the Internet, which allows for many remarkable applications like smart homes, smart grids, smart transport, and digital health to become a reality. However, such a massive number of connections are certain to generate huge amounts of data and present a real challenge to the network. Caching is one key solution to this problem; however, traditional caching solutions are ineffective in dealing with IoT-produced content due to its transient nature. As such, research on developing caching solutions for transient IC-IoT data has been gaining momentum. This paper aims to provide context for the research efforts in the field, highlight the significance of freshness-aware solutions, demonstrate the contributions of current works, and identify areas for future developments. As such, this article first provides a high-level overview of IC-IoT, caching principles, and data freshness. We then thoroughly examine the state-of-the-art cache placement, cache replacement, and cache coherence policies for transient IC-IoT data. After that, the surveyed works are analyzed from four perspectives: (1) cache deployment, (2) service model, (3) evaluation tools, and (4) evaluation metrics and baselines. Lastly, we conclude this paper by outlining what we believe to be the most promising six research directions for caching transient data in the IC-IoT domain.
Keywords: Transient data; Caching; Internet of Things (IoT); IoT caching; Cache coherence; Cache placement; Cache replacement; Information-Centric Networking (ICN)

Huabing Zhang, Liang Li, Qiong Lu, Yi Yue, Yakun Huang, Schahram Dustdar,
Distributed realtime rendering in decentralized network for mobile web augmented reality,
Future Generation Computer Systems,
2024,
,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.04.050.
(https://www.sciencedirect.com/science/article/pii/S0167739X24001924)
Abstract: Low-latency, real-time rendering of 3D objects is critical for mobile web-based augmented reality (MWAR) applications. While cloud-based or edge-based server rendering offloading can reduce the latency for mobile devices, a high volume of user requests in user aggregation scenarios can overload servers and transmission channels. It can result in a poor user experience due to resource consumption and high latency. This paper presents a decentralized and collaborative real-time rendering offloading network architecture (CRCDnet) to address this issue. CRCDnet makes contributions in the following three areas: (1) In the user aggregation scenarios, mobile devices that perform the same services are used as service nodes to perform the offloading of rendering computing and a collaborative rendering computing network is established. (2) For data exchange in decentralized networks, a data sharing middle layer based on blockchain key indexes separates sensitive service data and ensures a secure, reliable exchange mechanism and efficient data exchange. (3) A data request and computing offloading scheduling approach is proposed for the collaborative rendering computing network to optimize the rendering computation delay.
Keywords: Mobile web-based augment reality; Real-time rendering computing; Decentralized network; Distributed collaborative computing; Mobile computing; Device-to-Device

Gengbiao Shen, Qing Li, Wanxin Shi, Yong Jiang, Pei Zhang, Liang Gu, Mingwei Xu,
Modeling and optimization of the data plane in the SDN-based DCN by queuing theory,
Journal of Network and Computer Applications,
Volume 207,
2022,
103481,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103481.
(https://www.sciencedirect.com/science/article/pii/S108480452200128X)
Abstract: Complicated management tasks and various functions of traditional Data Center Network (DCNs) motivate the research on Software-Defined Networking (SDN). However, the limited flow table space in the SDN switch has a significant impact on the data plane especially the availability. In this paper, we propose a theoretical model based on queuing theory to estimate the flow table states of SDN switches, providing operators insights into the requirements of flow table resources and guaranteeing the performance of the data plane. First, we analyze the lifecycle of a flow table entry and construct a queuing theory based estimation model to estimate flow table states in the data plane. Second, we observe the real workloads of DCNs and figure out the probability characteristics of routing strategies to calculate the key parameters in our model. Third, for achieving an optimally latency-aware data plane, we propose an auxiliary method, Amora, to maintain the availability of the data plane by improving routing decisions with the consideration of the optimal flow table states obtained by our model. Comprehensive experiments show that the relative estimation error of flow table states in our model is around 10%, which exposes the actual flow table states and benefits the selection of proper switches. Moreover, Amora efficiently eliminates the failure cases of establishing routing paths in the data plane, which further demonstrates the precision and practicability of our estimation model.
Keywords: Software-defined networking; Queuing theory; Flow table; Data center networks; Network optimization

Zhenchun Wei, Jie Pan, Zengwei Lyu, Junyi Xu, Lei Shi, Juan Xu,
An offloading strategy with soft time windows in mobile edge computing,
Computer Communications,
Volume 164,
2020,
Pages 42-49,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.09.011.
(https://www.sciencedirect.com/science/article/pii/S0140366420319319)
Abstract: Task offloading is a core function of mobile edge computing to make up shortage of energy in mobile terminal. Scholars deploy techniques including cloud servers to improve offloading process with hard time window constraints, as there is strict deadline, which is exclusive to the flexibility of users facing task time-out. To increase the flexibility, this paper examines the time-varying features of users’ tolerance as the soft time windows perform. Pay of task processing varies because of completion time, which is a combinatorial optimization problem. This paper proposes a Hybrid Genetic Algorithm and Biogeography-Based Optimization (HGABBO) offloading algorithm. The experiments show that the proposed algorithm is effective to multitask processes and outperforms the Artificial Fish algorithm and Ant System algorithm.
Keywords: Mobile edge computing; Soft time window; Offloading; Biogeography-based optimization algorithm

Mennan Selimi, Adisorn Lertsinsrubtavee, Arjuna Sathiaseelan, Llorenç Cerdà-Alabern, Leandro Navarro,
PiCasso: Enabling information-centric multi-tenancy at the edge of community mesh networks,
Computer Networks,
Volume 164,
2019,
106897,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106897.
(https://www.sciencedirect.com/science/article/pii/S1389128618312787)
Abstract: Edge computing is radically shaping the way Internet services are run by enabling computations to be available close to the users - thus mitigating the latency and performance challenges faced in today’s Internet infrastructure. Emerging markets, rural and remote communities are further away from the cloud and edge computing has indeed become an essential panacea. Many solutions have been recently proposed to facilitate efficient service delivery in edge data centers. However, we argue that those solutions cannot fully support the operations in Community Mesh Networks (CMNs) since the network connection may be less reliable and exhibit variable performance. In this paper, we propose to leverage lightweight virtualisation, Information-Centric Networking (ICN), and service deployment algorithms to overcome these limitations. The proposal is implemented in the PiCasso system, which utilises in-network caching and name based routing of ICN, combined with our HANET (HArdware and NETwork Resources) service deployment heuristic, to optimise the forwarding path of service delivery in a network zone. We analyse the data collected from the Guifi.net Sants network zone, to develop a smart heuristic for the service deployment in that zone. Through a real deployment in Guifi.net, we show that HANET improves the response time up to 53% and 28.7% for stateless and stateful services respectively. PiCasso achieves 43% traffic reduction on service delivery in our real deployment, compared to the traditional host-centric communication. The overall effect of our ICN platform is that most content and service delivery requests can be satisfied very close to the client device, many times just one hop away, decoupling QoS from intra-network traffic and origin server load.
Keywords: Information-centric networking; Community networks; Mesh networks; Edge computing; Common-pool resources.

Rohit Kumar, Venkanna U., Vivek Tiwari,
Optimized traffic engineering in Software Defined Wireless Network based IoT (SDWN-IoT): State-of-the-art, research opportunities and challenges,
Computer Science Review,
Volume 49,
2023,
100572,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100572.
(https://www.sciencedirect.com/science/article/pii/S1574013723000394)
Abstract: Wireless networks have been in focus since the last few decades due to their indispensable role in the future generation networks like the Internet of Things (IoT). However, the associated challenges in wireless network implementation such as distance, line-of-sight, interference, weather, power issues, etc., affect the performance adversely. Software Defined Networking (SDN) is a future generation networking technology and has been proven to alleviate the performance challenges in the existing wireless IoT networks. It helps to evolve the wireless IoT domain in the form of Software Defined Wireless Network based IoT (SDWN-IoT). Traffic Engineering (TE) has been part of traditional network designs since long back, to improve the performance of the communication networks. However, its more optimized forms and their usefulness in SDWN-IoT networks have been under active investigation. This work explores the existing literature related to the major types of SDWN-IoT networks namely, Software Defined Wireless Sensor Network based IoT (SDWSN-IoT) and Software Defined Wireless Mesh Network based IoT (SDWMN-IoT). Additionally, the article also draws some useful inferences, and compares respective contributions and shortcomings. Finally, various research opportunities and challenges have been discussed with respect to the SDWSN-IoT and SDWMN-IoT networks.
Keywords: Internet of Things (IoT); Software Defined Networking (SDN); Software Defined Wireless Sensor Network based IoT (SDWSN-IoT); Software Defined Wireless Mesh Network based IoT (SDWMN-IoT); Traffic Engineering (TE)

Sahand Khodaparas, Abderrahim Benslimane, Saleh Yousefi,
A software-defined caching scheme for the Internet of Things,
Computer Communications,
Volume 158,
2020,
Pages 178-188,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.05.002.
(https://www.sciencedirect.com/science/article/pii/S0140366419316780)
Abstract: Exploiting Content-Centric Networking (CCN) caching capabilities in which contents are cached on intermediate nodes can be beneficial in IoT as it can decrease the latency, reduce required transmission hops, limit traffic load on the content producer and improves availability. This article presents a new scheme for caching the contents in IoT environments. In the proposed method, devices are grouped into clusters where the cluster heads act as the cache controller. Additionally, we consider a global SDN/Cache controller (GSCC), which is responsible for orchestrating cache decisions in the whole IoT network. Such a centrally managed caching system increases the efficiency of resource usage in the IoT network. In the proposed scheme, the decision about caching the content is made in three steps: 1) determining the value of content and make decisions about caching it, 2) determining the candidate cluster, and 3) selecting candidate nodes for caching the content. In each step, several metrics are taken into account, and Multi-Criteria Decision Making (MCDM) approaches such as Analytical Hierarchy Process (AHP) and TOPSIS are used to select the best option based on considered criteria. Simulation results show that our proposed caching method can achieve an average cache hit rate of 72% and decreases the average hop count of content retrievals by 42%. Moreover, our results confirm the superiority of our algorithm over some existing methods in terms of different evaluation metrics.
Keywords: Content-centric networking; Software defined networking; Internet of Things; Caching; MCDM

Mohd Hirzi Adnan, Zuriati Ahmad Zukarnain, Oluwatosin Ahmed Amodu,
Fundamental design aspects of UAV-enabled MEC systems: A review on models, challenges, and future opportunities,
Computer Science Review,
Volume 51,
2024,
100615,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100615.
(https://www.sciencedirect.com/science/article/pii/S1574013723000825)
Abstract: The huge prospects of the internet of things (IoT) have led to an ever-growing demand for computing power by IoT users to enable various applications. Multi-access edge computing (MEC) research and development has rapidly gained attention during the last decade. The ability to deploy edge servers at different points across a content delivery network that can offer communication and computing services close to mobile user devices is one of the main factors driving the evolution of MEC. Furthermore, MEC has been considered a potentially transformational approach for fifth-generation (5 G) and beyond 5 G (B5G) networks, as well as a potential improvement to conventional cloud computing. Unmanned aerial vehicles (UAVs) can be used as effective aerial platforms to offer reliable and ubiquitous connections in wireless communication networks due to their distinctive qualities, such as high cruising altitude, on-demand deployment, and three-dimensional (3D) maneuverability. The number of research studies published in this area has dramatically increased due to the growing interest in UAV-enabled MEC. Although UAV-enabled MEC systems have been well studied, the existing models are becoming increasingly heterogeneous and scattered without harmony. This paper provides a comprehensive analysis of the literature on UAV-enabled MEC systems with a special focus on the system modeling, and optimization techniques for five identified domains, such as energy efficiency, resource allocation, trajectory control, latency, and security. For each domain, we have highlighted the recent advances, critical findings, and the advantages and disadvantages. Additionally, the identified proposed techniques were analyzed and discussed, with emphasize on the constraints and performance metrics. We also discuss a general system model for each highlighted domain. Moreover, the lessons are also derived from the study on system optimization and system modeling techniques identified in this paper. Then we discuss open issues related to UAV-enabled MEC systems in each highlighted domain, including problem formulation and optimization techniques. Finally, this paper lay out directions for future research to solve the aforementioned problems associated with UAV-enabled MEC systems.
Keywords: Mobile edge computing; Unmanned aerial vehicles; UAV-enabled MEC; Energy efficient; Resource allocation; Trajectory control; Latency; Privacy

Christian Grasso, Raoul Raftopoulos, Giovanni Schembra, Salvatore Serrano,
H-HOME: A learning framework of federated FANETs to provide edge computing to future delay-constrained IoT systems,
Computer Networks,
Volume 219,
2022,
109449,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109449.
(https://www.sciencedirect.com/science/article/pii/S1389128622004832)
Abstract: In 6G systems, it will be mandatory that the network is able to support edge computing powered by Artificial Intelligence (AI) to provide mobile devices with the opportunity of job offloading for computation, so implementing the new paradigm of Intelligent Internet of Intelligent Things (IIoIT). In areas that are very difficult to be covered by the structured networks, Flying Ad-Hoc Networks (FANET) can be considered one of the most promising technologies to enhance coverage, capacity, reliability, and energy efficiency of wireless cellular networks, also providing edge-computing services. The goal of this paper is to propose a two-layer Hierarchical Horizontal-Offload ManagEment (H-HOME) framework for horizontal offload among the Unmanned Aerial Vehicles (UAV) of the same FANET, in order to minimize processing delay and jitter. The framework exploits Federated Reinforcement Learning in order to take advantage of knowledge sharing without incurring into problems of privacy and network overloading. A Markov Decision Process (MDP) is also defined to optimize decisions of the FANET Orchestrator (FO). Simulation results, obtained from two different analyses, demonstrate that H-HOME outperforms the traditional local training approach, based on simple recursive learning, and it can be effectively used to reduce power consumption and increase FANETs flight autonomy.
Keywords: 6G; Zero-touch network management; UAVs; Deep reinforcement learning; Federated learning; Green networking

Jinyuan Zhao, Zhigang Hu, Bing Xiong, Liu Yang, Keqin Li,
Modeling and optimization of packet forwarding performance in software-defined WAN,
Future Generation Computer Systems,
Volume 106,
2020,
Pages 412-425,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.12.010.
(https://www.sciencedirect.com/science/article/pii/S0167739X19315377)
Abstract: As a novel network paradigm, Software-Defined Networking (SDN) offers numerous benefits for wide-area networks (WAN), like promoting application performance and reducing deployment costs. However, it also comes along with an inherent penalty to essential network performance such as packet forwarding delay, primarily due to the involvement of logically centralized controllers. This paper is motivated to provide an accurate queueing system of packet forwarding performance in software-defined WAN based on modeling its controller cluster and OpenFlow switches. In particular, we approximate the packet-in message processing of the controller cluster as an M∕M∕n queue based on the derivation of its message arrival process. Meanwhile, we characterize the packet processing of an OpenFlow switch as an M∕G∕1 queue after taking an insight into its packet switching process. As a further step, we build an optimization model of controller cluster deployments to obtain the optimal number of controllers in the cluster. Finally, our proposed queueing model of SDN controller cluster is evaluated with the prevalent benchmark OFsuite_Performance by experiments, and their results indicate that our proposed model provides a more accurate approximation of controller cluster performance. Furthermore, we perform numerical analysis on packet forwarding delay and solve the optimal number of controllers for different varying parameters, which offer effective guidelines for software-defined WAN deployments.
Keywords: Controller cluster deployments; OpenFlow switches; Optimization models; Queueing system; Software-defined networking; Wide-area networks

Linbo Long, Jinpei Du, Xuxu Deng, Renping Liu, Yi Jiang, Yan Wang,
Optimizing data placement and size configuration for morphable NVM based SPM in embedded multicore systems,
Future Generation Computer Systems,
Volume 135,
2022,
Pages 270-282,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.05.005.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001704)
Abstract: Embedded multicore systems are widely designed to meet the high-performance requirement. Meanwhile, many embedded multicore systems are equipped with multiple scratchpad memories (SPM) because of their advantages in power efficiency and small area. Whereas, the traditional SRAM-based SPM is limited by its leakage power and capacity. Compared with SRAM, emerging non-volatile memory (NVM) has lower power consumption. Moreover, some morphable NVMs like resistive memory can build a “morphable NVM” to get a balance between capacity and performance, in which a memory cell can be converted between high-density MLC (multi-level cell) and low-latency SLC (single-level cell). Considering the features of NVM, this paper proposes a dynamic data placement and size configuration technique to fully utilize the benefits from morphable NVM based SPM in embedded multicore systems. The core idea is to dynamically convert the memory mode of NVM in each SPM with different workloads, and store the associated data into an optimal storage location for minimizing the total cost of memory access. Therefore, combining with the access pattern of embedded systems, an Integer Linear Programming (ILP) model is first explored for obtaining the optimal data placement and size configuration of SLC/MLC of SPMs. Then, our polynomial-time algorithm (CDA) is provided to obtain a near-optimal result. Finally, our experiments based on the gem5 simulator exhibit the proposed techniques can achieve the performance improvement and the reduction of energy consumption compared with the baseline scheme.
Keywords: Embedded multicore systems; Data placement; Size configuration; Morphable non-volatile memory; On-chip memory; Scratchpad memory

Huan Yang, Sheng Sun, Min Liu, Qiuping Zhang, Yuwei Wang,
MJOA-MU: End-to-edge collaborative computation for DNN inference based on model uploading,
Computer Networks,
Volume 231,
2023,
109801,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109801.
(https://www.sciencedirect.com/science/article/pii/S1389128623002463)
Abstract: As an emerging computing paradigm, edge computing can assist user equipments (UEs) in executing computation-intensive deep neural network (DNN) inference tasks, thereby satisfying the stringent QoS requirement and relieving the burden of UEs. Due to the customizability of DNN models and limited capacity of the edge server, it is more realistic to upload DNN models on demand during end-to-edge co-inference, instead of deploying all DNN models at the edge server in advance. Existing works adopt the serial model uploading manner that uploads subsequent DNN layers only after antecedent DNN layers finish execution, inevitably prolonging the DNN execution latency. To this end, we innovatively design a parallel-efficient model uploading mechanism that allows subsequent DNN layers to be uploaded simultaneously when executing antecedent DNN layers, so as to efficiently mitigate the performance drop caused by model uploading. On this basis, we propose a Multi-UE Joint Optimization Algorithm based on Model Uploading (MJOA-MU) to optimize DNN partitioning and resource allocation for heterogeneous UEs. Specifically, MJOA-MU includes a Pruned Binary Tree based DNN Partitioning (PBT-DP) sub-algorithm to efficiently make the near-optimal partitioning decision for chain and non-chain models based on the long-term influence between DNN layers, and an Asynchronous Resource Allocation (ARA) sub-algorithm to allocate computation and communication resources for UEs by quantifying the inner- and inter-association, so as to match with individual demand and resource budget. Extensive simulation results demonstrate that MJOA-MU outperforms the state-of-the-art in terms of the DNN execution latency, and specifically achieves up to 64.5% reduction.
Keywords: DNN inference; Model uploading; DNN partitioning; Resource allocation

Jiawei Lu, Jielin Jiang, Venki Balasubramanian, Mohammad R. Khosravi, Xiaolong Xu,
Deep reinforcement learning-based multi-objective edge server placement in Internet of Vehicles,
Computer Communications,
Volume 187,
2022,
Pages 172-180,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.02.011.
(https://www.sciencedirect.com/science/article/pii/S0140366422000548)
Abstract: In the typical scenario of the Internet of Vehicles (IoV), the edge servers (ESs) are laid out near the road side units (RSUs) to process the collected data for a variety of IoV services in real time. Generally, because ESs are lightweight compared with cloud servers, if the ESs are not appropriately distributed, it will cause the unbalanced workload of the ESs. Thus, developing an ES plan to avoid the risk of overload and improve the quality of service (QoS) remains a challenge. To tackle it, a deep reinforcement learning-based multi-objective edge server placement strategy, named DESP, is fully explored, to promote the coverage rate, the workload balancing and reduce the average delay of finishing tasks in the IoV. In particular, the Markov Decision Process (MDP) of the ES placement problem is formulated and the deep reinforcement learning, i.e., Deep Q-Network (DQN) is applied to obtain the optimal placement scheme achieving the multiple objectives above. At last, a real vehicular data set is used for assessing the validity of DESP.
Keywords: Edge server placement; IoV; Deep reinforcement learning; DQN

M. Luglio, S.P. Romano, C. Roseti, F. Zampognaro,
Satellite multi-beam multicast support for an efficient community-based CDN,
Computer Networks,
Volume 217,
2022,
109352,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109352.
(https://www.sciencedirect.com/science/article/pii/S1389128622003863)
Abstract: The design and implementation of an efficient end-to-end IP-based infrastructure for the delivery of multimedia content is of paramount importance in current public networks, dominated by the constant growth of video streaming traffic. Content Delivery Networks (CDNs) represent the main technological solution to manage the huge volumes of traffic involved, by guaranteeing high quality levels to applications and low impact to the core networks, thanks to the efficient distribution of content among edge caches that are located as close as possible to end-users. Nonetheless, the hierarchical data distribution associated with CDNs can be in principle subject to inefficiencies, as well as performance limitations in case of congested segments along the end-to-end delivery path. In this context, we propose the exploitation of satellite multicast capabilities offered by modern high throughput satellite (HTS) platforms, in a virtualized-compatible model, to compensate for flaws of terrestrial networks. The role of satellite communication is to offer multicast support (as a complement to landline connectivity) with wide geographical service areas based on the available satellite beams, enabling a popularity-based content distribution support. In addition, multi-beam satellite technology allows for a more fine-grained approach to popularity evaluation based on user location. The proposed service and the related network configuration are described in the paper, in relation with current and future SatCom platforms. We then present the results of the proposed CDN caching algorithms in a simulated environment, showing promising results associated with a preliminary performance evaluation.
Keywords: CDN; Multicast; SatCom; Very High Throughput Satellite; Popularity

Hai Lin, Sherali Zeadally, Zhihong Chen, Houda Labiod, Lusheng Wang,
A survey on computation offloading modeling for edge computing,
Journal of Network and Computer Applications,
Volume 169,
2020,
102781,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102781.
(https://www.sciencedirect.com/science/article/pii/S1084804520302551)
Abstract: As a promising technology, edge computing extends computation, communication, and storage facilities toward the edge of a network. This new computing paradigm opens up new challenges, among which computation offloading is considered to be the most important one. Computation offloading enables end devices to offload computation tasks to edge servers and receive the results after the servers' execution of the tasks. In computation offloading, offloading modeling plays a crucial role in determining the overall edge computing performance. We present a comprehensive overview on the past development as well as the recent advances in research areas related to offloading modeling in edge computing. First, we present some important edge computing architectures and classify the previous works on computation offloading into different categories. Second, we discuss some basic models such as channel model, computation and communication model, and energy harvesting model that have been proposed in offloading modeling. Next, we elaborate on different offloading modeling methods which are based on (non-)convex optimization, Markov decision process, game theory, Lyapunov optimization, or machine learning. Finally, we highlight and discuss some research directions and challenges in the area of offloading modeling in edge computing.
Keywords: Computation offloading; Edge computing; Modeling

Yan Ding, Kenli Li, Chubo Liu, Zhuo Tang, Keqin Li,
Short- and long-term cost and performance optimization for mobile user equipments,
Journal of Parallel and Distributed Computing,
Volume 150,
2021,
Pages 69-84,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.12.006.
(https://www.sciencedirect.com/science/article/pii/S0743731520304226)
Abstract: Task offloading strategy optimization in mobile edge computing (MEC) has always been a hot issue. However, the mobility of a user equipment (UE) seriously affects the UE’s cost and performance. This paper proposes three mobility types depending on whether the mobility characteristic of a UE is known, and formulates an energy minimization problem and a latency minimization problem to optimize the cost and performance, respectively. We first develop greedy strategy based task offloading algorithms for UEs according to their mobility characteristics. However, accurately obtaining the mobility characteristics of the UEs over a long time in practice is a huge challenge, especially in a highly random environment like the MEC. To address the issue, we use a Lyapunov optimization method to develop the algorithms that do not require any prior knowledge of the mobility characteristics to minimize the long-term energy and latency of UEs. Experimental results show that the greedy strategy based algorithms can optimize the cost and performance of UEs by using their mobility characteristics, and perform better than the Lyapunov optimization based algorithms in a short-term. However, the Lyapunov optimization based algorithms perform better than the greedy strategy based algorithms over a long-term.
Keywords: Greedy strategy; Lyapunov optimization; Mobile edge computing; Mobility characteristic; Task offloading strategy

Xijian Luo, Jun Xie, Liqin Xiong, Zhen Wang, Yaqun Liu,
UAV-assisted fair communications for multi-pair users: A multi-agent deep reinforcement learning method,
Computer Networks,
Volume 242,
2024,
110277,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110277.
(https://www.sciencedirect.com/science/article/pii/S1389128624001099)
Abstract: Unmanned aerial vehicle (UAV) plays an important role in scenarios like search and rescue, remote communication relay, battlefield mobile networks, etc. In this paper, we investigate multiple UAV relays providing real-time transmission for multi-pair ground users (GUs) in the absence of ground-based stations (GBSs). Due to the limited load capacities and energy resources on-board, fairness between multi-pair users, throughput maximization, as well as the connectivity maintenance between UAVs are jointly considered. We formulate the energy-efficient fair throughput objective function, which turns to be non-convex with hybrid variables. To solve this intractable problem, we utilize the power of neural networks in function approximation and propose a multi-agent deep reinforcement learning (MADRL)-based algorithm. Different from traditional MADRL algorithms, we utilize the method, named independent proximal policy optimization (IPPO), which allows agents to update according to their own observations memory and encourages more explorations to some extend, as the base of our solution. Simulation results demonstrate that our algorithm outperforms some baselines in terms of fairness, throughput as well as energy consumption.
Keywords: Fair communications; Multi-pair users; MADRL

Jian Sun,
Research on resource allocation of vocal music teaching system based on mobile edge computing,
Computer Communications,
Volume 160,
2020,
Pages 342-350,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.05.016.
(https://www.sciencedirect.com/science/article/pii/S0140366420303716)
Abstract: How to allocate vocal teaching system resources plays an important role in improving the performance of vocal teaching systems. With the development of mobile edge computing, the calculation and storage of resource data can be adapted to the operating needs of vocal teaching systems. This study proposes a method of system resource allocation based on power iteration and sets the throughput of the unloading process as an objective function and realizes the optimal allocation of normal power by iterative optimization. At the same time, in view of the low energy efficiency and resource utilization of edge servers, a heterogeneous network based on edge servers is proposed to find the optimal strategy based on ensuring the Nash equilibrium point. In addition, in order to verify the performance of the algorithm in this paper, a comparative experiment is performed by designing simulation experiments. The results show that the method proposed in this paper has certain effects and strong practicability, which can provide theoretical references for subsequent related research.
Keywords: Mobile edge computing; Vocal music; System resources; Resource allocation; Nash equilibrium

Haifeng Lu, Chunhua Gu, Fei Luo, Weichao Ding, Xinping Liu,
Optimization of lightweight task offloading strategy for mobile edge computing based on deep reinforcement learning,
Future Generation Computer Systems,
Volume 102,
2020,
Pages 847-861,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.07.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X19308209)
Abstract: With the maturity of 5G technology and the popularity of intelligent terminal devices, the traditional cloud computing service model cannot deal with the explosive growth of business data quickly. Therefore, the purpose of mobile edge computing (MEC) is to effectively solve problems such as latency and network load. In this paper, deep reinforcement learning (DRL) is first proposed to solve the offloading problem of multiple service nodes for the cluster and multiple dependencies for mobile tasks in large-scale heterogeneous MEC. Then the paper uses the LSTM network layer and the candidate network set to improve the DQN algorithm in combination with the actual environment of the MEC. Finally, the task offloading problem is simulated by using iFogSim and Google Cluster Trace. The simulation results show that the offloading strategy based on the improved IDRQN algorithm has better performance in energy consumption, load balancing, latency and average execution time than other algorithms.
Keywords: Mobile edge computing; Task offloading; Deep reinforcement learning; LSTM network; Candidate network

Bowen Liu, Shunmei Meng, Xutong Jiang, Xiaolong Xu, Lianyong Qi, Wanchun Dou,
A QoS-guaranteed online user data deployment method in edge cloud computing environment,
Journal of Systems Architecture,
Volume 118,
2021,
102185,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102185.
(https://www.sciencedirect.com/science/article/pii/S1383762121001326)
Abstract: App vendors now have a new way to deploy applications on edge servers thanks to the rise of edge cloud computing. App vendors will be able to deliver better services to users throughout this form. Compared to the traditional centralized system, the new distributed computing paradigm constructs a novel cyber–physical–social system. However, how to design a data deployment method to meet the requirement of the CPS system is a challenge. A QoS-guaranteed edge user data deployment method needs to maximize the reduction in service latency and minimize the overall system cost (SC) with available resources. The EDD (edge data deployment) problem is formulated as an online problem and we propose a convinced method to solving this problem. We analyze the performance of our approach theoretically and conduct experiments to compare the proposed method to three different baseline methods with a real-word dataset. Our method’s efficiency and efficacy are shown by the experimental results.
Keywords: Edge cloud computing; Data deployment; Resource management; QoS guaranteed; Constraint optimization

Elham Karimi, Yuanzhu Chen, Behzad Akbari,
Task offloading in vehicular edge computing networks via deep reinforcement learning,
Computer Communications,
Volume 189,
2022,
Pages 193-204,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.04.006.
(https://www.sciencedirect.com/science/article/pii/S0140366422001104)
Abstract: Given the rapid increase of various applications in vehicular networks, it is crucial to consider a flexible architecture to improve the Quality of Service (QoS). Utilizing Multi-access Edge Computing (MEC) as a distributed paradigm, with resource capabilities closer to the vehicles, would be a promising solution to reduce response time in such a network. However, MEC suffers from limited resources and is deprived of handling high mobilities with many diverse applications. This paper proposes cooperation between MEC and central cloud decisions for different vehicular application offloading. We formulate a new resource allocation problem to guarantee the required response time. To solve such an NP-hard problem, we utilize deep reinforcement learning, a proper computational model, to automatically learn the dynamics of the network state and rapidly capture an optimal solution. Extensive numerical analysis and results illustrate how our proposed scheme can achieve a high acceptance rate with a low response time.
Keywords: Vehicular networks; Multi-access edge computing; Central cloud; Resource allocation; Task offloading; Response time; Deep reinforcement learning

Xiang Ju, Shengchao Su, Chaojie Xu, Haoxuan Wang,
Computation offloading and tasks scheduling for the internet of vehicles in edge computing: A deep reinforcement learning-based pointer network approach,
Computer Networks,
Volume 223,
2023,
109572,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109572.
(https://www.sciencedirect.com/science/article/pii/S1389128623000178)
Abstract: In the Internet of Vehicles, vehicles can offload computation tasks to edge servers for execution. So, execution delay of tasks and energy consumption of vehicles can be reduced. However, the current research on computation offloading has not fully considered the priority of computation offloading and tasks scheduling within the server. In this regard, a computation offloading and task scheduling scheme based on pointer network was proposed in the paper. This scheme can maximize the number of offloading executions of computation tasks with limited computing resources of the edge server while satisfying the priority of computation offloading. First, depending on whether the task can be executed in the vehicle's device, we divided the task into two types and gave a two-stage offloading policy. Next, according to the characteristics of the scheduling problem, a task offloading decision and scheduling scheme based on pointer network was proposed. Then, considering the uncertainty of the number of tasks, the extra time delay caused by task offloading, the waiting time of tasks, and the complexity of task scheduling and computing resource allocation, a deep reinforcement learning algorithm was used to train the pointer network. Finally, the trained pointer network was used for task offloading decision-making and scheduling. The experimental results revealed the effectiveness of the scheme proposed in this study.
Keywords: Internet of vehicles; Edge computing; Task scheduling; Pointer network; Deep reinforcement learning

Srujan Teja Thomdapu, Palash Katiyar, Ketan Rajawat,
Dynamic cache management in content delivery networks,
Computer Networks,
Volume 187,
2021,
107822,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107822.
(https://www.sciencedirect.com/science/article/pii/S1389128621000116)
Abstract: The importance of content delivery networks (CDN) continues to rise with the exponential increase in generation and consumption of electronic media. In order to ensure high quality of experience, CDNs often deploy cache servers that capable of storing some of the popular files close to users. Such edge caching solutions not only increase the content availability, but also result in higher download rates and lower latency. Different from the classical eviction-based algorithms, the present work formulates the content placement problem from an optimization perspective and puts forth an online algorithm for the same. In contrast to the existing optimization-based solutions, the proposed algorithm is incremental and incurs very low computation cost, while yielding storage allocations that are provably near-optimal. The proposed algorithm can handle time-varying content popularity, thereby obviating the need for periodically estimating demand distribution. Using synthetic and real IPTV data, we show that the proposed policies outperform the traditional eviction based techniques in terms of various metrics.
Keywords: Content delivery networks; Incremental dual ascent; Content placement

Zhao Tong, Jiake Wang, Jing Mei, Kenli Li, Wenbin Li, Keqin Li,
Multi-type task offloading for wireless Internet of Things by federated deep reinforcement learning,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 536-549,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001383)
Abstract: With the popularity of Internet of Things (IoT) smart devices, the amount of data generated by these devices has grown rapidly. In these mobile edge computing (MEC) environments, it is not only important to save time and energy in offloading tasks, but also to protect user data. In this paper, due to the dynamics and complexity of the system, a multi-type task offloading based on a multi-capability federated deep Q-network (M2FD) algorithm is proposed to optimize the bi-objective performance. The algorithm consists of two parts, federated learning protects user privacy by transmitting model for training instead of data, and deep reinforcement learning trains model accuracy and identifies suitable offloading nodes with heterogeneous capabilities for multi-type tasks. In addition, under the constraints of the service experience guarantee (SEG) model, the tasks are offloaded with the goal of improving system utility while reducing system cost. Experiments show that the M2FD increases system utility, guarantees privacy, and reduces task response time and energy consumption.
Keywords: Deep reinforcement learning; Federated learning; Internet of Things (IoT); Mobile edge computing (MEC); Multi-type task offloading; Service experience guarantee

Kun Jiang, Shaofeng Du, Fu Zhao, Yong Huang, Chunlin Li, Youlong Luo,
Effective data management strategy and RDD weight cache replacement strategy in Spark,
Computer Communications,
Volume 194,
2022,
Pages 66-85,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.07.008.
(https://www.sciencedirect.com/science/article/pii/S0140366422002559)
Abstract: With the dramatic increase in internet users and their demand for real-time network performance, Spark has distributed computing environment has emerged. It is widely used due to its high-performance caching mechanism and high scalability. In the face of the unpredictability of data access patterns in the current big data environment, the data shuffling phase is prone to the problems of under-utilization of Spark cluster resources, high computational latency, and high task processing latency. Based on this, this paper proposes an intermediate data management strategy based on the data shuffling phase. Firstly, the size of the data generated in the data shuffling phase of the Spark platform is predicted by random sampling. The strength division strategy obtains the skewed data degree to obtain the part with excessive skew deviation. Finally, the adaptive data management strategy is applied to perform the corresponding computation tasks by the data deviation. In addition, to improve the response time, memory usage, and computation latency of Spark applications, an adaptive cache replacement algorithm based on RDD partition weights is proposed, which takes into account the influence of four weight factors such as computation cost, usage times, partition size and life cycle of RDDs by reasonably calculating the RDD partition weight values. Compared with the current mainstream baseline algorithms, the data management algorithm based on the data mash-up phase proposed in this paper can effectively reduce resource usage and computational response latency. The RDD-based partition weighted adaptive cache replacement algorithm proposed in this paper can fully use memory resources and effectively reduce the problem of resource wastage.
Keywords: Data shuffling; Data management; Cache gain; RDD partition weights; Adaptive cache replacement

Uchechukwu Awada, Jiankang Zhang, Sheng Chen, Shuangzhi Li, Shouyi Yang,
EdgeDrones: Co-scheduling of drones for multi-location aerial computing missions,
Journal of Network and Computer Applications,
Volume 215,
2023,
103632,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103632.
(https://www.sciencedirect.com/science/article/pii/S1084804523000516)
Abstract: Low altitude platform (LAP) unmanned aerial vehicles (UAVs), also called drones, are currently being exploited by Edge computing (EC) systems to execute complex resource-hungry use cases, such as virtual reality, smart cities, autonomous vehicles, etc., by attaching portable edge devices on them. However, a typical drone has limited flight time, coupled with the resource-constrained attached edge device, which can jeopardize aerial computing missions if they are not holistically taking into consideration. Moreover, the fundamental challenge is how to co-schedule multi-drone among multi-location where EC services are needed, such that drones are scheduled to maximize the utility from the activities while meeting computing resource and flight time constraints. Therefore, for a given fleet of drones and tasks across disjointed target locations in a city, we derive a machine learning (ML) linear regression model that estimates these tasks resource requirement and execution time. Leveraging this estimation values, we jointly consider each drone’s flight time availability and its attached edge device resource capacity, and formulate a novel Multi-Location Capacitated Mission Scheduling Problem (MLCMSP) that selects suitable drones and co-schedules their flight routes with the least total distance to visit and execute tasks at the target locations. Then, we show that faster scheduling and execution of complex tasks at each location, while considering the inter-task dependencies is important to achieve effective solution for our MLCMSP. Hence, we further propose EdgeDrones, a variant bin-packing optimization approach through gang-scheduling of inter-dependent tasks that co-schedules and co-locates tasks tightly so as to achieve faster execution time, as well as to fully utilize available resources. Extensive experiments on Alibaba cluster trace with information on task dependencies (about 12,207,703 dependencies) show that EdgeDrones achieves up to 73% higher resource utilization, up to 17.6 times faster executions, and up to 2.87 times faster flight travel time compared to the baseline approaches.
Keywords: Edge computing; Aerial computing; Vehicle routing; Linear regression; Execution time; Resource efficiency; Co-location

Wei Qin, Haiming Chen, Lei Wang, Yinshui Xia, Alfredo Nascita, Antonio Pescapè,
MCOTM: Mobility-aware computation offloading and task migration for edge computing in industrial IoT,
Future Generation Computer Systems,
Volume 151,
2024,
Pages 232-241,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.10.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X23003795)
Abstract: Mobility-aware devices are crucial components of Industrial Internet of Things (IIoT). However, they face limitations in terms of battery capacity and computation power, which restrict their ability to provide services requiring broad bandwidth and strong computation power for computation-intensive tasks. While offloading can strengthen device computation power, ineffective offloading decisions result from device mobility and limited adaptability to changes in environmental resources, or are not applicable to the current mobile edge computing (MEC) environment. In this paper, we address these challenges by proposing a mobility-aware computation offloading and task migration approach (MCOTM) based on trajectory and resource prediction to address this issue of mobility offloading, which minimizes task turnaround time and system energy consumption. Simultaneously, our approach enhances the decision agent continuously to decrease task migration rates. MCOTM uses Lagrange interpolation equations to determine the trajectory of mobile devices, and Long Short-Term Memory (LSTM) to track the time-varying resources characteristics in IIoT. These prediction results will be used to assist Deep Deterministic Policy Gradient (DDPG) for making online computation offloading, task migration and resource allocation decisions. Experimental results show that the proposed MCOTM effectively reduces task turnaround time by at least 42% and system energy consumption by 10% while maintaining a low task migration rate of around 50%, even with an increasing number of tasks.
Keywords: IIoT; MEC; Computation offloading; Task migration; Deep reinforcement learning

Tianheng Li, Xiaofan He, Siming Jiang, Juan Liu,
A survey of privacy-preserving offloading methods in mobile-edge computing,
Journal of Network and Computer Applications,
Volume 203,
2022,
103395,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103395.
(https://www.sciencedirect.com/science/article/pii/S1084804522000546)
Abstract: By moving computing resources to the network logical edge, mobile edge computing is promising for providing low-latency computing services to mobile users, and task offloading is one of its key enabling techniques. However, serious privacy concerns come along with offloading, due to the vulnerability of edge servers and the wireless transmission feature. Although many research efforts have been devoted to privacy-preserving offloading in recent years, there still lacks a comprehensive survey to systematically review these pioneering works. With this consideration, a thorough review for the state-of-the-art on this topic is provided in this work. Specifically, the privacy issues in offloading as well as the related metrics and application scenarios are discussed first. Then, based on their associated offloading phases, existing privacy-preserving offloading methods are classified into three categories: managing offloading data and pattern, secure transmission of offloaded data, and offloading destination selection. Finally, potential future directions of privacy-preserving offloading are discussed.
Keywords: Mobile-edge computing; Offloading; Privacy

Xiaohui Gu, Guoan Zhang,
Energy-efficient computation offloading for vehicular edge computing networks,
Computer Communications,
Volume 166,
2021,
Pages 244-253,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.12.010.
(https://www.sciencedirect.com/science/article/pii/S0140366420320168)
Abstract: The demanding computing capacity of emerging vehicular applications has emerged as a challenge in Internet of vehicles (IoVs). Multi-access edge computing (MEC) can significantly enhance computing capability and prolong battery life of vehicles through offloading computation-intensive tasks for edge computing. Considering the impact of vehicles’ mobility on communication quality, this paper provides an energy-efficient computation offloading scheme for vehicular edge computing networks (VECN). An energy-efficiency cost (EEC) minimization problem is formulated to make a tradeoff between latency and energy consumption, for completing computational tasks in an effective manner. Since that multiple variables and time-varying channel conditions make the formulated problem difficult to solve, we transform the original non-convex problem into a two-level optimization problem and develop an iterative distributed algorithm to obtain an optimal solution. Numerical results verify the convergence and superiority of the proposed algorithm.
Keywords: Vehicular networks; Multi-access edge computing; Computation offloading; Resource allocation; Mobility

Yihong Li, Congshi Jiang,
Distributed task offloading strategy to low load base stations in mobile edge computing environment,
Computer Communications,
Volume 164,
2020,
Pages 240-248,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.10.021.
(https://www.sciencedirect.com/science/article/pii/S0140366420319691)
Abstract: Due to the limited computing resources and battery capacity of existing mobile devices, it cannot meet the requirements of low load base station group for computing capacity and delay, and the emergence of mobile edge computing (MEC) technology provides the possibility for it. Therefore, a distributed task unloading strategy to low load base station group under MEC environment is proposed. Firstly, the communication resource, computing resource and task queue of low load base station group are modeled to quantify the energy cost in the process of task unloading. Then, the game theory is introduced, and the potential game model is used to solve the problem of distributed task unloading. The target function of energy optimization based on delay limitation is transformed into the potential game equation, and the mobile device selects MEC nodes according to the game results to calculate the unloading. Finally, based on the MATLAB platform, the algorithm is simulated, and the results show that the proposed potential game equation can converge to the Nash equilibrium. Compared with other algorithms, the proposed distributed task unloading algorithm can effectively save the energy consumption of task unloading.
Keywords: Mobile edge computing; Low load base station group; Distributed task offload strategy; Game theory; Communication resources; Computing resources

Wassim Boudieb, Abdelhamid Malki, Mimoun Malki, Ahmed Badawy, Mahmoud Barhamgi,
Microservice instances selection and load balancing in fog computing using deep reinforcement learning approach,
Future Generation Computer Systems,
Volume 156,
2024,
Pages 77-94,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.03.010.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000815)
Abstract: Fog-native computing is an emerging paradigm that makes it possible to build flexible and scalable Internet of Things (IoT) applications using microservice architecture at the network edge. With this paradigm, IoT applications are decomposed into multiple fine-grained microservices, strategically deployed on various fog nodes to support a wide range of IoT scenarios, such as smart cities and smart farming. Nonetheless, the performance of these IoT applications is affected by their limited effectiveness in processing offloaded IoT requests originating from multiple IoT devices. Specifically, the requested IoT services are composed of multiple dependent microservice instances collectively referred to as a service plan (SP). Each SP comprises a series of tasks designed to be executed in a predefined order, with the objective of meeting heterogeneous Quality of Service (QoS) requirements (e.g., low service delays). Different from the cloud, selecting the appropriate service plan for each IoT request can be a challenging task in dynamic fog environments due to the dependency and decentralization of microservice instances, along with the instability of network conditions and service requests (i.e., change quickly over time). To deal with this challenge, we study the microservice instances selection problem for IoT applications deployed on fog platforms and propose a learning-based approach that employs Deep Reinforcement Learning (DRL) to compute the optimal service plans. The latter optimizes the delay of application requests while effectively balancing the load among microservice instances. In our selection process, we carefully address the plan-dependency to efficiently select valid service plans for every request by introducing two distinct approaches; an action masking approach and an adaptive action mapping approach. Additionally, we propose an improved experience replay to address delayed action effects and enhance our model training efficiency. A series of experiments were conducted to assess the performance of our Microservice Instances Selection Policy (MISP) approach. The results demonstrate that our model reduces the average failure rate by up to 65% and improves load balance by up to 45% on average when compared to the baseline algorithms.
Keywords: Internet of Things; Fog computing; Microservice selection; Deep reinforcement learning; Deadline-aware; Load balancing

Rong Chai, Xizheng Yang, Chunling Du, Qianbin Chen,
Network cost optimization-based capacitated controller deployment for SDN,
Computer Networks,
Volume 197,
2021,
108326,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108326.
(https://www.sciencedirect.com/science/article/pii/S1389128621003273)
Abstract: As a novel network paradigm, software-defined networking (SDN) is capable of simplifying network management and offering flexible support to various user services. In order to meet the rapidly increasing transmission demands of SDN switches, the controller deployment strategy in an SDN scenario should be designed. In this paper, we investigate the capacitated controller deployment problem for SDN. Consider the signaling transmission and processing performance of switches and address the worst-case performance, we define network response time (NRT) as the maximum control plane response time of switches. Then aiming to achieve the tradeoff between NRT and the cost of controllers, we introduce the concept of network cost which is defined as the weighted sum of NRT and controller cost. The capacitated controller deployment problem is formulated as a constrained network cost minimization problem. To solve the optimization problem, we propose a two-stage heuristic algorithm, which first tackles the controller deployment subproblem under the unlimited capacity constraint, and then solves controller-type matching subproblem. Specifically, during the first stage, a minimum eccentricity-based controller deployment algorithm is designed to determine the number and location of controllers as well as the association strategy between controllers and switches. During the second stage, a greedy method-based controller-type matching strategy is proposed to determine the types of deployed controllers. Extensive simulations are performed and the results certify the effectiveness of the proposed algorithm.
Keywords: Software-defined networking; Controller deployment; Response time; Network cost

Chenglin Xu, Cheng Xu, Bo Li, Siqi Li, Tao Li,
Load-aware dynamic controller placement based on deep reinforcement learning in SDN-enabled mobile cloud-edge computing networks,
Computer Networks,
Volume 234,
2023,
109900,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109900.
(https://www.sciencedirect.com/science/article/pii/S1389128623003456)
Abstract: Software-defined networks (SDNs) can improve network resource utilization and optimize the performance of mobile cloud-edge computing networks (MECCNs) through unified and flexible network management. However, network traffic in MECCNs can change over time and space, which affects the performance of the control plane in an SDN. Further, an MECCN may require the temporary addition of network access points, further reducing the network management capabilities of the control plane. To ensure that the control plane can handle the constant changes in network traffic, adapt to dynamic changes in network access points, and provide continuous and efficient network management functions, this study focuses on the dynamic controller placement problem in SDN-enabled MECCNs. We study the deployment of a two-layer control plane and accordingly construct the corresponding delay, load balancing, and control reliability models. Next, we construct a joint optimization problem considering the developed delay, load balancing, and control reliability, and solve this problem using an algorithm based on the deep deterministic policy gradient algorithm. The experimental results demonstrate that the proposed algorithm outperforms other algorithms on a variable-node network.
Keywords: Software-defined network; Controller placement problem; Variable-node network; Deep deterministic policy gradient

Kashif Bilal, Emna Baccour, Aiman Erbad, Amr Mohamed, Mohsen Guizani,
Collaborative joint caching and transcoding in mobile edge networks,
Journal of Network and Computer Applications,
Volume 136,
2019,
Pages 86-99,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.02.004.
(https://www.sciencedirect.com/science/article/pii/S1084804519300487)
Abstract: Video streaming has become a leading consumer of network resources in the last decade. Despite considerable developments, video content providers still face major challenges, which include minimizing data transfer from Content Delivery Network (CDN) or origin servers, CDN cost, and video startup delays. Recent edge computing technologies, such as Mobile Edge Computing (MEC) introduces new opportunities for Radio Access Networks (RANs) by providing computing and storage resources at the Mobile Base Stations (MBSs). Caching and processing videos at the edge networks relieve excessive data transfers over the backhaul links and minimize the viewers perceived delay. Collaborative caching and processing strategies have been proposed to efficiently utilize the edge resources, where neighboring MEC servers share the cached videos. However, such strategies introduce new challenges due to excessive backhaul links utilization for video sharing and limited resources. We propose a collaborative joint caching and processing strategy using the X2 network interface for sharing video data among multiple caches. Our design aims to minimize: (a) backhaul links usage for sharing video data, (b) network usage in transferring data from the CDN, (c) the viewer perceived delay, and (d) CDN cost. We also propose to fetch the higher bitrate version video from the origin/CDN servers and transcode it to the requested version on the fly to effectively use the Adaptive Bit Rate (ABR) streaming and online transcoding. This joint caching and processing approach is formulated as a minimization problem, subject to storage, processing, and bandwidth constraints. We also propose an online greedy algorithm that controls video transcoding, sharing using the X2 or backhaul links, and manages video caching and removing at the edge caches. Simulation results prove a better performance of our proposed algorithm compared to the recent edge caching approaches in terms of cost, average delay, cache removal, and cache hit ratio for different configurations.
Keywords: Mobile Edge Computing; Collaborative caching and processing; Adaptive bitrate video transcoding; The X2 interface; Backhaul traffic

Xiaohui Gu, Guoan Zhang,
A survey on UAV-assisted wireless communications: Recent advances and future trends,
Computer Communications,
Volume 208,
2023,
Pages 44-78,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.05.013.
(https://www.sciencedirect.com/science/article/pii/S0140366423001743)
Abstract: Due to their characteristics of mobility, flexibility and adaptive altitude, unmanned aerial vehicles (UAVs), also known as drones, have immense potential applications in wireless systems. In particular, UAVs can operate as flying mobile terminals within a cellular network, and such cellular-connected UAVs have enabled several applications ranging from real-time video streaming to item delivery. Furthermore, the deployment of UAVs in wireless networks as flying base stations (BSs), servers or relays is expected to enhance network performance in terms of coverage, capacity, reliability and energy-efficiency. In this paper, a comprehensive tutorial on the potential applications and benefits of UAVs is presented. Specifically, UAVs as aerial BSs collecting data from ground sensors, transmitting power to energy-constrained devices and broadcasting information to ground users are explored along with representative design objectives. Then, UAVs as relay nodes for coverage extension, pathloss compensation and interference mitigation are surveyed comprehensively. Besides, UAVs as mobile servers performing edge computing for ground users are introduced, and various aerial offloading modes are reviewed thoroughly. Furthermore, radio and computation resource optimization, as well as mathematical tools and developed algorithms, are described in UAV-assisted communicating, relaying, and computing systems. Finally, open problems and potential research directions pertaining UAV-assisted communications are presented. In a nutshell, this tutorial provides key guidelines on how to design, analyze and optimize UAV-assisted wireless communication systems.
Keywords: UAV; Wireless networks; Aerial base station; Aerial relay; Mobile edge computing; Air–ground communications

Zhao Yang, Shengbing Zhang, Chuxi Li, Miao Wang, Jiaying Yang, Meng Zhang,
Joint heterogeneity-aware personalized federated search for energy efficient battery-powered edge computing,
Future Generation Computer Systems,
Volume 146,
2023,
Pages 178-194,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.024.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001644)
Abstract: The limited battery capacity of edge devices has a significant impact on the deployment of Federated Learning (FL). As a result, maintaining computation sustainability is a critical issue for edge FL. Furthermore, the heterogeneities of deployed edge devices reduce FL efficiency and effectiveness, making FL computation sustainability more challenging to maintain. To address these issues raised by heterogeneities, we perform a joint heterogeneity-aware personalized federated search for energy-efficient edge computing. To achieve energy-efficient on-device inference and training, a one-training process is adopted to search for personalized partial network structures on each device. We begin by tailoring the network scale on each node based on the efficiency of model inference, which also serves as the search space for optimization. This strategy can mitigate the straggler problem and improve the energy efficiency of FL by guiding the efficient FL training process in each round. To further optimize the energy consumption of edge devices, we design a lightweight search controller during the search process. This controller meets the low energy consumption requirements of the edge devices and reduces their energy consumption during the search process. Finally, we introduce an adaptive search strategy to guarantee personalized training convergence on each device. By reducing the energy consumption of each training round and ensuring the training convergence of personalized models, we can significantly improve the energy efficiency of FL on battery-powered edge devices. Our framework can obtain competitive accuracy with state-of-the-art methods while improving inference efficiency by up to 1.43× and training energy efficiency by up to 2.63×.
Keywords: Federated learning; Heterogeneity-aware; Personalization; Energy efficient; Federated search; Battery-powered edge device

Jiagang Liu, Yun Mi, Xinyu Zhang, Xiaocui Li,
Task graph offloading via deep reinforcement learning in mobile edge computing,
Future Generation Computer Systems,
2024,
,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.04.034.
(https://www.sciencedirect.com/science/article/pii/S0167739X24001638)
Abstract: Various mobile applications that comprise dependent tasks are gaining widespread popularity and are increasingly complex. These applications often have low-latency requirements, resulting in a significant surge in demand for computing resources. With the emergence of mobile edge computing (MEC), it becomes the most significant issue to offload the application tasks onto small-scale devices deployed at the edge of the mobile network for obtaining a high-quality user experience. However, since the environment of MEC is dynamic, most existing works focusing on task graph offloading, which rely heavily on expert knowledge or accurate analytical models, fail to fully adapt to such environmental changes, resulting in the reduction of user experience. This paper investigates the task graph offloading in MEC, considering the time-varying computation capabilities of edge computing devices. To adapt to environmental changes, we model the task graph scheduling for computation offloading as a Markov Decision Process (MDP). Then, we design a deep reinforcement learning algorithm (SATA-DRL) to learn the task scheduling strategy from the interaction with the environment, to improve user experience. Extensive simulations validate that SATA-DRL is superior to existing strategies in terms of reducing average makespan and deadline violation.
Keywords: Mobile edge computing; Task graph; Computation offloading; Reinforcement learning

Mateusz Starzec, Grażyna Starzec, Aleksander Byrski, Wojciech Turek, Kamil Piętak,
Desynchronization in distributed Ant Colony Optimization in HPC environment,
Future Generation Computer Systems,
Volume 109,
2020,
Pages 125-133,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.03.045.
(https://www.sciencedirect.com/science/article/pii/S0167739X19327402)
Abstract: Metaheuristics have significant computing requirements, in particular Ant Colony Optimization (ACO) processes a population of individuals (agents/ants) roaming in a graph, leaving the pheromone trails and getting inspired by its amount perceived on the edges. If the considered problem instance is large or the time is crucial, one can try to leverage parallel, hybrid or distributed infrastructure, but the algorithm itself must be properly prepared to deal with new possibilities. We have already presented a method for efficient implementation of distributed ACO, in this paper we follow up with introducing planned desynchronization in the pheromone matrix updates in order to further increase the scalability of the proposed system. The proposed modifications allowed the algorithm to scale up to 400 computations nodes without a significant impact on results quality. Efficacy of the algorithm outperforms the standard Max–Min Ant System by 10%.

Weibo Chu, Xinming Jia, Zhiwen Yu, John C.S. Lui, Yi Lin,
On incentivizing resource allocation and task offloading for cooperative edge computing,
Computer Networks,
Volume 246,
2024,
110428,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110428.
(https://www.sciencedirect.com/science/article/pii/S1389128624002603)
Abstract: Cooperative edge computing serves as an effective solution to provide reliable and elastic edge computing services through pooling geographically proximate edge resources and efficiently allocating them to users. The incentive mechanism is critical for realizing cooperative edge computing. In this paper, we propose a virtual machine (VM) or container based resource allocation scheme and two market-based mechanisms to stimulate collaboration among edge servers (ESs). Our first mechanism is a novel two-stage double auction based approach where edge resources are allocated to services/VMs at the first stage and then distributed among ESs at the second stage via a two-sided multi-unit auction. To make it more practical, we use markups to set the ask and bid prices for the participants in the market. Moreover, with the concept of virtual sellers and virtual buyers, we cast the multi-unit auction as single-unit one so that the auction becomes tractable. We further propose a price-based decentralized mechanism in case there is no central authority. With an appropriately constructed objective function, we formulate the resource allocation task as a network utility maximization (NUM) problem and leverage the dual-based decomposition theory for a distributed solution. We theoretically prove that both mechanisms guarantee properties such as budget balance and individual rationality for all ESs. Numerical studies with real-world dataset are presented to demonstrate the superior performance of our mechanisms over baselines.
Keywords: Cooperative edge computing; Resource allocation; Incentive mechanism; Double auction; Decentralized mechanism

Xiaonan Wang, Xingwei Wang, Yanli Li,
NDN-based IoT with Edge computing,
Future Generation Computer Systems,
Volume 115,
2021,
Pages 397-405,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.09.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X20303903)
Abstract: The Internet of Things (IoT) consists of smart devices that can capture and sense real-time information for monitoring. IoT is typically applied in some delay-sensitive fields, so it is significant to achieve efficient IoT-based data communications. IoT works based on the IP-based end-to-end data delivery mechanism, but this end-to-end mechanism is inefficient in the IoT scenarios. The Named Data Networking (NDN) is a new data communication paradigm and its advantages might help improve IoT-based data communication efficiency. However, IoT and NDN have different architectures and IoT devices have limited resources, so it is hard to directly deploy NDN in IoT. To exploit the advantages of NDN to improve IoT-based data communication efficiency, we are motivated to integrate IoT with Edge computing and clustering (IoTE) so that edge devices and cluster heads can help achieve request aggregation and in-network caching in NDN. Based on the idea, we propose an NDN-based IoTE (NIoTE) framework so that IoT devices can employ the advantages of NDN to retrieve data from the nearest provider via one data communication process. The experimental results verify the advantages of NIoTE, and demonstrate that NIoTE effectively decreases data communication latency and costs.
Keywords: Named data networking; Internet of things; Edge computing; Request aggregation; Forwarding interest base

Ali Nauman, Mashael Maashi, Hend K. Alkahtani, Fahd N. Al-Wesabi, Nojood O. Aljehane, Mohammed Assiri, Sara Saadeldeen Ibrahim, Wali Ullah Khan,
Efficient resource allocation and user association in NOMA-enabled vehicular-aided HetNets with high altitude platforms,
Computer Communications,
Volume 216,
2024,
Pages 374-386,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.01.021.
(https://www.sciencedirect.com/science/article/pii/S0140366424000215)
Abstract: The increasing demand for massive connectivity and high data rates has made the efficient use of existing spectrum resources an increasingly challenging problem. Non-orthogonal multiple access (NOMA) is a potential solution for future heterogeneous networks (HetNets) due to its high capacity and spectrum efficiency. In this study, we analyze an uplink NOMA-enabled vehicular-aided HetNet, where multiple vehicular user equipment (VUEs) share the access link spectrum, and a high-altitude platform (HAP) communicates with roadside units (RSUs) through a backhaul communication link. We propose an improved algorithm for user association that selects VUEs for HAPs based on channel coefficient ratios and terrestrial VUEs based on a caching-state backhaul communication link. The joint optimization problems aim to maximize a utility function that considers VUE transmission rates and cross-tier interference while meeting the constraints of backhaul transmission rates and QoS requirements of each VUE. The joint resource allocation optimization problem consists of three sub-problems: bandwidth allocation, user association, and transmission power allocation. We derive a closed-form solution for bandwidth allocation and solve the transmission power allocation sub-problem iteratively using Taylor expansion to transform a non-convex term into a convex one. Our proposed three-stage iterative algorithm for resource allocation integrates all three sub-problems and is shown to be effective through simulation results. Specifically, the results demonstrate that our solution achieves performance improvements over existing approaches.
Keywords: Non-orthogonal multiple access (NOMA); Heterogeneous networks (hetNets); Vehicular user equipment (VUE); High altitude platform (HAP); Roadside units (RSUs)

Abdelhak Hidouri, Haifa Touati, Mohamed Hadded, Nasreddine Hajlaoui, Paul Muhlethaler, Samia Bouzefrane,
Q-ICAN: A Q-learning based cache pollution attack mitigation approach for named data networking,
Computer Networks,
Volume 235,
2023,
109998,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109998.
(https://www.sciencedirect.com/science/article/pii/S1389128623004437)
Abstract: The Cache Pollution Attack (CPA) is a recent threat that poses a significant risk to Named Data Networks (NDN). This attack can impact the caching process in various ways, such as causing increased cache misses for legitimate users, delays in data retrieval, and exhaustion of resources in NDN routers. Despite the numerous countermeasures suggested in the literature for CPA, many of them have detrimental effects on the NDN components. In this paper, we introduce Q-ICAN, a novel intelligent technique for detecting and mitigating cache pollution attacks in NDN. More specifically, Q-ICAN uses Q-Learning as an automated CPA prediction mechanism. Each NDN router integrates a reinforcement learning agent that utilizes impactful metrics such as the variation of the Cache Hit Ratio (CHR) and the interest inter-arrival time to learn how to differentiate between malicious and legitimate interests. We conducted several simulations using NDNSim to assess the effectiveness of our solution in terms of Cache Hit Ratio (CHR), Average Retrieval Delay (ARD) and multiple artificial intelligence evaluation metrics such as accuracy, precision, recall, etc. The obtained results confirm that Q-ICAN detects CPA attacks with a 95.09% accuracy rate, achieves a 94% CHR, and reduces ARD by 18%. Additionally, Q-ICAN adheres to the security policy of the NDN architecture and consumes fewer resources from NDN routers compared to existing state-of-the-art solutions.
Keywords: Named data networking; Cache pollution attack; Q-learning

Saira Bano, Nicola Tonellotto, Pietro Cassarà, Alberto Gotta,
Artificial intelligence of things at the edge: Scalable and efficient distributed learning for massive scenarios,
Computer Communications,
Volume 205,
2023,
Pages 45-57,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.04.010.
(https://www.sciencedirect.com/science/article/pii/S0140366423001238)
Abstract: Federated Learning (FL) is a distributed optimization method in which multiple client nodes collaborate to train a machine learning model without sharing data with a central server. However, communication between numerous clients and the central aggregation server to share model parameters can cause several problems, including latency and network congestion. To address these issues, we propose a scalable communication infrastructure based on Information-Centric Networking built and tested on Apache Kafka®. The proposed architecture consists of a two-tier communication model. In the first layer, client updates are cached at the edge between clients and the server, while in the second layer, the server computes global model updates by aggregating the cached models. The data stored in the intermediate nodes at the edge enables reliable and effective data transmission and solves the problem of intermittent connectivity of mobile nodes. While many local model updates provided by clients can result in a more accurate global model in FL, they can also result in massive data traffic that negatively impacts congestion at the edge. For this reason, we couple a client selection procedure based on a congestion control mechanism at the edge for the given architecture of FL. The proposed algorithm selects a subset of clients based on their resources through a time-based backoff system to account for the time-averaged accuracy of FL while limiting the traffic load. Experiments show that our proposed architecture has an improvement of over 40% over the network-centric based FL architecture, i.e., Flower. The architecture also provides scalability and reliability in the case of mobile nodes. It also improves client resource utilization, avoids overflow, and ensures fairness in client selection. The experiments show that the proposed algorithm leads to the desired client selection patterns and is adaptable to changing network environments.
Keywords: 6G; Federated Learning; In-network caching; Publish/subscribe; Clients selection; Edge computing; Information-centric networking

Gaurav Baranwal, Dinesh Kumar, Deo Prakash Vidyarthi,
Blockchain based resource allocation in cloud and distributed edge computing: A survey,
Computer Communications,
Volume 209,
2023,
Pages 469-498,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.07.023.
(https://www.sciencedirect.com/science/article/pii/S0140366423002566)
Abstract: Cloud computing and distributed edge computing provide computing resources to meet the surging demands for computing caused by developments in technologies such as the Internet of Things (IoT) and Mobile communication (5G). Centralized resource allocation approaches in both computing paradigms suffer from single-point failure, tampering, modification in allocation results, and biased actions. Recently, blockchain has become popular in designing decentralized systems because of its features, including transparency, decentralization, and anti-tamper. In this paper, we provide a comprehensive survey of the research works applying blockchain in resource allocation in both computing paradigms, addressing the issues in centralized resource allocation approaches. Firstly, we identify several key research questions acting as motivation. To provide background knowledge, first, we discuss the centralized resource allocation in both computing paradigms and associated challenges. Then we discuss blockchain, its structure, working, characteristics and types, followed by its benefits to resource allocation. We identify several metrics to provide a comparative study of the works. We present a depth overview of blockchain-based resource allocation works in three domains: cloud computing, distributed edge computing and integrated edge and cloud computing. In each domain, works are summarized from three aspects: works using blockchain platforms, works providing blockchain frameworks and works advocating blockchain. We discuss consensus mechanisms in the works related to blockchain-based resource allocation, as the consensus mechanism is a fundamental part of the blockchain. Further, we provide key challenges requiring our attention. Finally, we conclude the survey.
Keywords: Cloud computing; Edge computing; Blockchain; Decentralization; Resource allocation; Computation offloading

Mu Wang, Changqiao Xu, Xingyan Chen, Lujie Zhong, Gabriel-Miro Muntean,
Decentralized asynchronous optimization for dynamic adaptive multimedia streaming over information centric networking,
Journal of Network and Computer Applications,
Volume 157,
2020,
102574,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102574.
(https://www.sciencedirect.com/science/article/pii/S1084804520300485)
Abstract: By the envision of combing smooth viewing experience with high-efficiency content distribution, dynamic adaptive streaming (DAS) over information-centric networking (ICN) is becoming a promising trend for the future video services. However, optimizations of DAS flow transmission control and rate adaptation need to be revisited for better adopting the ICN with multicast, multi-rate forwarding and decentralized framework. In this paper, we propose a decentralized asynchronous method for ICN-DAS. We first formulate the problem as a two-stage optimization, wherein the first stage's objective is to optimize the transmission rate within network capacity constraints, and the second is adapting the video bitrate for the long-term viewing utility. A distributed asynchronous optimization algorithm (DAOA) is then proposed for solving the two-stage problem iteratively by a novel distributed switching mirror descent and virtual queue-based iterations. Analytic results including convergence, computation complexity and time-varying adaptation are provided to validate theoretically the DAOA's performance. Simulation-based testing has also been conducted for evaluating DAOA's performance and assess its viewing experience, in comparison with state-of-the-art solutions.
Keywords: Information centric networking (ICN); Dynamic adaptive video streaming(DAS); Distributed concave optimization; Stochastic optimization

Diego Hortelano, Ignacio de Miguel, Ramón J. Durán Barroso, Juan Carlos Aguado, Noemí Merayo, Lidia Ruiz, Adrian Asensio, Xavi Masip-Bruin, Patricia Fernández, Rubén M. Lorenzo, Evaristo J. Abril,
A comprehensive survey on reinforcement-learning-based computation offloading techniques in Edge Computing Systems,
Journal of Network and Computer Applications,
Volume 216,
2023,
103669,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103669.
(https://www.sciencedirect.com/science/article/pii/S1084804523000887)
Abstract: In recent years, the number of embedded computing devices connected to the Internet has exponentially increased. At the same time, new applications are becoming more complex and computationally demanding, which can be a problem for devices, especially when they are battery powered. In this context, the concepts of computation offloading and edge computing, which allow applications to be fully or partially offloaded and executed on servers close to the devices in the network, have arisen and received increasing attention. Then, the design of algorithms to make the decision of which applications or tasks should be offloaded, and where to execute them, is crucial. One of the options that has been gaining momentum lately is the use of Reinforcement Learning (RL) and, in particular, Deep Reinforcement Learning (DRL), which enables learning optimal or near-optimal offloading policies adapted to each particular scenario. Although the use of RL techniques to solve the computation offloading problem in edge systems has been covered by some surveys, it has been done in a limited way. For example, some surveys have analysed the use of RL to solve various networking problems, with computation offloading being one of them, but not the primary focus. Other surveys, on the other hand, have reviewed techniques to solve the computation offloading problem, being RL just one of the approaches considered. To the best of our knowledge, this is the first survey that specifically focuses on the use of RL and DRL techniques for computation offloading in edge computing system. We present a comprehensive and detailed survey, where we analyse and classify the research papers in terms of use cases, network and edge computing architectures, objectives, RL algorithms, decision-making approaches, and time-varying characteristics considered in the analysed scenarios. In particular, we include a series of tables to help researchers identify relevant papers based on specific features, and analyse which scenarios and techniques are most frequently considered in the literature. Finally, this survey identifies a number of research challenges, future directions and areas for further study.
Keywords: Computation offloading; Edge computing; MEC; Multi-Access Edge Computing; Reinforcement Learning; Deep Reinforcement Learning

Sparsh Mittal,
A Survey on optimized implementation of deep learning models on the NVIDIA Jetson platform,
Journal of Systems Architecture,
Volume 97,
2019,
Pages 428-442,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.01.011.
(https://www.sciencedirect.com/science/article/pii/S1383762118306404)
Abstract: Design of hardware accelerators for neural network (NN) applications involves walking a tight rope amidst the constraints of low-power, high accuracy and throughput. NVIDIA’s Jetson is a promising platform for embedded machine learning which seeks to achieve a balance between the above objectives. In this paper, we provide a survey of works that evaluate and optimize neural network applications on Jetson platform. We review both hardware and algorithmic optimizations performed for running NN algorithms on Jetson and show the real-life applications where these algorithms have been applied. We also review the works that compare Jetson with similar platforms. While the survey focuses on Jetson as an exemplar embedded system, many of the ideas and optimizations will apply just as well to existing and future embedded systems. It is widely believed that the ability to run AI algorithms on low-cost, low-power platforms will be crucial for achieving the “AI for all” vision. This survey seeks to provide a glimpse of the recent progress towards that goal.
Keywords: Review; Embedded system; NVIDIA Jetson; Neural network; Deep learning; Autonomous driving; Drone; Low-power computing

Lunyuan Chen, Shunpu Tang, Venki Balasubramanian, Junjuan Xia, Fasheng Zhou, Lisheng Fan,
Physical-layer security based mobile edge computing for emerging cyber physical systems,
Computer Communications,
Volume 194,
2022,
Pages 180-188,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.07.037.
(https://www.sciencedirect.com/science/article/pii/S0140366422002857)
Abstract: This paper studies a secure mobile edge computing (MEC) for emerging cyber physical systems (CPS), where there exist K eavesdroppers in the network, which can threaten the task offloading. These K eavesdroppers can work either in a colluding mode where they cooperate to decode the secret message, or in a non-colluding mode where the eavesdroppers decode the message individually. For both eavesdropping nodes, we design the secure MEC system by devising a computation offloading ratio, transmit power and computational capability allocation to optimize the system performance mainly measured by the latency. In particular, a novel deep reinforcement learning (DRL) together with convex optimization (DRCO) is proposed, where the DRL is used to find a proper solution to the offloading ratio, while the convex optimization is implemented to solve the allocation of transmission power and computational capability. Simulation results show that the proposed DRCO method is superior to other conventional methods, and can provide a guaranteed secrecy and latency.
Keywords: Cyber physical systems; Mobile edge computing; Secure communication; Eavesdropping

Bin Liang, Mark A. Gregory, Shuo Li,
Multi-access Edge Computing fundamentals, services, enablers and challenges: A complete survey,
Journal of Network and Computer Applications,
Volume 199,
2022,
103308,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103308.
(https://www.sciencedirect.com/science/article/pii/S1084804521002976)
Abstract: Traffic over mobile cellular networks has significantly increased over the past decade, and with the introduction of 5G there is a growing focus on throughput capacity, reliability, and low latency to meet the demands of new and innovative applications. Multi-access Edge Computing (MEC) is being developed to achieve a series of challenges posed by the introduction of new applications and services that require ultra-low latency and high bandwidth. This article is a comprehensive survey of recent advances in MEC and provides a description of the MEC concept, framework, and capabilities. We also summarize a set of MEC technology enablers including Software Defined Networking, Network Function Virtualization, Information-Centric Networking, Service Function Chaining, Cloud-Radio Access Networks, Fog-computing based Radio Access Networks and Network Slicing. The MEC use cases and the open research challenges are presented.
Keywords: Multi-access Edge Computing; Software Defined Networking; Network Function Virtualization; Information-Centric Networking; Service Function Chaining; Network Slicing; Cloud-Radio Access Network; Fog-computing based Radio Access Network

Polychronis Valsamas, Sotiris Skaperas, Lefteris Mamatas, Luis M. Contreras,
Virtualization Technology Blending for resource-efficient edge clouds,
Computer Networks,
Volume 225,
2023,
109646,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109646.
(https://www.sciencedirect.com/science/article/pii/S1389128623000919)
Abstract: Edge computing brings virtualized services closer to users to improve their operation (e.g., in terms of communication latency, reliability, or data privacy) and is considered as a main technological enabler for 5G and beyond ecosystems. Edge cloud orchestration approaches are typically adapting server and network resources to rapid changes in the workload, mitigate resource exhaustion, or address service performance requirements. In this context, we introduce and investigate a new cloud orchestration strategy, called Virtualization Technology Blending (VTB). VTB considers edge clouds supporting alternative virtualization options, including multiple unikernel flavors and container builds of the same or a similar service, which can be effectively blended to handle cases with challenging resource requirements, since they exhibit diverse resource requirements and performance capabilities. Here, we provide an elaboration of VTB, backed by a relevant optimization framework, in order to validate its feasibility and assess its benefits. According to our results, the proposed strategy augments the number of users that can be served up to 74% and improves the server and network resource utilization up to 50.9% and 34.4%, respectively.
Keywords: Edge cloud orchestration; Cloud resource optimization; Virtualization Technology Blending; Unikernels; Containers

Shahnawaz Ahmad, Iman Shakeel, Shabana Mehfuz, Javed Ahmad,
Deep learning models for cloud, edge, fog, and IoT computing paradigms: Survey, recent advances, and future directions,
Computer Science Review,
Volume 49,
2023,
100568,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100568.
(https://www.sciencedirect.com/science/article/pii/S1574013723000357)
Abstract: In recent times, the machine learning (ML) community has recognized the deep learning (DL) computing model as the Gold Standard. DL has gradually become the most widely used computational approach in the field of machine learning, achieving remarkable results in various complex cognitive tasks that are comparable to, or even surpassing human performance. One of the key benefits of DL is its ability to learn from vast amounts of data. In recent years, the DL field has witnessed rapid expansion and has found successful applications in various conventional areas. Significantly, DL has outperformed established ML techniques in multiple domains, such as cloud computing, robotics, cybersecurity, and several others. Nowadays, cloud computing has become crucial owing to the constant growth of the IoT network. It remains the finest approach for putting sophisticated computational applications into use, stressing the huge data processing. Nevertheless, the cloud falls short because of the crucial limitations of cutting-edge IoT applications that produce enormous amounts of data and necessitate a quick reaction time with increased privacy. The latest trend is to adopt a decentralized distributed architecture and transfer processing and storage resources to the network edge. This eliminates the bottleneck of cloud computing as it places data processing and analytics closer to the consumer. Machine learning (ML) is being increasingly utilized at the network edge to strengthen computer programs, specifically by reducing latency and energy consumption while enhancing resource management and security. To achieve optimal outcomes in terms of efficiency, space, reliability, and safety with minimal power usage, intensive research is needed to develop and apply machine learning algorithms. This comprehensive examination of prevalent computing paradigms underscores recent advancements resulting from the integration of machine learning and emerging computing models, while also addressing the underlying open research issues along with potential future directions. Because it is thought to open up new opportunities for both interdisciplinary research and commercial applications, we present a thorough assessment of the most recent works involving the convergence of deep learning with various computing paradigms, including cloud, fog, edge, and IoT, in this contribution. We also draw attention to the main issues and possible future lines of research. We hope this survey will spur additional study and contributions in this exciting area.
Keywords: Deep learning; Cloud computing; Edge computing; Fog computing; IoT; Models

Zeyu Luan, Qing Li, Yong Jiang, Jingpu Duan, Ruobin Zheng, Dingding Chen, Shaoteng Liu,
MATE: When multi-agent Deep Reinforcement Learning meets Traffic Engineering in multi-domain networks,
Computer Networks,
Volume 247,
2024,
110399,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110399.
(https://www.sciencedirect.com/science/article/pii/S1389128624002317)
Abstract: As the global network evolves into a large-scale interconnection system with multiple distributed domains, scalability and efficiency have become equally important for Traffic Engineering (TE). Traditional centralized TE suffers from significant computational complexity and privacy concerns, while distributed TE results in suboptimal routing decisions due to its lack of a coordination mechanism. This paper proposes MATE, an innovative, scalable, and efficient TE framework for multi-domain networks featuring a hierarchical control plane. MATE designs a three-stage workflow to optimize routing decisions for inter-domain traffic while ensuring their QoS guarantees. First, in the bottom-up topology abstraction stage, MATE aggregates network resources within each domain, creating an abstract view of the global network state while protecting each domain’s internal information. Second, in the top-down decision-making stage, MATE computes a QoS-constrained domain sequence based on the global network state and then converts it into multi-path routing in all related domains. Third, MATE conducts parallel inferences across relevant domains on traffic split ratios for multi-path routing using well-designed multi-agent reinforcement learning. We evaluate MATE on both real-world and synthetic topologies under various traffic patterns. In a large-scale topology encompassing 16 domains, MATE achieves near-optimal link utilization across 97% network scenarios, with an approximation ratio of below 1.3. The experimental results demonstrate MATE’s superiority in fulfilling QoS requirements, minimizing maximum link utilization, and maintaining robustness against traffic pattern variations and random link failures.
Keywords: Traffic Engineering; Software-Defined Networking; Deep Reinforcement Learning

Ning Ai, Bin Wu, Boyu Li, Zhipeng Zhao,
5G heterogeneous network selection and resource allocation optimization based on cuckoo search algorithm,
Computer Communications,
Volume 168,
2021,
Pages 170-177,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.12.026.
(https://www.sciencedirect.com/science/article/pii/S0140366421000244)
Abstract: In order to solve the problem of spectrum resource shortage and high-speed access in 5G network, the multi-agent system is embedded into the standard cuckoo algorithm, and the multi-agent cuckoo algorithm is proposed. Firstly, the spectrum information of network channel sharing available to users is obtained, and the cuckoo search algorithm is used to optimize under the condition of satisfying the quality of service (QoS) guarantee of users, and the optimal allocation scheme is obtained by iterating for many times. The use steps are illustrated by examples. Compared with the traditional genetic algorithm, the calculation complexity can be reduced, and it can also be extended to more users and networks. In this algorithm, each cuckoo represents an agent, and all the agents constitute a von-Neumann structure. Through the neighborhood competition cooperation operator, mutation operator, self-learning operator and the evolution mechanism of cuckoo algorithm, they can continuously enhance the energy and improve the adaptability, and can quickly and accurately find the optimal solution of the problem.
Keywords: Cuckoo search algorithm; 5G heterogeneous network; Resource allocation; Optimization

Felipe Rabuske Costa, Rodrigo da Rosa Righi, Cristiano André da Costa, Cristiano Bonato Both,
Nuoxus: A proactive caching model to manage multimedia content distribution on fog radio access networks,
Future Generation Computer Systems,
Volume 93,
2019,
Pages 143-155,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.10.031.
(https://www.sciencedirect.com/science/article/pii/S0167739X18302449)
Abstract: One of the most recently proposed 5G architectures, Fog Radio Access Network or F-RAN, has the same cloud computing-based approach of C-RANs while giving additional storage and processing power to the edge nodes so that they can address network activities. The main problem of F-RANs is the intense data traffic in the fronthaul, which is the centralized physical channel used to connect the antennas (F-APs) to the BBU pool. To minimize this bottleneck, we propose the Nuoxus model, whose objective is to cache multimedia content in the edge of the network. By taking advantage of the hierarchical structure of F-RANs, Nuoxus content replacement policy analyses the similarity of requests amongst the children nodes to define the relevance of storing a requested content in cache. It also uses this process to proactively suggest to nodes located lower in the network hierarchy the caching of relevant content. The state-of-the-art analysis reveals Nuoxus as the first model to explore the nodes' request history to perform proactive caching without relying on external and centralized components. Furthermore, the developed proof-of-concept shows significant gains when compared to other caching strategies.
Keywords: FOG radio access networks; Edge radio access networks; F- RAN; Caching; Nuoxus; Cosine similarity

Si Shen, Guojiang Shen, Xiaoxue Yang, Feng Xia, Hao Du, Xiangjie Kong,
Mean-field reinforcement learning for decentralized task offloading in vehicular edge computing,
Journal of Systems Architecture,
Volume 146,
2024,
103048,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.103048.
(https://www.sciencedirect.com/science/article/pii/S1383762123002278)
Abstract: Vehicular Edge Computing (VEC) is a promising paradigm for providing low-latency and high-reliability services in the Internet of Vehicles (IoV). The increasing number of mobile devices and the diverse resource requirements of the growing IoV have resulted in a shift from centralized resource management to a decentralized approach. This shift offers improved fault tolerance, scalability, and privacy preservation. However, constructing collaborative awareness and coordination mechanisms between multiple vehicles and edge nodes in a decentralized manner is a challenge. To address this issue, we propose a decentralized many-to-many task offloading method that aims to minimize the average task completion latency of vehicles. In this study, we propose a data-sharing mechanism between vehicles and edge servers using the digital twin service, which provides global environmental perceptions to the vehicles by a low-cost approach. Additionally, we develop a mean-field multi-agent reinforcement learning algorithm to generate coordinated task offloading schemes. Instead of interacting with multiple agents, the vehicle only needs to respond to the mean action of the environment. Based on this transition, the agent generates coordinated task offloading decisions in complex scenarios. We evaluate the performance of our method using real urban traffic data. Experiment results verify the efficiency of our proposed method.
Keywords: Vehicular edge computing; Mean-field reinforcement learning; Digital twin; Task offloading

Tuo Cao, Qinhui Wang, Yuhan Zhang, Zhuzhong Qian, Yue Zeng, Mingtao Ji, Hesheng Sun, Baoliu Ye,
Walking on two legs: Joint service placement and computation configuration for provisioning containerized services at edges,
Computer Networks,
Volume 239,
2024,
110144,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110144.
(https://www.sciencedirect.com/science/article/pii/S1389128623005893)
Abstract: With the development of edge computing and container virtualization, provisioning containerized services at network edges is proposed for high responsiveness and low wide area network (WAN) traffic. However, realizing its full potential faces multiple challenges. First, due to containers’ fine-grained computation resource isolation, finely configuring services’ computation resource requirements is needed, especially for resource-constrained edge nodes. Second, due to edges’ heterogeneity and services’ diversity, system performance highly depends on which edge node to place each service. Third, as the main metric of quality of service, service response time involves the computing-network delay tradeoff and urges to optimize the decisions jointly. Prior works on edge-enabled service placement either ignore computation resource isolation and configuration, or assume computation resource configuration is given manually. To fill this gap, this paper investigates the joint service placement and computation configuration problem for provisioning containerized services at edges. Then based on the convex and submodular optimization techniques, we propose a two-stage greedy and local-search combined algorithm, TeLa for short. Rigorous theoretical analyses demonstrate that TeLa is a polynomial-time algorithm with performance guarantees. Finally, we implement twelve containerized services and an edge computing prototype to realistically evaluate TeLa. The results confirm TeLa’s empirical superiority over state-of-the-art algorithms, in terms of 39% on average reduction on the weighted sum of service response time and WAN traffic.
Keywords: Containerized service provisioning; Edge computing; Service placement; Computation configuration

Jinlei Zhu, Houjin Chen, Pan Pan,
A novel rate control algorithm for low latency video coding base on mobile edge cloud computing,
Computer Communications,
Volume 187,
2022,
Pages 134-143,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.02.009.
(https://www.sciencedirect.com/science/article/pii/S0140366422000469)
Abstract: The current low-latency video coding rate control algorithm has similar video sequence structure, low rate control accuracy, long control time, etc., which cannot meet the expectations of modern workers. Based on this, this paper proposes a mobile edge cloud computing-based Low-latency video coding rate control algorithm, optimize the original control algorithm, improve its rate control accuracy, etc. By analyzing the relationship between mobile edge computing and cloud computing, combined with edge computing in the context of cloud computing, this paper proposes a mobile edge cloud computing method and designs the mobile edge cloud computing architecture. Low latency video sequences are obtained using mobile edge cloud computing, and a mobile edge cloud computing model is established to control the update rate parameters of delayed video files. According to the principle of rate control, based on joint rate–distortion theory and frame coding complexity calculation model, the low latency video coding rate control is realized. The experimental results show that the byte data volume of the low-latency video coding sequence of the proposed method is 5653 dB. The rate control accuracy can be as high as 99.64%, and the rate control time is 1.37 s. The proposed method has a high similarity of video sequence structure and high rate control accuracy and can effectively shorten the rate control time of low latency video coding.
Keywords: Mobile edge computing; Cloud computing; Video coding; Rate control

András Attila Sulyok, Gábor Dániel Balogh, István Z Reguly, Gihan R Mudalige,
Locality optimized unstructured mesh algorithms on GPUs,
Journal of Parallel and Distributed Computing,
Volume 134,
2019,
Pages 50-64,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.07.011.
(https://www.sciencedirect.com/science/article/pii/S0743731519301698)
Abstract: Unstructured-mesh based numerical algorithms such as finite volume and finite element algorithms form an important class of applications for many scientific and engineering domains. The key difficulty in achieving higher performance from these applications is the indirect accesses that lead to data-races when parallelized. Current methods for handling such data-races lead to reduced parallelism and suboptimal performance. Particularly on modern many-core architectures, such as GPUs, that has increasing core/thread counts, reducing data movement and exploiting memory locality is vital for gaining good performance. In this work we present novel locality-exploiting optimizations for the efficient execution of unstructured-mesh algorithms on GPUs. Building on a two-layered coloring strategy for handling data races, we introduce novel reordering and partitioning techniques to further improve efficient execution. The new optimizations are then applied to several well established unstructured-mesh applications, investigating their performance on NVIDIA’s latest P100 and V100 GPUs. We demonstrate significant speedups (1.1–1.75×) compared to the state-of-the-art. A range of performance metrics are benchmarked including runtime, memory transactions, achieved bandwidth performance, GPU occupancy and data reuse factors and are used to understand and explain the key factors impacting performance. The optimized algorithms are implemented as an open-source software library and we illustrate its use for improving performance of existing or new unstructured-mesh applications.
Keywords: Finite volume; Finite element; Race condition; GPU

Rui Xu, Xiaoqiang Di, Jing Chen, Haowei Wang, Hao Luo, Hui Qi, Xiongwen He, Wenping Lei, Shiwei Zhang,
A hybrid caching strategy for information-centric satellite networks based on node classification and popular content awareness,
Computer Communications,
Volume 197,
2023,
Pages 186-198,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.10.025.
(https://www.sciencedirect.com/science/article/pii/S0140366422004133)
Abstract: Low Earth Orbit (LEO) satellite networks are inexpensive to deploy and have wide coverage, and are widely available for differentiated content distribution services. With the development of on-board storage and computing capabilities, in-network caching technology in information centric networking (ICN) has proven to be an effective way to increase the throughput and content distribution efficiency of satellite networks. However, traditional caching and distribution schemes, which do not take into account the high-speed motion of satellite nodes and dynamic changes in topology, are not applicable to satellite networks. To address these issues, a hybrid caching strategy for satellite networks based on node classification and popular content awareness (NCPCA) is proposed. Firstly, the time slot partitioning method based on interlayer similarity is proposed to transform the dynamically changing process into a set of time slots with stable topology. Next, the satellite nodes are dynamically divided into two categories by fully considering the changes in connection relationships and interaction order during the spatial–temporal evolution of the satellite nodes. Nodes with satellite topological and functional characteristics are screened out as core nodes with TOPSIS algorithm, and the remaining nodes are regarded as edge nodes. Finally, a probabilistic caching scheme based on content popularity is adopted with core nodes as caching nodes to ensure caching performance and promote the diversity of cached content. Simulation results show that this strategy can effectively improve cache hit rate, reduce user request delay and content fetching hops, and promote the stable operation of satellite networks compared with other caching strategies.
Keywords: LEO satellite networks; Node classification; TOPSIS; Popular content awareness; Probabilistic caching strategy

Xiaolong Xu, Qingxiang Liu, Yun Luo, Kai Peng, Xuyun Zhang, Shunmei Meng, Lianyong Qi,
A computation offloading method over big data for IoT-enabled cloud-edge computing,
Future Generation Computer Systems,
Volume 95,
2019,
Pages 522-533,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.12.055.
(https://www.sciencedirect.com/science/article/pii/S0167739X18319770)
Abstract: The Internet of mobile things is a burgeoning technique that generates, stores and processes big real-time data to render rich services for mobile users. In order to mitigate conflicts between the resource limitation of mobile devices and users’ demands of decreasing processing latency as well as prolonging battery life, it spurs a popular wave of offloading mobile applications for execution to centralized and decentralized data centers, such as cloud and edge servers. Due to the complexity and difference of mobile big data, arbitrarily offloading the mobile applications poses a remarkable challenge to optimizing the execution time and the energy consumption for mobile devices, despite the improved performance of Internet of Things (IoT) in cloud-edge computing. To address this challenge, we propose a computation offloading method, named COM, for IoT-enabled cloud-edge computing. Specifically, a system model is investigated, including the execution time and energy consumption for mobile devices. Then dynamic schedules of data/control-constrained computing tasks are confirmed. In addition, NSGA-III (non-dominated sorting genetic algorithm III) is employed to address the multi-objective optimization problem of task offloading in cloud-edge computing. Finally, systematic experiments and comprehensive simulations are conducted to corroborate the efficiency of our proposed method.
Keywords: IoT; Big data; Cloud-edge computing; Computation offloading; Energy consumption

Jie Lin, Lin Huang, Hanlin Zhang, Xinyu Yang, Peng Zhao,
A Novel Lyapunov based Dynamic Resource Allocation for UAVs-assisted Edge Computing,
Computer Networks,
Volume 205,
2022,
108710,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108710.
(https://www.sciencedirect.com/science/article/pii/S1389128621005685)
Abstract: Mobile edge computing (MEC), as a key component in the development of IoT and 5G technologies, can provide extra computation resources in edge servers for mobile devices to complete their computation tasks with low latency and high reliability. Considerable efforts on computation offloading and resource allocation have been developed to reduce the energy consumption and computation latency in edge computing. Nonetheless, the system utility of heterogeneous edge computing system (e.g., UAVs-assisted edge computing), in which multiple unmanned aerial vehicles (UAVs) are involved in an edge computing system to serve as edge servers still needs to be further investigated. To this end, in this paper, we propose a novel Lyapunov based Dynamic Resource Allocation (LDRA) for UAVs-assisted Mobile Edge Computing, which can effectively choose suitable edge servers for mobile devices to offload and complete their computation tasks with low system cost and great system utility of UAVs-assisted edge computing system, as well as acceptable computation latency and great reliability for computation tasks of mobile devices. Particularly, a random queue model for edge servers is conducted in our LDRA scheme to support the dynamic of offloaded computation tasks of mobile devices. Additionally, a system cost model of UAVs-assisted edge computing is developed considering the combination of multiple constraints, such as both the mobility of UAVs and mobile devices, energy consumption, communication cost, etc. With the objective of minimizing the system cost and maximizing the system utility in providing edge resources to complete the offloaded computation tasks of mobile devices, by introducing Lyapunov optimization, a dynamic resource allocation scheme is proposed to effectively determine edge servers to offload tasks of mobile devices with considering both the real-time execution state of offloaded tasks in edge servers and states of the communication link. Through analysis and performance evaluations, our results show that our proposed LDRA scheme can achieve a great balance between system cost and system stability. Additionally, our results also demonstrate that our LDRA scheme also can achieve better system utility in comparison with existing schemes.
Keywords: UAVs-assisted edge computing; Dynamic resource allocation; Lyapunov optimization; System cost and utility

Elif Bozkaya,
Digital twin-assisted and mobility-aware service migration in Mobile Edge Computing,
Computer Networks,
Volume 231,
2023,
109798,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109798.
(https://www.sciencedirect.com/science/article/pii/S1389128623002438)
Abstract: Mobile Edge Computing (MEC) is emerging as one of the key technologies to process massive amount of data at the edge of the network for upcoming 6G networks. In the paradigm of MEC, users can offload the computation-intensive and delay-sensitive tasks to the edge servers, since the capabilities of User Equipment (UE) cannot be not sufficient to meet different quality of service (QoS) requirements. However, due to the limited coverage of edge servers, users can move between multiple edge servers. In such cases, service migration is complicated which arises the challenge of where and when to migrate the service dynamically to maintain acceptable QoS. In addition, high uncertainty of the user mobility and different QoS requirements of the services are challenging. Instead of assigning each user to the nearest edge server, we propose a Digital Twin (DT)-assisted service migration approach to minimize the task completion time. The contributions in this paper are threefold: First, a DT edge network architecture is presented that uses the real-time and historical data to determine service migration strategy. Second, the DT classifies the computing tasks as delay-sensitive task and delay-insensitive tasks to meet the QoS requirements. Third, the DT predicts the mobility of users in the future based on the Hidden Markov Model (HMM) and studies the impact of reducing the cost of service migration. Thus, we propose an optimization algorithm to find optimal resource allocation at each edge server. The simulation results shows that our DT-assisted service migration model can achieve less delay and service migration rate compared with the traditional methods.
Keywords: Mobile Edge Computing; Digital Twin; Service migration; Digital Twin edge network; User mobility; Resource allocation

Lucas Bréhon–Grataloup, Rahim Kacimi, André-Luc Beylot,
Mobile edge computing for V2X architectures and applications: A survey,
Computer Networks,
Volume 206,
2022,
108797,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108797.
(https://www.sciencedirect.com/science/article/pii/S1389128622000263)
Abstract: In mobile environments, with the help of larger bandwidths and cloud computing solutions, any task can be offloaded from a mobile user equipment to be handled remotely. However, even though this process is accelerated with every cellular generation, with 5G being no exception, offloading to a faraway centralized cloud implies non-negligible delay. To tackle this issue concerning delay-sensitive applications, mobile edge computing, now denominated as multi-access edge computing (MEC), was brought to light. With cloud resources brought closer to the edge of the network, MEC greatly reduces task offloading delay, thereby striving to satisfy the constraints of real-time applications. As highly demanding mobile applications, vehicular networks are a target to be addressed in terms of performance, especially communication and computation delay. In this article, we establish the specificities of MEC when applied to the Internet of Vehicles (IoV), and survey recent papers studying implementations of MEC relevant to real-time vehicular considerations. We categorize these latest V2X architectures so as to unveil the mechanisms behind their improved performance: network availability and coverage, reliability and loss of network connectivity, large data handling and task offloading. This survey not only provides an initial understanding of the state-of-the-art advancements in the field of MEC-enabled vehicular networks, but also raises open issues and challenges that need to be addressed before enjoying the full benefits of this paradigm.
Keywords: Internet of Vehicles; IEEE-802.11p; Cellular networks; Mobile edge computing; Multi-access edge computing; Vehicular edge computing; Task offloading; Quality of service; Cache management

Divya Prerna, Rajkumar Tekchandani, Neeraj Kumar,
Device-to-device content caching techniques in 5G: A taxonomy, solutions, and challenges,
Computer Communications,
Volume 153,
2020,
Pages 48-84,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.057.
(https://www.sciencedirect.com/science/article/pii/S0140366419318225)
Abstract: Over the past many years, content caching is one of the major challenges in the 5G environment. With the advent of the Internet of Things (IoT) and 5G, users want to access various services within a fraction of seconds resulting an extra burden on the underlying network infrastructure to maintain Quality of Service (QoS) and Quality of Experience (QoE) provisions of different applications for the end-users and service providers. However, caching the most popular contents followed by device-to-device (D2D) sharing can resolve the aforementioned problems. But, access delay is still one of the challenges in front of the research community during content caching using D2D communications in a 5G environment. Motivated from these facts, this paper provides an in-depth survey of various D2D based content caching techniques used for popular content sharing among different devices in the 5G environment. A detailed taxonomy is presented to give deep insights to the readers about the findings, constraints, and challenges of various existing proposals. Finally, a relative comparison of the existing D2D content caching proposals is given in the text with respect to various parameters. It gives deep insights to the readers about the applicability of different caching techniques in 5G.
Keywords: 5G; D2D; Content caching; Resource allocation; Trust management; Security and privacy

Zhongyu Wang, Tiejun Lv, Zheng Chang,
Computation offloading and resource allocation based on distributed deep learning and software defined mobile edge computing,
Computer Networks,
Volume 205,
2022,
108732,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108732.
(https://www.sciencedirect.com/science/article/pii/S1389128621005843)
Abstract: In this paper, a software defined mobile edge computing (SD-MEC) in Internet of Things (IoT) is investigated, in which multiple IoT devices choose to offload their computation tasks to an appropriate edge server to support the emerging IoT applications with strict computation-intensive and latency-critical requirements. In considered SD-MEC networks, a joint computation offloading and power allocation problem is proposed to minimize the utility of weighted delay and power consumption in the distributed dense IoT. The optimization problem is a mixed-integer non-linear programming problem and difficult to solve by general optimization tools due to the nonconvexity and complexity. We propose a distributed deep learning based computation offloading and resource allocation (DDL-CORA) algorithm for SD-MEC IoT in which multiple parallel deep neural networks (DNNs) are invoked to generate the optimal offloading decision and resource scheduling. Additionally, we design a shared replay memory mechanism to effectively store newly generated offloading decisions which are further used to train and improve DNNs. The simulation results show that the proposed DDL-CORA algorithm can reduce the system utility on average 7.72% than reference Deep Q-network (DQN) algorithm and 31.9% than reference Branch-and-Bound (BNB) algorithm, and keep a good tradeoff between the complexity and utility performance.
Keywords: Software defined mobile edge computing; Internet of Things; Computation offloading; Power allocation; System utility; Distributed deep learning

Sparsh Mittal,
A survey on applications and architectural-optimizations of Micron’s Automata Processor,
Journal of Systems Architecture,
Volume 98,
2019,
Pages 135-164,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.07.006.
(https://www.sciencedirect.com/science/article/pii/S1383762119302139)
Abstract: Problems from a wide variety of application domains can be modeled as “nondeterministic finite automaton” (NFA) and hence, efficient execution of NFAs can improve the performance of several key applications. However, traditional architectures, such as CPU and GPU are not inherently suited for executing NFAs, and hence, special-purpose architectures are required for accelerating them. Micron’s automata processor (AP) exploits massively parallel in-memory processing capability of DRAM for executing NFAs and hence, it can provide orders of magnitude performance improvement compared to traditional architectures. In this paper, we present a survey of techniques that propose architectural optimizations to AP and use it for accelerating problems from various application domains. This paper will be useful not only for computer architects and processor-designers, but also for researchers in the field of bioinformatics, data-mining, machine learning and others.
Keywords: Micron; Processing-in-memory; Data-mining; Big data; DRAM; Non-deterministic finite automata; Pattern-matching

Kun Cao, Junlong Zhou, Tongquan Wei, Mingsong Chen, Shiyan Hu, Keqin Li,
A survey of optimization techniques for thermal-aware 3D processors,
Journal of Systems Architecture,
Volume 97,
2019,
Pages 397-415,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.01.003.
(https://www.sciencedirect.com/science/article/pii/S138376211830540X)
Abstract: Interconnect scaling has become a major design challenge for traditional planar (2D) integrated circuits (ICs). Three-dimensional (3D) IC that stacks multiple device layers through 3D stacking technology is regarded as an effective solution to this dilemma. A promising 3D IC design direction is to construct 3D processors. However, 3D processors are likely to suffer from more serious thermal issues as compared to conventional 2D processors, which may hinder the employment or even offset the benefits of 3D stacking. Therefore, thermal-aware design techniques should be adopted to alleviate the thermal problems with 3D processors. In this survey, we review works on system level optimization techniques for thermal-aware 3D processor design from hierarchical perspectives of architecture, floorplanning, memory management, and task scheduling. We first survey 3D processor architectures to demonstrate how a 3D processor can be constructed by using 3D stacking technology, and present an overview of thermal characteristics of the constructed 3D processors. We then review thermal-aware floorplanning, memory management and task scheduling techniques to show how the thermal impact on 3D processor performance can be reduced. A systematic classification method is utilized throughout the survey to emphasize similarities and differences of various thermal-aware 3D processor optimization techniques. This paper shows that the thermal impact on 3D processors is manageable by adopting thermal-aware techniques, thus making 3D processors into the mainstream in the near future.
Keywords: 3D processors; Architecture; Floorplanning; Memory management; Task scheduling; Thermal characteristics

Dewang Ren, Xiaolin Gui, Kaiyuan Zhang, Jie Wu,
Hybrid collaborative caching in mobile edge networks: An analytical approach,
Computer Networks,
Volume 158,
2019,
Pages 1-16,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.04.026.
(https://www.sciencedirect.com/science/article/pii/S1389128618308375)
Abstract: With caching at the base stations (BSs) in a cooperative manner, mobile edge caching (MEC) can alleviate the heavy backhaul burden and reduce the duplicated transmissions of content downloads, which has recently been considered a promising solution to cope with the exponentially increasing data traffic. However, how to maximize the storage utilization while reduce service latency and improve energy savings is still a critical issue in the large-scale mobile edge networks (MENs), since the growth of MENs in size as well as uneven users’ distribution make it difficult to determine which MEC should cache which content. To address this problem, we propose a hybrid collaborative caching (Hy-CoCa) design that jointly leverages local independent, intra-group collaborative and intra-network collaborative caching manners. MECs are clustered into disjoint groups, and then each MEC’s storage is partitioned into local, intra-group and intra-network portions. Local storage is reserved for storing the most popular contents to each MEC locally, so that users can directly fetch them from their associated MEC; Intra-group storage of different MECs inside the same group are regarded as an entity, which is used for collaboratively storing the middle popular contents, so as to reduce the probability for requesting contents from distant MECs; Intra-network storage of all MECs are leveraged for collaboratively storing less popular contents to different MECs in the entire MENs, as a means to improve the overall content diversity. Specifically, we first develop the Hy-CoCa’s framework to support users’ requests locally and conduct the construction of logical groups, considering users’ distribution and MECs’ proximity. Moreover, under the storage and popularity constraints, we formulate the storage allocation optimization problem to minimize average service latency and derive the optimal storage allocation. Furthermore, given an optimal storage allocation, we also formulate the request-aware content placement problem into an integer linear programming problem to maximize the overall energy savings. We prove the submodularity property of the objective function and propose a greedy algorithm with linear computational complexity, which can achieve (1−1/e)-optimality. Simulation results with real-world YouTube trace data demonstrate that our caching strategy can achieve 6% to 28% latency reduction and 9% to 75% energy savings improvement compared with other existing caching strategies.
Keywords: Mobile edge networks; Hybrid collaborative caching; Optimal storage allocation; Energy-efficient

Binghao Yan, Qinrang Liu, JianLiang Shen, Dong Liang,
BatchUp: Achieve fast TCAM update with batch processing optimization in SDN,
Future Generation Computer Systems,
Volume 134,
2022,
Pages 93-106,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.03.042.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001236)
Abstract: The parallel access mechanism supported by TCAM has made it popular with commercial switches. However, due to the ordered arrangement of rules, TCAM’s update speed cannot keep up with policy update requests, especially for software-defined networks, which expect to achieve fine-grained control over the network. Meanwhile, the legacy single-rule update solutions cause redundant rule moves due to weak global view. In this paper, we present a new two-stage batch processing solution BatchUp to alleviate the high time cost of the rule update process. In stage 1, we propose the costless rule insertion algorithm CRIA. CRIA models the rules to be inserted with the available locations in TCAM as a bipartite graph. By solving for maximum matching, CRIA obtains an upper bound on the number of rules that can be inserted without TCAM movement. However, the high utilization of TCAM makes it unrealizable to find available locations for all rules to be inserted directly. Therefore, in stage 2, we model the remaining rules insertion as an optimization problem for solving the minimum movement cost and prove that it has NP-hard complexity. A heuristic updating algorithm HRIA based on simulated annealing and greedy algorithm is proposed to search for approximate optimal solution. The evaluation results present that BatchUp has advantages over existing state-of-the-art solutions in terms of the number of TCAM moves and overall time cost, and has applicability and robustness in different scenarios.
Keywords: Software-defined networks; TCAM; Batch update; Maximum bipartite matching; Heuristic algorithm

Afia Anjum, Paul Agbaje, Arkajyoti Mitra, Emmanuel Oseghale, Ebelechukwu Nwafor, Habeeb Olufowobi,
Towards named data networking technology: Emerging applications, use cases, and challenges for secure data communication,
Future Generation Computer Systems,
Volume 151,
2024,
Pages 12-31,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.09.031.
(https://www.sciencedirect.com/science/article/pii/S0167739X23003631)
Abstract: Named Data Networking (NDN), an emerging Internet architecture is altering the basics of the networking model by making content directly addressable and routable. NDN changes the communication model by shifting from the current host-centric model to a content-centric model naming each content object hierarchically instead of using IP addresses. This change enables data consumers to request content using application-layer names that optimize network traffic and allow data security directly at the network layer, making the content of every data packet verifiable and enabling resilient communication in dynamic network environments, such as mobile ad hoc networks. This paper presents the concepts of NDN architecture and a comprehensive overview of emerging application use cases of the technology for secure data communications. We discuss the integration of NDN with the current Internet protocol and highlight how NDN works as a facilitator for addressing numerous concerns related to unique applications. Furthermore, we highlight the trust management and security aspects of NDN. Finally, we outline challenges relating to NDN adoption and present some proposed solutions.
Keywords: Named data networking; Trust management; TCP/IP; Information-centric networking; Network protocol; Data communication

Xu Zhao, Guangqiu Huang, Ling Gao, Maozhen Li, Quanli Gao,
Low load DIDS task scheduling based on Q-learning in edge computing environment,
Journal of Network and Computer Applications,
Volume 188,
2021,
103095,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103095.
(https://www.sciencedirect.com/science/article/pii/S108480452100117X)
Abstract: Edge computing, as a new computing model, is facing new challenges in network security while developing rapidly. Due to the limited performance of edge nodes, the distributed intrusion detection system (DIDS), which relies on high-performance devices in cloud computing, needs to be improved to low load to detect packets nearby the network edge. This paper proposes a low load DIDS task scheduling method based on Q-Learning algorithm in reinforcement learning, which can dynamically adjust scheduling strategies according to network changes in the edge computing environment to keep the overall load of DIDS at a low level, while maintaining a balance between the two contradictory indicators of low load and packet loss rate. Simulation experiments show that the proposed method has better low-load performance than other scheduling methods, and indicators such as malicious feature detection rate are not significantly reduced.
Keywords: Reinforcement learning; Intrusion detection; Task scheduling; Q-learning

Vishal Sharma, Dushantha Nalin K. Jayakody, Marwa Qaraqe,
Osmotic computing-based service migration and resource scheduling in Mobile Augmented Reality Networks (MARN),
Future Generation Computer Systems,
Volume 102,
2020,
Pages 723-737,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X19310738)
Abstract: Resources and services between the servers in Mobile Augmented Reality Networks (MARN) are tedious to manage. These networks comprise users possessing Augmented Reality (AR)-Virtual Reality (VR) applications. Low latency, robustness, and tolerance are the key requirements of these networks, which can be attained by using near-user solutions such as edge computing. However, management of services and scheduling them to near-user servers in an integrated environment of edge and public/private infrastructure are complex tasks. These require an optimal solution, which can be obtained by using “Osmotic Computing”, that has been recently proposed as a paradigm for the integration of edge and public/private cloud. This paper uses osmotic computing for effectively migrating and scheduling the services between the servers of the different layers. The paper also presents the details on various components that are used for applying osmotic computing to a network followed by core applications, types, service classification, migration, and scheduling through the rules of osmotic game formulated for its operations. The evaluations are conducted on 100,000 requests and the proposed approach shows significant performance with the probability of the error being 0.1 at 55.72% conservation of the energy and memory resources for the entire network despite the increasing number of users. The proposed approach also satisfies the conditions of the joint optimization functions presented in the system model and demonstrates that the system holds true even with varying users, thus, proving its robustness and tolerance against the number of users.
Keywords: Osmotic computing; Edge-computing; Augmented reality; Resource scheduling; Service migrations

Olufemi Odegbile, Chaoyi Ma, Shigang Chen, Yuanda Wang,
Policy enforcement in traditional non-SDN networks,
Journal of Parallel and Distributed Computing,
Volume 177,
2023,
Pages 39-52,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.02.005.
(https://www.sciencedirect.com/science/article/pii/S0743731523000205)
Abstract: Middleboxes are widely used in modern networks for a variety of network functions in cybersecurity, performance enhancement, and monitoring. Middlebox policy enforcement is however complex and tedious with unreliable manual re-configuration of legacy routers. The existing solution on automated policy enforcement relies on software-defined networking and does not apply to the traditional non-SDN networks, which remain popular today in enterprise deployment and core networks. This paper proposes a new architecture based entirely on software-defined middleboxes (instead of using software-defined switches in the prior art) to enable dependable and automated policy enforcement in non-SDN networks whose routers forward packets based on traditional routing protocols that are not policy-sensitive. We present a hot-potato enforcement strategy, which is then enhanced with two optimizations for load-balanced policy enforcement among software-defined middleboxes. Next, we propose two additional optimizations that minimize total traffic and aggregate end-to-end delays subject to link capacity constraints. Further enhancements are made to relieve middlebox processing overhead, avoid packet fragmentation due to policy enforcement, recover from failures, and mitigate delay for time-sensitive applications. We evaluate the proposed architecture on a real-life campus network topology and two simulated topologies to demonstrate the superior performance of our load-balanced enforcement strategies.
Keywords: Computer network; Network security; Policy enforcement; Network optimization; Middleboxes

Mian Guo, Qirui Li, Zhiping Peng, Xiushan Liu, Delong Cui,
Energy harvesting computation offloading game towards minimizing delay for mobile edge computing,
Computer Networks,
Volume 204,
2022,
108678,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108678.
(https://www.sciencedirect.com/science/article/pii/S1389128621005491)
Abstract: Mobile edge computing (MEC) has emerged for meeting the ever-increasing computation demands from mobile applications. Mobile users endowing with computation and energy harvesting (EH) capabilities, called EH devices, are desired in MEC systems. With computation capability, EH devices can switch between local computing and mobile-edge computing for better QoEs. With EH technologies, mobile users could survive perpetually for supporting long-term task processing. However, the heterogeneous computation resource among EH devices and edge servers, limited wireless resource as well as time-constrained harvestable energy challenge the computation offloading. This paper investigates a green MEC with EH devices and develops an effective EH computation offloading scheme towards minimizing delay for green MEC. We formulate and analyze the EH computation offloading problem with game theory. Then, an EH computation offloading game scheme, including the Lyapunov drift-based energy harvesting and computation offloading best response algorithms, is designed to find out optimal energy harvesting and computation offloading solutions to address the problem. The simulation results have been conducted to demonstrate the efficiency of the proposal.
Keywords: Mobile edge computing; Computation offloading; Energy harvesting

Esther Villar-Rodriguez, María Arostegi Pérez, Ana I. Torre-Bastida, Cristina Regueiro Senderos, Juan López-de-Armentia,
Edge intelligence secure frameworks: Current state and future challenges,
Computers & Security,
Volume 130,
2023,
103278,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103278.
(https://www.sciencedirect.com/science/article/pii/S0167404823001888)
Abstract: At the confluence of two great paradigms such as Edge Computing and Artificial Intelligence, Edge Intelligence arises. This new concept is about the smart exploitation of Edge Computing by bringing together reasoning and learning by Artificial Intelligence algorithms and the sensors/actuators computing capabilities. Security is the third paradigm that must join the team in order to have resilient and reliable systems to be used in real-world applications and use cases. Hence, smartness is, in this context, a puzzle of several independent pieces which, once fitted, can derived unprecedented benefits: a) security, b) low communication latency and network load, c) cost and energy saving and d) scalability by means of resource virtualization close to the IoT data generators (IoT devices). In fact, by paying exclusive attention to some of those main pillars and, therefore, disregarding others, edge computation once in operation often suffers from bad performance, unforeseen events or does not exploit the enormous potential that should be unlocked if a proper and complete specification had been laid down. With all this in mind, this work provides a technical review of the available and up-to-date frameworks to implement secure Edge Intelligence, pinpoints the most relevant unfilled gaps (strengths and weaknesses) and, last but nos least, includes challenges and future research lines as a result of our exploration.
Keywords: Edge computing; Edge intelligence; Artificial intelligence on edge; AI On edge; Distributed AI; Green computing; Security paradigm

Akash Sachan, Bibhas Ghoshal,
Learning based compilation of embedded applications targeting minimal energy consumption,
Journal of Systems Architecture,
Volume 116,
2021,
102116,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102116.
(https://www.sciencedirect.com/science/article/pii/S1383762121000886)
Abstract: The choice of correct optimization passes during compilation is one of the major factors that determines the performance and energy consumption of an application when executed on a target hardware. However, making such a choice is challenging since it requires knowledge of target architecture, code features and compiler pass interdependence. Software developers mostly rely on heuristic approaches while choosing the optimization passes. However, such heuristic solutions and random choices often yield sub-optimal results. In this work, we propose an automated technique of determining the optimal compiler optimization setting for minimum energy consumption utilizing power profile results and performance characteristics from real hardware and machine learning models. The proposed approach on one hand brings about reduction in energy consumption and on the other releases embedded software developers from making the complicated compiler optimization choices for each application. Our proposed five phase strategy when implemented on multi-core ARM based ODROID-XU4 experimental platform using the Clang Compiler shows a maximum of 8.43% improvement in power dissipation and a maximum of 27.27% in energy consumption for MiBench and Polybench representative benchmarks in comparison to state-of-art energy aware compilation strategies.
Keywords: Embedded system; Compiler; Minimal power dissipation; Machine learning; Multi-core processor

Denise Joanitah Birabwa, Daniel Ramotsoela, Neco Ventura,
Multi-agent deep reinforcement learning for user association and resource allocation in integrated terrestrial and non-terrestrial networks,
Computer Networks,
Volume 231,
2023,
109827,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109827.
(https://www.sciencedirect.com/science/article/pii/S1389128623002724)
Abstract: Integrating the terrestrial network with non-terrestrial networks to provide radio access as anticipated in the beyond 5G networks calls for efficient user association and resource allocation strategies. In this work, a weighted sum single objective optimization problem that maximizes the total network data rate while minimizing the mobility-induced handoffs and prioritizing service provisioning of mission-critical users in the integrated terrestrial and non-terrestrial network is formulated. The problem’s complexity is reduced by solving the defined problem in two phases; the user association followed by the resource distribution phase. Several proposed approaches to the user association sub-problem utilize a central node that requires nearly-complete information, which may not be available in real time. On the contrary, this paper proposes a centralized training and distributed execution multi-agent dueling double deep Q network (MA3DQN) solution. Each user collects the channel state and access node loading information in this approach and makes an association decision that considers its quality of service requirements. The algorithm’s performance is validated through comparison with the genetic algorithm (GA), the integer linear programming (ILP), a heuristic approximation-based solution, the greedy approach, and the random user association (RUA) algorithm. Moreover, the paper simulates the multi-agent deep Q network solution as an additional benchmark algorithm. Simulation results reveal that as the number of users in the network increases, the acquired data rate of the MA3DQN is within 0.48% and 0.4% of that achieved by the GA and ILP, respectively, and outperforms all other algorithms. Notably, the proposed MA3DQN algorithm presents the best running time, attaining a gain of 99.9% over the GA algorithm, which performs the poorest among the algorithms characterized by polynomial worst-case time complexity. Besides, the MA3DQN approach maintains a handoff probability of zero, unlike the ILP, the approximation-based, greedy, and RUA solutions.
Keywords: RAN user association; Resource allocation; Terrestrial networks; Non-terrestrial networks; Multi-agent deep reinforcement learning

Wazir Zada Khan, Ejaz Ahmed, Saqib Hakak, Ibrar Yaqoob, Arif Ahmed,
Edge computing: A survey,
Future Generation Computer Systems,
Volume 97,
2019,
Pages 219-235,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.02.050.
(https://www.sciencedirect.com/science/article/pii/S0167739X18319903)
Abstract: In recent years, the Edge computing paradigm has gained considerable popularity in academic and industrial circles. It serves as a key enabler for many future technologies like 5G, Internet of Things (IoT), augmented reality and vehicle-to-vehicle communications by connecting cloud computing facilities and services to the end users. The Edge computing paradigm provides low latency, mobility, and location awareness support to delay-sensitive applications. Significant research has been carried out in the area of Edge computing, which is reviewed in terms of latest developments such as Mobile Edge Computing, Cloudlet, and Fog computing, resulting in providing researchers with more insight into the existing solutions and future applications. This article is meant to serve as a comprehensive survey of recent advancements in Edge computing highlighting the core applications. It also discusses the importance of Edge computing in real life scenarios where response time constitutes the fundamental requirement for many applications. The article concludes with identifying the requirements and discuss open research challenges in Edge computing.
Keywords: Mobile edge computing; Edge computing; Cloudlets; Fog computing; Micro clouds; Cloud computing

Mingyang Song, Chunlin Li, Jingsong Ye, Xunqiang Gong, Youlong Luo,
Efficient consensus algorithm based on improved DPoS in UAV-assisted mobile edge computing,
Computer Communications,
Volume 207,
2023,
Pages 86-99,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.05.008.
(https://www.sciencedirect.com/science/article/pii/S0140366423001664)
Abstract: Aiming at the problems of low decentralization, low motivation for node voting, and malicious behavior of nodes for the traditional DPoS consensus mechanism in the blockchain-based UAV-assisted mobile edge computing environment, this paper proposes an improved DPoS-based consensus mechanism approach. First, the framework of a blockchain-based UAV-assisted mobile edge computing system is given, and the consensus mechanism design problem in this framework is analyzed. Second, a proxy node selection model is established based on the rights and votes obtained by the blockchain nodes, and the proxy nodes are selected to participate in the consensus process of the current cycle. Then, the voting behavior, block generation behavior, and block verification behavior of nodes are classified into positive and malicious behaviors to reward and punish the reputation value of nodes. Finally, a blockchain-based UAV-assisted mobile edge computing experimental environment is built, and the TDPoS algorithm, ADRP algorithm, and RDPoS algorithm are used as benchmark algorithms to experimentally compare with the proposed improved DPoS consensus-based algorithm. The experimental results show that the algorithms in this paper can improve network throughput, reduce block-out delay, and increase the proportion of secure proxy nodes.
Keywords: Consensus algorithm; DPoS; UAV-assisted mobile edge computing

Songli Zhang, Weijia Jia, Zhiqing Tang, Jiong Lou, Wei Zhao,
Efficient instance reuse approach for service function chain placement in mobile edge computing,
Computer Networks,
Volume 211,
2022,
109010,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109010.
(https://www.sciencedirect.com/science/article/pii/S1389128622001700)
Abstract: The combination of mobile edge computing and network function virtualization has led to the emergence of Virtualized Network Function (VNF) in a broader range of application scenarios. These latency-sensitive and highly dynamic services can be provided by combining multiple VNFs into Service Function Chains (SFCs). However, existing work has conspicuously neglected that online placing SFC with instance reuse can significantly improve resource utilization and save initialization time, which requires considering both the dynamic distribution of required VNFs over time and resource constraints on the edge network. In this paper, we initiate the study of Online SFC placement combined with Instance Reuse. An OSIR algorithm is proposed to gain a tradeoff between service costs and users’ quality of experience. The OSIR is designed based on deep reinforcement learning, which improves the system performance by maximizing the long-term cumulative reward. In OSIR, an SFC queue network is designed to extract the dynamic distribution of required VNFs over time, composed of memory space and the long short-term memory learning approach. The experimental results with real-world data traces show that OSIR can efficiently and effectively improve system performance and outperform the best result of all existing algorithms ranging from 17% to 26%.
Keywords: Mobile edge computing; Service function chain; Instance reuse

Juan Sebastian Camargo, Estefanía Coronado, Wilson Ramirez, Daniel Camps, Sergi Sánchez Deutsch, Jordi Pérez-Romero, Angelos Antonopoulos, Oscar Trullols-Cruces, Sergio Gonzalez-Diaz, Borja Otura, Giovanni Rigazzi,
Dynamic slicing reconfiguration for virtualized 5G networks using ML forecasting of computing capacity,
Computer Networks,
Volume 236,
2023,
110001,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110001.
(https://www.sciencedirect.com/science/article/pii/S1389128623004462)
Abstract: As 5G deployments continue to increase worldwide, new applications can fully leverage the exceptional features of the emerging mobile networks. Ultra-Reliable Low Latency Communications (URLLC) serve as an excellent example of applications highly sensitive to jitter and packet loss. To meet these demanding requirements, 5G relies on network slicing, network virtualization, and software-defined networks. This ecosystem enables the precise allocation of resources for each network slice. However, the applications’ resource demands may vary over time. In this challenging and overwhelming environment, traditional human decision-making for slice reconfiguration is not suitable anymore, due to the multitude of parameters and the need for extremely fast response times. Machine Learning (ML) comes as a tool that can enable better use of the available resources with faster and more intelligent management. This paper introduces an ML model that can predict slices’ traffic and dynamically reconfigure computational capacity. With these forecasting capabilities, the virtualized resources can be fine-tuned to suit the slices’ requirements, guaranteeing their Quality of Service (QoS). By doing so, Mobile Network Operators can make optimized use of the equipment, tailoring their needs to each service while complying with the QoS level. The results obtained demonstrate that the proposed ML model, in combination with a specific set of hysteresis rules, can accurately predict the saturation of virtualized capacity with up to 91% accuracy and proactively adapt it to the network slice requirements.
Keywords: O-RAN; NFV; Resource forecasting; AI; ML; Network reconfiguration; Kubernetes

Yunseong Lee, Arooj Masood, Wonjong Noh, Sungrae Cho,
DQN based user association control in hierarchical mobile edge computing systems for mobile IoT services,
Future Generation Computer Systems,
Volume 137,
2022,
Pages 53-69,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.07.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002333)
Abstract: This paper proposes a new user association control method to maximize system utility in terms of system throughput, edge throughput, and handover cost in mmWave-based HMEC (mm-HMEC) systems. We first formulated a non-convex mathematical programming model and then proposed centralized and distributed deep Q-network (DQN)-based range expansion bias control algorithms. We discovered that the proposed schemes provided polynomial communication overhead and computation complexity. Through simulations, we evaluated the proposed centralized and distributed schemes under various user equipment (UE) mobility models: random waypoint mobility model (RWM), random direction mobility model (RDM), and Manhattan mobility model (MAN). We first confirmed that the proposed distributed control achieves almost the same performance as the proposed centralized control. Second, with 10 slave-MEC servers, the proposed distributed control provides 212.17%, 62.07%, 46.36%, and 48.63% enhanced utility; 33.68%, 18.19%, 9.28%, and 12.66% enhanced average cell throughput; 14.32%, 38.32%, 63.87%, and 24.57% enhanced edge throughput; and 39.09%, 25.42%, 29.32%, and 25.95% reduced handover cost compared to random cell range expansion (CRE), standard CRE, dynamic CRE, and Q-learning-based CRE, respectively.
Keywords: Machine learning; Heterogeneous computing system; Resource allocation; Mobile networks; Edge computing; Mobility based services

S.M. Asiful Huda, Sangman Moh,
Survey on computation offloading in UAV-Enabled mobile edge computing,
Journal of Network and Computer Applications,
Volume 201,
2022,
103341,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103341.
(https://www.sciencedirect.com/science/article/pii/S1084804522000108)
Abstract: With the increasing growth of internet-of-things (IoT) devices, effective computation performance has become a critical issue. Many services provided by IoT devices (e.g., augmented reality, location-tracking, traffic systems, and autonomous driving) require intensive real-time data processing, which demands powerful computational resources. Mobile edge computing (MEC) has been introduced to effectively handle this problem reliably over the internet. The inclusion of a MEC server allows computationally intensive tasks to be offloaded from IoT devices. However, communication overhead and delays are major drawbacks. With the advantages of high mobility and low cost, unmanned aerial vehicles (UAVs) can mitigate this issue by acting as MEC servers. The offloading decisions for such scenarios involve service latency, energy/power consumption, and execution delays. For this reason, this study reviews UAV-enabled MEC solutions in which offloading was the focus of research. We compare the algorithms qualitatively to assess features and performance. Finally, we discuss open issues and research challenges in terms of design and implementation.

Samiran Kawtikwar, Rakesh Nagi,
HyLAC: Hybrid linear assignment solver in CUDA,
Journal of Parallel and Distributed Computing,
Volume 187,
2024,
104838,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2024.104838.
(https://www.sciencedirect.com/science/article/pii/S0743731524000029)
Abstract: The Linear Assignment Problem (LAP) is a fundamental combinatorial optimization problem with a wide range of applications. Over the years, significant progress has been made in developing efficient algorithms to solve the LAP, particularly in the realm of high-performance computing, leading to remarkable reductions in computation time. In recent years, hardware improvements in General Purpose Graphics Processing Units (GPGPUs) have shown promise in meeting the ever-increasing compute bandwidth requirements. This has attracted researchers to develop GPU-accelerated algorithms to solve the LAP. Recent work in the GPU domain has uncovered parallelism available in the problem structure to achieve significant performance improvements. However, each solution presented so far targets either sparse or dense instances of the problem and has some scope for improvement. The Hungarian algorithm is one of the most famous approaches to solving the LAP in polynomial time. Hungarian algorithm has classical O(N4) (Munkres') and tree based O(N3) (Lawler's) implementations. It is well established that the Munkres' implementation is faster for sparse LAP instances while the Lawler's implementation is faster for dense instances. In this work, we blend the GPU implementations of Munkres' and Lawler's to develop a Hybrid GPU-accelerated solver for LAP that switches between the two implementations based on available sparsity. Also, we improve the existing GPU implementations to reduce memory contention, minimize CPU-GPU synchronizations, and coalesced memory access. The resulting solver (HyLAC) works faster than existing CPU/GPU LAP solvers for sparse as well as dense problem instances. HyLAC achieves a speedup of up to 6.14× over existing state-of-the-art GPU implementation when run on the same hardware. We also develop an implementation to solve a list of small LAPs (tiled LAP), which is particularly useful in the optimization domain. This tiled LAP solver performs 22.59× faster than the existing implementation.
Keywords: Hungarian algorithm; Linear assignment problem; GPU-accelerated graph algorithms; High-performance computing; Parallel algorithm

Xutong Jiang, Yuhu Sun, Bowen Liu, Wanchun Dou,
Combinatorial double auction for resource allocation with differential privacy in edge computing,
Computer Communications,
Volume 185,
2022,
Pages 13-22,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.11.025.
(https://www.sciencedirect.com/science/article/pii/S0140366421004643)
Abstract: Application service providers deploy their services by purchasing certain edge computing resources from edge service providers. In this way, they can offer computing or storage support for users which promotes the formation of edge computing resource trading market. However, few research focus on the protection of the privacy of transaction participants. In this view of challenge, we propose a combinatorial double auction scheme based on differential privacy for multi-resource allocation in the trading market. In this scheme, our objective is to maximize the social welfare of the auction and apply the differential privacy based on Gaussian Mechanism to protect the security of the auction market. The simulation results show that the proposed scheme ensures the security and privacy of the auctioneers.
Keywords: Edge computing; Resource allocation; Differential privacy; Combinatorial double auction

Ali Shakarami, Mostafa Ghobaei-Arani, Ali Shahidinejad,
A survey on the computation offloading approaches in mobile edge computing: A machine learning-based perspective,
Computer Networks,
Volume 182,
2020,
107496,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107496.
(https://www.sciencedirect.com/science/article/pii/S1389128620311634)
Abstract: With the rapid developments in emerging mobile technologies, utilizing resource-hungry mobile applications such as media processing, online Gaming, Augmented Reality (AR), and Virtual Reality (VR) play an essential role in both businesses and entertainments. To soften the burden of such complexities incurred by fast developments of such serving technologies, distributed Mobile Edge Computing (MEC) has been developed, aimed at bringing the computation environments near the end-users, usually in one hop, to reach predefined requirements. In the literature, offloading approaches are developed to connect the computation environments to mobile devices by transferring resource-hungry tasks to the near servers. Because of some rising problems such as inherent software and hardware heterogeneity, restrictions, dynamism, and stochastic behavior of the ecosystem, the computation offloading issues consider as the essential challenging problems in the MEC environment. However, to the best of the author's knowledge, in spite of its significance, in machine learning-based (ML-based) computation offloading mechanisms, there is not any systematic, comprehensive, and detailed survey in the MEC environment. In this paper, we provide a review on the ML-based computation offloading mechanisms in the MEC environment in the form of a classical taxonomy to identify the contemporary mechanisms on this crucial topic and to offer open issues as well. The proposed taxonomy is classified into three main fields: Reinforcement learning-based mechanisms, supervised learning-based mechanisms, and unsupervised learning-based mechanisms. Next, these classes are compared with each other based on the essential features such as performance metrics, case studies, utilized techniques, and evaluation tools, and their advantages and weaknesses are discussed, as well. Finally, open issues and uncovered or inadequately covered future research challenges are argued, and the survey is concluded.
Keywords: Computation offloading; Mobile edge computing; Machine learning; Reinforcement learning; Supervised learning; Unsupervised learning

Abeer Z. Al-Marridi, Amr Mohamed, Aiman Erbad,
Reinforcement learning approaches for efficient and secure blockchain-powered smart health systems,
Computer Networks,
Volume 197,
2021,
108279,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108279.
(https://www.sciencedirect.com/science/article/pii/S1389128621003005)
Abstract: Emerging technological innovation toward e-Health transition is a worldwide priority for ensuring people’s quality of life. Hence, secure exchange and analysis of medical data amongst diverse organizations would increase the efficiency of e-Health systems toward elevating medical phenomena such as outbreaks and acute patients’ disorders. However, medical data exchange is challenging since issues, such as privacy, security, and latency may arise. Thus, this paper introduces Healthchain-RL, an adaptive, intelligent, consortium, and secure Blockchain-powered health system employing artificial intelligence, especially Deep Reinforcement Learning (DRL). Blockchain and DRL technologies show their robust performance in different fields, including healthcare systems. The proposed Healthchain-RL framework aggregates heterogeneous healthcare organizations with different requirements using the power of Blockchain while maintaining an optimized framework via an online intelligent decision-making RL algorithm. Hence, an intelligent Blockchain Manager (BM) was proposed based on the DRL, mainly Deep Q-Learning and it is variations, to optimizes the Blockchain network’s behavior in real-time while considering medical data requirements, such as urgency and security levels. The proposed BM works toward intelligently changing the blockchain configuration while optimizing the trade-off between security, latency, and cost. The optimization model is formulated as a Markov Decision Process (MDP) and solved effectively using three RL-based techniques. These three techniques are Deep Q-Networks (DQN), Double Deep Q-Networks (DDQN), and Dueling Double Deep Q-Networks (D3QN). Finally, a comprehensive comparison is conducted between the proposed techniques and two heuristic approaches. The proposed strategies converge in real-time adaptivity to the system status while maintaining maximum security and minimum latency and cost.
Keywords: Reinforcement learning; Blockchain; e-Health; Multi-objective optimization; Healthchain-RL

Ruifeng Ma, Yufeng Zhan, Yuanqing Xia, Chuge Wu, Liwen Yang, Runze Gao,
Sonnet: A control-theoretic approach for resource allocation in cluster management,
Future Generation Computer Systems,
Volume 153,
2024,
Pages 169-181,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.11.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X23004272)
Abstract: Cluster users expect to minimize the resource costs while ensuring target performance for different applications. It is particularly difficult to reach such a goal, because the applications are diverse with dynamic load changes, and interference exists between them. In addition, the performance of the applications depends on heterogeneous resources with different costs. However, existing works either use simplistic and generalized heuristics that disregard resource-specific characteristics or need suspending service to get expert knowledge to optimize the resource cost for a brand-new application or runtime, which fails to optimize the resource allocation finely. In this paper, we propose Sonnet, a control-theoretic approach to perform efficient resource allocation. Sonnet can efficiently optimize the cost of resources while satisfying the SLO by quickly establishing new application performance models through only online profiling and without affecting service. Experiments on Docker Swarm using various open-source benchmarks demonstrate that Sonnet can decrease the SLO violation rate by 91% while reducing resource costs up to 47% compared with the state-of-the-arts.
Keywords: Dynamic applications; Resource allocation; Cluster management; Model-free adaptive control

Chen Chen, Lars Nagel, Lin Cui, Fung Po Tso,
Distributed federated service chaining: A scalable and cost-aware approach for multi-domain networks,
Computer Networks,
Volume 212,
2022,
109044,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109044.
(https://www.sciencedirect.com/science/article/pii/S1389128622001931)
Abstract: Future networks are expected to support cross-domain, cost-aware and fine-grained services in an efficient and flexible manner. Service Function Chaining (SFC) has been introduced as a promising approach to deliver these services. In the literature, centralized resource orchestration is usually employed to process SFC requests and manage computing and network resources. However, centralized approaches inhibit the scalability and domain autonomy in multi-domain networks. They also neglect location and hardware dependencies of service chains. In this paper, we propose Distributed Federated Service Chaining (DFSC), a framework for orchestrating and maintaining SFC placement in a distributed fashion while sharing only a minimal amount of domain information and control. First, a deployment cost minimization problem is formulated as an Integer Linear Programming (ILP) problem with fine-grained constraints for location and hardware dependencies. We show that this problem is NP-hard. Then, a placement algorithm is devised to use information only on inter-domain paths and border nodes. Our extensive experimental results demonstrate that DFSC efficiently optimizes the deployment cost, supports domain autonomy and enables faster decision-making. The results also show that DFSC finds solutions within a factor 1.15 of the optimal solution on average. Compared to a centralized approach in the literature, DFSC reduces the deployment cost by up to 20% and uses 70% less decision-making time.
Keywords: Multi-domain; Distributed orchestration; Service function chaining

Qiang Tang, Lixin Liu, Caiyan Jin, Jin Wang, Zhuofan Liao, Yuansheng Luo,
An UAV-assisted mobile edge computing offloading strategy for minimizing energy consumption,
Computer Networks,
Volume 207,
2022,
108857,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108857.
(https://www.sciencedirect.com/science/article/pii/S1389128622000688)
Abstract: Owing to the high economic benefits, flexible deployment, and controllable maneuverability, unmanned aerial vehicles (UAVs) have been envisioned as promising and potential technologies for dispensing wireless communication services. This paper investigates a mobile edge computing (MEC) system assisted by multiple access points (APs) and an UAV, in which APs may not be able to straightly establish wireless communications with terrestrial Internet of Thing devices (IoT) due to ground signal blockage. Consequently, an UAV is dispatched as a mobile AP to serve a group of users and render the air-to-ground channel. In this scenario, we contemplate dividing the computing tasks of IoTDs into three parts: either be calculated locally, or offloaded to the UAV for processing, or accomplished on AP through relaying. This work attempts to minimize the weighted sum of communication consumption, calculation consumption, and the UAV’s flight consumption over a finite UAV mission duration by jointly optimizing calculation task allocation ratio, power distribution as well as the UAV’s trajectory. However, the resulting problem we put forward is demonstrated to be highly non-convex and challenging to solve. To tackle this issue, we decompose the original problem into two sub-problems hinging on the block coordinate descent (BCD) method. We settle the two sub-problems iteratively through the Lagrangian duality method and succession convex approximation (SCA) technique. The simulation results further reveal that the proposed approach is superior to other comparison baselines.
Keywords: Mobile edge computing; Calculation task allocation; Unmanned aerial vehicle communications

Jingling Yuan, Hua Xiao, Zhishu Shen, Tiehua Zhang, Jiong Jin,
ELECT: Energy-efficient intelligent edge–cloud collaboration for remote IoT services,
Future Generation Computer Systems,
Volume 147,
2023,
Pages 179-194,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.030.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001681)
Abstract: Existing centralized cloud-based solution is challenging to cope with the explosive growth of data generated from massive IoT devices to meet the requirements of critical services, especially for remote IoT services provided in remote/rural areas where the resource and energy capacity of local data processing infrastructure is limited. To solve this issue, we propose ELECT, an energy-efficient intelligent edge-cloud collaboration scheme that achieves satisfactory data processing performance. ELECT includes a platform that utilizes edge computing node as a core element that locates close to the IoT nodes for coordinating the data processing between the cloud and the IoT devices. Specifically, based on the importance of a node’s collected data in the overall data processing performance, a dynamic IoT node management algorithm is developed to manage each node’s active/inactive status to reduce energy consumption. Moreover, a deep Q-network (DQN)-based workflow scheduling algorithm that fully utilizes the data-centric device–edge–cloud continuum is introduced to reduce makespan and energy consumption for obtaining a compromising solution. For verification, we develop an experimental environment simulating structural health monitoring (SHM) services in remote areas. Extensive experiments verify the effectiveness of ELECT in terms of various service requirements, including makespan, energy consumption and communication cost.
Keywords: Edge–cloud collaboration; Dynamic management of IoT nodes; Workflow scheduling; Services in remote areas; Structural health monitoring; Deep Q-network

Ibrahim A. Elgendy, Weizhe Zhang, Yu-Chu Tian, Keqin Li,
Resource allocation and computation offloading with data security for mobile edge computing,
Future Generation Computer Systems,
Volume 100,
2019,
Pages 531-541,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.05.037.
(https://www.sciencedirect.com/science/article/pii/S0167739X18328346)
Abstract: With the considerable growth of mobile users (MUs) and IoT devices, complex applications and multimedia services are rapidly increasing, thereby requiring additional computations and high data communication. However, these devices are still resource-constrained with limited computation power and energy. Furthermore, security is considered a critical issue for sensitive information communication. This study presents a multiuser resource allocation and computation offloading model with data security to address the limitations of such devices. First, the computation and radio resources are jointly considered for multiuser scenarios to guarantee the efficient utilization of shared resources. In addition, an AES cryptographic technique is introduced as a security layer to protect sensitive information from cyber-attacks. Furthermore, an integrated model, which jointly considers security, computation offloading, and resource allocation, is formulated to minimize time and energy consumption of the entire system. Finally, an offloading algorithm is developed with detailed processes to determine the optimal computation offloading decision for MUs. Simulation results show that our model and algorithm can significantly improve the performance of the entire system compared with local execution and full offloading schemes.
Keywords: Computation offloading; Internet of Things (IoT); Mobile-edge computing; Optimization; Security

Kan Wang, Xuan Liu, Hongfang Zhou, Dapeng Lan, Zhen Gao, Amir Taherkordi, Yujie Ye, Yuan Gao,
Reinforcement learning-based cost-efficient service function chaining with CoMP zero-forcing beamforming in edge networks,
Future Generation Computer Systems,
Volume 141,
2023,
Pages 355-368,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.11.022.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003831)
Abstract: As two promising paradigms in emerging 6G wireless systems, service function chaining (SFC) and mobile edge computing (MEC) have attracted insensitive attentions from both industry and academia, and would bring more close-proximity services to 6G users with communication, computing and caching (3C) resources, yet also faced with challenges arising in time-varying channel conditions and resource dynamics. In this work, boosted by recent advents in artificial intelligence and reinforcement learning, we investigate the on-line SFC deployment in the edge of 6G wireless systems via the actor–critic learning framework. First, one long-run cost-efficient SFC deployment problem is investigated, and the coordinated multiple points (CoMP)-based zero-forcing beamforming is utilized to cancel the interference across SFCs. Then, by exploiting the Markov decision processes (MDP) property of long-run SFC deployment, one natural gradient-based actor–critic framework is proposed to characterize edge network dynamics, and meanwhile facilitates the training of neural networks to the global optimum. Next, to lower the size of action space, we follow the principle that a subproblem is embedded into each state–action pair’s critic to solve the reward function, and then utilize both the ℓp (0<p<1) norm-based successive convex approximation (SCA) and proximal center-based dual decomposition to approach the global optimum and accelerate the convergence. Finally, numerical results are used to validate proposed actor–critic approach, showing that the communication resource management deserves special attentions in the SFC deployment in the edge of 6G wireless systems.
Keywords: Service function chaining; Mobile edge computing; Actor–critic; Zero-forcing beamforming; Proximal center dual decomposition

Dinesh Soni, Neetesh Kumar,
Machine learning techniques in emerging cloud computing integrated paradigms: A survey and taxonomy,
Journal of Network and Computer Applications,
Volume 205,
2022,
103419,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103419.
(https://www.sciencedirect.com/science/article/pii/S1084804522000765)
Abstract: Cloud computing offers Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) to provide compute, network, and storage capabilities to the clients utilizing the pay-per-use model. On the other hand, Machine Learning (ML) based techniques are playing a major role in effective utilization of the computing resources and offering Quality of Service (QoS). Based on the customer’s application requirements, several cloud computing-based paradigms i.e., edge computing, fog computing, mist computing, Internet of Things (IoT), Software-Defined Networking (SDN), cybertwin, and industry 4.0 have been evolved. These paradigms collaborate to offer customer-centric services with the backend of cloud server/data center. In brief, cloud computing has been emerged with respect to the above-mentioned paradigms to enhance the Quality of Experience (QoE) for the users. In particular, ML techniques are the motivating factor to backend the cloud for emerging paradigms, and ML techniques are essentially enhancing the usages of these paradigms by solving several problems of scheduling, resource provisioning, resource allocation, load balancing, Virtual Machine (VM) migration, offloading, VM mapping, energy optimization, workload prediction, device monitoring, etc. However, a comprehensive survey focusing on multi-paradigm integrated architectures, technical and analytical aspects of these paradigms, and the role of ML techniques in emerging cloud computing paradigms are still missing, and this domain needs to be explored. To the best of the authors’ knowledge, this is the first survey that investigates the emerging cloud computing paradigms integration considering the most dominating problem-solving technology i.e., ML. This survey article provides a comprehensive summary and structured layout for the vast research on ML techniques in the emerging cloud computing paradigm. This research presents a detailed literature review of emerging cloud computing paradigms: cloud, edge, fog, mist, IoT, SDN, cybertwin, and industry 4.0 (IIoT) along with their integration using ML. To carry out this study, majorly, the last five years (2017-21) articles are explored and analyzed thoroughly to understand the emerging integrated architectures, the comparative study on several attributes, and recent trends. Based on this, research gaps, challenges, and future trends are revealed.
Keywords: Cloud computing; Edge computing; Fog computing; Mist computing; IoT; SDN; Cybertwin; Industry 4.0; Digitaltwin; IIoT; Machine learning (ML); Integrated computing paradigm

Simone Bolettieri, Raffaele Bruno, Enzo Mingozzi,
Application-aware resource allocation and data management for MEC-assisted IoT service providers,
Journal of Network and Computer Applications,
Volume 181,
2021,
103020,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103020.
(https://www.sciencedirect.com/science/article/pii/S1084804521000473)
Abstract: To support the growing demand for data-intensive and low-latency IoT applications, Multi-Access Edge Computing (MEC) is emerging as an effective edge-computing approach enabling the execution of delay-sensitive processing tasks close to end-users. However, most of the existing works on resource allocation and service placement in MEC systems overlook the unique characteristics of new IoT use cases. For instance, many IoT applications require the periodic execution of computing tasks on real-time data streams that originate from devices dispersed over a wide area. Thus, users requesting IoT services are typically distant from the data producers. To fill this gap, the contribution of this work is two-fold. Firstly, we propose a MEC-compliant architectural solution to support the operation of multiple IoT service providers over a common MEC platform deployment, which enables the steering and shaping of IoT data transport within the platform. Secondly, we model the problem of service placement and data management in the proposed MEC-based solution taking into account the dependencies at the data level between IoT services and sensing resources. Our model also considers that caches can be deployed on MEC hosts, to allow the sharing of the same data between different IoT services with overlapping geographical scope, and provides support for IoT services with heterogeneous QoS requirements, such as different frequencies of periodic task execution. Due to the complexity of the optimisation problem, a heuristic algorithm is proposed using linear relaxation and rounding techniques. Extensive simulation results demonstrate the efficiency of the proposed approach, especially when traffic demands generated by the service requests are not uniform.
Keywords: IoT; Mobile edge computing (MEC); Service placement; Data management; Traffic shaping; Application-aware caching; Optimisation

Shiyong Li, Huan Liu, Wenzhe Li, Wei Sun,
Optimal cross-layer resource allocation in fog computing: A market-based framework,
Journal of Network and Computer Applications,
Volume 209,
2023,
103528,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103528.
(https://www.sciencedirect.com/science/article/pii/S1084804522001692)
Abstract: The potential upsides of fog computing are undeniable, including better near-real-time response, reduced transmission costs and analysis for IoT, which will further have a significant impact on operation of factories, enterprises, organizations and even the quality of our life. However, due to the resource-constrained nature, heterogeneity and distributed nodes, how to manage the extraordinarily complex fog computing resources is necessary to improve the QoE of users. This paper investigates the rational resource allocation for fog computing from the cross-layer point of perspective, and formulates the resource allocation scenario as Cloud–Fog–User market which is composed of cloud center devices, fog nodes, and user devices. Firstly, the resource market is divided into Cloud–Fog market containing cloud layer and fog layer, and Fog–User market containing fog layer and user layer based on different scenarios separately. The paper proposes a market-based framework for fog computing networks, which enables the cloud layer and the fog layer to allocate their resources in the form of pricing, payment and supply–demand relationship, while analyzes the relationship among resource allocation participants in the market. Then the paper investigates the utility optimization models from the viewpoints of both the participants of respective markets, so as to optimize optimal payment and optimal resource allocation via convex optimization techniques, and proposes a gradient-based iterative algorithm to optimize the utilities, respectively. Finally, experimental results have displayed the performance of convex analysis results and effectiveness of the proposed algorithm.
Keywords: Fog computing; Cross-layer design; Market framework; Gradient-based algorithm; Resource allocation; Utility maximization

Pu Wang, Tao Ouyang, Guocheng Liao, Jie Gong, Shuai Yu, Xu Chen,
Edge intelligence in motion: Mobility-aware dynamic DNN inference service migration with downtime in mobile edge computing,
Journal of Systems Architecture,
Volume 130,
2022,
102664,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102664.
(https://www.sciencedirect.com/science/article/pii/S1383762122001771)
Abstract: Edge intelligence (EI) becomes a trend to push the deep learning frontiers to the network edge, so that deep neural networks (DNNs) applications can be well leveraged at resource-constrained mobile devices with benefits of edge computing. Due to the high user mobility among scattered edge servers in many scenarios such as internet of vehicular applications, dynamic service migration is desired to maintain a reliable and efficient quality of service (QoS). However, inevitable service downtime incurred by service migration would largely degrade the real-time performance of delay-sensitive DNN inference services. To address this issue, we advocate a user-centric management for dynamic DNN inference service migration with flexible multi-exit mechanism, aiming at maximizing overall user utility (e.g., DNN model inference accuracy) with various service downtime. We first leverage dynamic programming to propose an optimal offline migration and exit point selection strategy (OMEPS) algorithm when complete future information of user behaviors is available. Amenable to a more practical application domain without complete future information, we incorporate the OMEPS algorithm into a model predictive control (MPC) framework, then construct a mobility-aware service migration and DNN exit point selection (MOMEPS) algorithm, which improves the long-term user utility within limited predictive future information. However, heavy computation overheads of MOMEPS algorithm impose burdens on mobile devices, thus we further advocate a cost-efficient algorithm, named smart-MOMEPS, which introduces a smart migration judgement based on Neural Networks to control the implementation of (MOMEPS) algorithm by wisely estimating whether the DNN service should be migrated or not. Extensive trace-driven simulation results demonstrate the superior performance of our smart-MOMEPS algorithm for achieving significant overall utility improvements with low computation overheads compared with other online algorithms.
Keywords: DNN service migration; Multi-exit DNN; Service downtime; Mobile edge computing; Model predictive control

Shaobo Zhang, Xiong Li, Zhiyuan Tan, Tao Peng, Guojun Wang,
A caching and spatial K-anonymity driven privacy enhancement scheme in continuous location-based services,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 40-50,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.10.053.
(https://www.sciencedirect.com/science/article/pii/S0167739X17321398)
Abstract: With the rapid pervasion of location-based services (LBSs), protection of location privacy has become a significant concern. In most continuous LBSs’ privacy-preserving solutions, users need to transmit the location query data to an untrusted location service provider (LSP) to obtain query results, and the users discard these results immediately after using them. This results in an ineffective use of these results by future queries and in turn leads to a higher risk to user privacy from the LSP. To address these issues, we generally use caching to cache the query results for users’ future queries. However, the minimization of the interaction between users and LSPs is a challenge. In this paper, we propose an enhanced user privacy scheme through caching and spatial K-anonymity (CSKA) in continuous LBSs; it adopts multi-level caching to reduce the risk of exposure of users’ information to untrusted LSPs. In continuous LBS queries, our scheme first utilizes the Markov model to predict the next query location according to the user mobility. Then, according to the predicted location, cell’s cache contribution rate, and data freshness, an algorithm for forming spatial K-anonymity is designed to improve the user’s cache hit rate and enhance the user location privacy. The security analysis and simulation results demonstrate that our proposed CSKA scheme can provide higher privacy protection than a few previous methods, and it can minimize the overhead of the LBS server.
Keywords: Location privacy; Multi-level caching; Spatial K-anonymity; User mobility; Cache hit rate

Hsin-Yao Hsu, Gautam Srivastava, Hsin-Te Wu, Mu-Yen Chen,
Remaining useful life prediction based on state assessment using edge computing on deep learning,
Computer Communications,
Volume 160,
2020,
Pages 91-100,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.05.035.
(https://www.sciencedirect.com/science/article/pii/S0140366420306277)
Abstract: Intelligent industrial production has recently emerged as an important trend for application of the Industrial Internet of Things (IIoT) in edge computing. This study applied remote edge devices and edge servers, preprocessing the signal sensor, through covert data to cloud storage, and loaded the data to propose several deep learning methods to assess the status of aircraft engines in operation, and to classify stages of operational degradation so as to predict the functional remaining lifespan of components. The predicted results are transmitted to a cloud-based server for monitoring and maintenance.
Keywords: Edge computing; Deep learning; Remaining useful life; Degradation state; Internet of Things

Alireza Erfanian, Farzad Tashtarian, Christian Timmerer, Hermann Hellwagner,
QoCoVi: QoE- and cost-aware adaptive video streaming for the Internet of Vehicles,
Computer Communications,
Volume 190,
2022,
Pages 1-9,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.03.003.
(https://www.sciencedirect.com/science/article/pii/S0140366422000780)
Abstract: Recent advances in embedded systems and communication technologies enable novel, non-safety applications in Vehicular Ad Hoc Networks (VANETs). Video streaming has become a popular core service for such applications. In this paper, we present QoCoVi as a QoE- and cost-aware adaptive video streaming approach for the Internet of Vehicles (IoV) to deliver video segments requested by mobile users at specified qualities and deadlines. Considering a multitude of transmission data sources with different capacities and costs, the goal of QoCoVi is to serve the desired video qualities with minimum costs. By applying Dynamic Adaptive Streaming over HTTP (DASH) principles, QoCoVi considers cached video segments on vehicles equipped with storage capacity as the lowest-cost sources for serving requests. We design QoCoVi in two SDN-based operational modes: (i) centralized and (ii) distributed. In centralized mode, we can obtain a suitable solution by introducing a mixed-integer linear programming (MILP) optimization model that can be executed on the SDN controller. However, to cope with the computational overhead of the centralized approach in real IoV scenarios, we propose a fully distributed version of QoCoVi based on the proximal Jacobi alternating direction method of multipliers (ProxJ-ADMM) technique. The effectiveness of the proposed approach is confirmed through emulation with Mininet-WiFi in different scenarios.
Keywords: Adaptive video streaming; Internet of Vehicles; Distributed optimization; ProxJ-ADMM

Zhe Dai, Liang Deng, YongGang Che, Ming Li, Jian Zhang, Yueqing Wang,
Evaluating performance portability of five shared-memory programming models using a high-order unstructured CFD solver,
Journal of Parallel and Distributed Computing,
Volume 187,
2024,
104831,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104831.
(https://www.sciencedirect.com/science/article/pii/S0743731523002010)
Abstract: This paper presents implementing and optimizing a high-order unstructured computational fluid dynamics (CFD) solver using five shared-memory programming models: CUDA, OpenACC, OpenMP, Kokkos, and OP2. The study aims to evaluate the performance of these models on different hardware architectures, including NVIDIA GPUs, x86-based Intel/AMD, and Arm-based systems. The goal is to determine whether these models can provide developers with performance-portable solvers running efficiently on various architectures. The paper forms a more holistic view of a high-order solver across multiple platforms by visualizing performance portability (PP) and measuring productivity. It gives guidelines for translating existing codebases and their data structures to these models.
Keywords: Performance; High-order CFD; Portability; Kokkos; DSL

Adam Krzywaniak, Paweł Czarnul, Jerzy Proficz,
Dynamic GPU power capping with online performance tracing for energy efficient GPU computing using DEPO tool,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 396-414,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.03.041.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001267)
Abstract: GPU accelerators have become essential to the recent advance in computational power of high-performance computing (HPC) systems. Current HPC systems’ reaching an approximately 20–30 mega-watt power demand has resulted in increasing CO2 emissions, energy costs and necessitate increasingly complex cooling systems. This is a very real challenge. To address this, new mechanisms of software power control could be employed. In this paper, a dynamic new method of limiting software power is introduced on one of the latest NVIDIA GPUs: a software tool called the Dynamic Energy-Performance Optimiser (DEPO). DEPO minimizes the energy consumption of the CUDA based GPU workloads, with respect to one of the three given metrics: minimum of energy (E), Energy-Delay product (EDP) and Energy-Delay sum (EDS). The tool gathers power measurements from NVIDIA Management Library (NVML). Measuring the application progress at runtime is based on CUDA Profiling Tools Interface (CUPTI) kernel-counting. We have evaluated the DEPO tool on the NVIDIA RTX A4500 and A100 GPUs with machine learning workloads. Depending on the application (training of neural networks: Resnet152, Densenet161, VGG-19 or a GEMM benchmark) for the E target metric, we were able to obtain energy savings exceeding 22% for both NVIDIA A100 and RTX A4500 GPUs while the performance drop has never been higher than 20%. Using one of the bi-objective EDP or EDS metrics allowed finding configurations resulting in 15% or 18% of energy saved with only 8% of performance loss. For most of the experiments the percentage-wise performance penalty is lower than the energy savings. This demonstrates its potential for energy consumption reduction in HPC systems with GPU accelerators.
Keywords: Energy-aware computing; High-performance computing; Green computing; Machine learning; GPU energy optimization

Tri Nguyen, Huong Nguyen, Tuan Nguyen Gia,
Exploring the integration of edge computing and blockchain IoT: Principles, architectures, security, and applications,
Journal of Network and Computer Applications,
Volume 226,
2024,
103884,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103884.
(https://www.sciencedirect.com/science/article/pii/S1084804524000614)
Abstract: IoT systems are widely used in various applications, including healthcare, agriculture, manufacturing, and smart cities. However, these systems still have limitations, such as lack of security, high latency, energy inefficiency, the inefficiency of bandwidth utilization, and shortage of automaticity. The integration of edge computing and blockchain into IoT has been proposed to address these limitations. Yet, this integration is challenging and has not been deeply investigated. This paper aims to conduct a review of the integration of edge computing and blockchain into IoT systems. To the best of our knowledge, this is the first review paper that covers all aspects of system architectures and categories of blockchain-based edge deployment, complete security requirements, including confidentiality, integrity, authentication, authorization/access control, privacy, trust/confidence, transparency, availability, secure automaticity, and tolerance, and applications of blockchain-based edge potential usages with consideration of security requirements. Additionally, this review provides comprehensive discussions of challenges and insights into the future direction of blockchain-based edge IoT systems. The review aims to serve as an entry point for non-expert readers and researchers to various aspects of blockchain-based edge IoT systems.
Keywords: Edge computing; Blockchain; Internet-of-thing; Architecture; Security; Application

Abhishek Hazra, Pradeep Rana, Mainak Adhikari, Tarachand Amgoth,
Fog computing for next-generation Internet of Things: Fundamental, state-of-the-art and research challenges,
Computer Science Review,
Volume 48,
2023,
100549,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100549.
(https://www.sciencedirect.com/science/article/pii/S1574013723000163)
Abstract: In recent times, the Internet of Things (IoT) applications, including smart transportation, smart healthcare, smart grid, smart city, etc. generate a large volume of real-time data for decision making. In the past decades, real-time sensory data have been offloaded to centralized cloud servers for data analysis through a reliable communication channel. However, due to the long communication distance between end-users and centralized cloud servers, the chances of increasing network congestion, data loss, latency, and energy consumption are getting significantly higher. To address the challenges mentioned above, fog computing emerges in a distributed environment that extends the computation and storage facilities at the edge of the network. Compared to centralized cloud infrastructure, a distributed fog framework can support delay-sensitive IoT applications with minimum latency and energy consumption while analyzing the data using a set of resource-constraint fog/edge devices. Thus our survey covers the layered IoT architecture, evaluation metrics, and applications aspects of fog computing and its progress in the last four years. Furthermore, the layered architecture of the standard fog framework and different state-of-the-art techniques for utilizing computing resources of fog networks have been covered in this study. Moreover, we included an IoT use case scenario to demonstrate the fog data offloading and resource provisioning example in heterogeneous vehicular fog networks. Finally, we examine various challenges and potential solutions to establish interoperable communication and computation for next-generation IoT applications in fog networks.
Keywords: Internet of Things; Fog computing; Cloud computing; Objectives; Architecture; Applications; Research challenges

Xianlong Zhao, Kexin Yang, Qimei Chen, Duo Peng, Hao Jiang, Xianze Xu, Xinzhuo Shuang,
Deep learning based mobile data offloading in mobile edge computing systems,
Future Generation Computer Systems,
Volume 99,
2019,
Pages 346-355,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.04.039.
(https://www.sciencedirect.com/science/article/pii/S0167739X19304406)
Abstract: Mobile Edge Computing (MEC) has been regarded as a key technology of the future communication systems in the industry due to its capability to satisfy a wide range of requirements of the emerging wireless terminals (virtual reality devices, augmented reality, and Intelligent Vehicles), such as high data rate, low latency, and huge computation. Besides, difficulties in the lack of resources in the licensed band have prompted researches on mobile data offloading. Owing to the cheap and effective characteristics of WiFi AP, it is utilized to offload some devices from small base stations (SBS) in this paper. Furthermore, a multi-Long Short Term Memory (LSTM) based deep-learning model is constructed to predict the real-time traffic of SBS, which may help us perform the offloading process accurately. According to the prediction results, an mobile data offloading strategy based on cross entropy (CE) method has been proposed. The presented results based on actual dataset provide strong proofs of the applicability of the prediction and offloading scheme we proposed.
Keywords: Mobile data offloading; Deep learning; Mobile edge computing

Santanu Pattanayak, Subhrajit Nag, Sparsh Mittal,
CURATING: A multi-objective based pruning technique for CNNs,
Journal of Systems Architecture,
Volume 116,
2021,
102031,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102031.
(https://www.sciencedirect.com/science/article/pii/S1383762121000357)
Abstract: As convolutional neural networks (CNNs) improve in accuracy, their model size and computational overheads have also increased. These overheads make it challenging to deploy the CNNs on resource-constrained devices. Pruning is a promising technique to mitigate these overheads. In this paper, we propose a novel pruning technique called CURATING that looks at the pruning of CNNs as a multi-objective optimization problem. CURATING retains filters that (i) are very different (less redundant) from each other in terms of their representation (ii) have high saliency score i.e., they reduce the model accuracy drastically if pruned (iii) are likely to produce higher activations. We treat a filter specific to an output channel as a probability distribution over spatial filters to measure the similarity between filters. The similarity matrix is leveraged to create filter embeddings, and we constrain our optimization problem to retain a diverse set of filters based on these filter embeddings. On a range of CNNs over well-known datasets, CURATING exercises a better or comparable tradeoff between model size, accuracy, and inference latency than existing techniques. For example, while pruning VGG16 on the ILSVRC-12 dataset, CURATING achieves higher accuracy and a smaller model size than the previous techniques.
Keywords: CNN pruning; Saliency score; Hardware-efficient deep learning

Xiaocui Li, Zhangbing Zhou, Junqi Guo, Shangguang Wang, Junsheng Zhang,
Aggregated multi-attribute query processing in edge computing for industrial IoT applications,
Computer Networks,
Volume 151,
2019,
Pages 114-123,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.01.022.
(https://www.sciencedirect.com/science/article/pii/S1389128619301008)
Abstract: The popularity of smart things constructs sensing networks for the Internet of Things (IoT), and promotes intelligent decision-makings to support industrial IoT applications, where multi-attribute query processing is an essential ingredient. Considering the huge number of smart things and large-scale of the network, traditional query processing mechanisms may not be applicable, since they mostly depend on a centralized index tree structure. To remedy this issue, this article proposes a multi-attribute aggregation query mechanism in the context of edge computing, where an energy-aware IR-tree is constructed to process query processing in single edge networks, while an edge node routing graph is established to facilitate query processing for marginal smart things contained in contiguous edge networks. This decentralized and localized strategy has shown its efficiency and applicability of query processing in IoT sensing networks. Experimental evaluation results demonstrate that this technique performs better than the rivals in reducing the traffic and energy consumption of the network.
Keywords: Multi-attribute aggregation query; Energy-aware IR-tree; Edge node routing graph; Edge computing

Chunlin Li, Mingyang Song, Min Zhang, Youlong Luo,
Effective replica management for improving reliability and availability in edge-cloud computing environment,
Journal of Parallel and Distributed Computing,
Volume 143,
2020,
Pages 107-128,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.04.012.
(https://www.sciencedirect.com/science/article/pii/S0743731520302719)
Abstract: The multi-replica strategy can create multiple data replicas for the edge cloud system and store them in different DataNodes, which improves data availability and data service quality. However, the storage resources of DataNodes are limited and the user demand for data is time-varying, the unreasonable number of data replicas will cause a high storage burden on the file system or low data service quality. Therefore, the number of data replicas needs to be dynamically adjusted according to the actual situation. Based on this, a dynamic replica creation strategy based on the gray Markov chain is proposed. If the number of replicas needs to be increased, the newly added replicas need to be placed on the DataNodes. Considering the problem of load balancing of the DataNode during replica placement, this paper proposes a replica placement strategy based on the Fast Non-dominated Sorting Genetic algorithm. In addition, considering the problem of data replica synchronization and the data recovery of failed DataNodes in the edge cloud system, this paper proposes a delay-adaptive replica synchronization strategy and a load-balancing based replica recovery strategy. Finally, the experiments prove the effectiveness of the proposed strategies.
Keywords: Edge-cloud; Replica synchronization; Replica placement

Raafat O. Aburukba, Mazin AliKarrar, Taha Landolsi, Khaled El-Fakih,
Scheduling Internet of Things requests to minimize latency in hybrid Fog–Cloud​ computing,
Future Generation Computer Systems,
Volume 111,
2020,
Pages 539-551,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.039.
(https://www.sciencedirect.com/science/article/pii/S0167739X18303327)
Abstract: Delivering services for Internet of Things (IoT) applications that demand real-time and predictable latency is a challenge. Several IoT applications require stringent latency requirements due to the interaction between the IoT devices and the physical environment through sensing and actuation. The limited capabilities of IoT devices require applications to be integrated in Cloud and Fog computing paradigms. Fog computing significantly improves on the service latency as it brings resources closer to the edge. The characteristics of both Fog and Cloud computing will enable the integration and interoperation of a large number of IoT devices and services in different domains. This work models the scheduling of IoT service requests as an optimization problem using integer programming in order to minimize the overall service request latency. The scheduling problem by nature is NP-hard, and hence, exact optimization solutions are inadequate for large size problems. This work introduces a customized implementation of the genetic algorithm (GA) as a heuristic approach to schedule the IoT requests to achieve the objective of minimizing the overall latency. The GA is tested in a simulation environment that considers the dynamic nature of the environment. The performance of the GA is evaluated and compared to the performance of waited-fair queuing (WFQ), priority-strict queuing (PSQ), and round robin (RR) techniques. The results show that the overall latency for the proposed approach is 21.9% to 46.6% better than the other algorithms. The proposed approach also showed significant improvement in meeting the requests deadlines by up to 31%.
Keywords: Internet of Things; Cloud computing; Fog computing; Latency; Scheduling; Optimization; Genetic algorithm

Yinghao Guo, Zichao Zhao, Ke He, Shiwei Lai, Junjuan Xia, Lisheng Fan,
Efficient and flexible management for industrial Internet of Things: A federated learning approach,
Computer Networks,
Volume 192,
2021,
108122,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108122.
(https://www.sciencedirect.com/science/article/pii/S1389128621001961)
Abstract: In this paper, we devise an efficient and flexible management for mobile edge computing (MEC)-aided industrial Internet of Things (IIoT), from a federated learning approach. In the considered IIoT networks, all devices have some computational tasks to be computed with the help of some computational access points (CAPs). Although the performance of the IIoT networks can be optimized by using the resource allocation based on some centralized schemes, such solution is neither efficient nor flexible. To address this issue, we use a deep reinforcement learning (DRL) algorithm based federated learning algorithm to adjust three parameters: the task offloading ratio, bandwidth allocation ratio and transmit power. The optimization can minimize the normalized system cost, while reduce the communication cost in the optimization process. Moreover, simulation results are demonstrated to verify that the proposed federated framework can achieve an efficient and flexible management for the IIoT networks.
Keywords: IIoT; Efficient and flexible management; Federated learning; Mobile edge computing

Xiangcong Kong, Xiaoying Zheng, Yongxin Zhu, Gaoxiang Duan, Zikang Chen,
I/O-efficient GPU-based acceleration of coherent dedispersion for pulsar observation,
Journal of Systems Architecture,
Volume 142,
2023,
102958,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102958.
(https://www.sciencedirect.com/science/article/pii/S1383762123001376)
Abstract: Coherent dedispersion is a frequency domain filtering method extensively used in big data stream processing to achieve high time resolution and pulse-profile precision in pulsar observation. However, the computational complexity and challenges associated with processing large-scale data streams have hindered its widespread adoption in modern telescopes. Overcoming these hurdles is crucial to realizing wide-band real-time coherent dedispersion. In this paper, we propose a highly parallel and high-I/O throughput implementation of coherent dedispersion on GPU, specifically designed to support consecutive dispersion measure trials. Our primary objective is to achieve real-time coherent dedispersion, enabling precise high-time resolution and pulse-profile analysis in modern telescopes. To accomplish this, we leverage the power of GPU parallelism and employ advanced multi-threading techniques to optimize both computation and I/O throughput. Experiments show that our method achieves performance gains of 3.96x and 4.65x in processing time and IO throughput, respectively, when compared with the baseline work, the Coherent Dispersion Measure Trials (CDMT). Finally, the proposed techniques for improving computation parallelism and I/O efficiency are general that can provide valuable insights and serve as a reference for optimizing related algorithms in the field of big data stream processing.
Keywords: Pulsar searching; Coherent dedispersion algorithm; GPU; Disk I/O optimization

Yan Yan, Baoxian Zhang, Cheng Li,
A networked multi-agent reinforcement learning approach for cooperative FemtoCaching assisted wireless heterogeneous networks,
Computer Networks,
Volume 220,
2023,
109513,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109513.
(https://www.sciencedirect.com/science/article/pii/S1389128622005473)
Abstract: To meet the explosive growth of mobile traffic requirement in the 5th generation (5 G) mobile system, FemtoCaching at the network edge has been regarded as a promising technique for the 5 G mobile system. In this paper, we focus on studying the cooperative FemtoCaching problem in wireless heterogeneous networks (HetNets), which is aimed to minimize the overall fetching delays of all users. Owing to the NP-hardness of the problem, we formulate the cooperative FemtoCaching problem as a Networked Multi-Agent Reinforcement Learning (NMARL) problem and accordingly propose a Soft Attentional Networked Multi-Agent Actor-Critic (SAN-AC) Reinforcement Learning algorithm. Simulation results demonstrate that the proposed algorithm can significantly increase the overall performance compared with existing work.
Keywords: FemtoCaching; HetNets; MARL; and Graph Attention Networks (GAT)

Rui Wang, Yong Cao, Adeeb Noor, Thamer A Alamoudi, Redhwan Nour,
Agent-enabled task offloading in UAV-aided mobile edge computing,
Computer Communications,
Volume 149,
2020,
Pages 324-331,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.10.021.
(https://www.sciencedirect.com/science/article/pii/S0140366419306292)
Abstract: With the appearance of various mobile applications, such as automatic driving and augmented reality, it is difficult for the power and computing ability of mobile terminals to satisfy user demands. Therefore, an increasing number of terminal devices are requesting computing resources on the edge cloud. Because an unmanned aerial vehicle (UAV) is quite flexible and closer to the user side, an UAV can be adopted to assist mobile edge computing (MEC) while executing task offloading, which may reduce the pressure on edge clouds. However, it is unreasonable for users to make blind requests for resources due to the information asymmetry between a user and a service provider, and thus the quality of experience of user may be reduced. In this paper, an agent is introduced into the offloading of computing tasks, and a novel framework of agent-enabled task offloading in UAV-aided MEC(UMEC) is put forth to help the user, UAV, and edge cloud execute the offloading of computing tasks. With the intelligence and perceptibility of an agent, a system model is formulated in this paper to guide the agent in obtaining the optimum computing offloading plan, with minimum task execution delay and energy consumption. Simulation results showed that the introduction of an agent may significantly reduce delay and energy consumption, and the effectiveness of agent has been illustrated.
Keywords: Agent; Task offloading; MEC; UMEC

Alexandra Ferrerón, Jesús Alastruey-Benedé, Darío Suárez Gracia, Teresa Monreal Arnal, Pablo Ibáñez Marín, Víctor Viñals Yúfera,
A fault-tolerant last level cache for CMPs operating at ultra-low voltage,
Journal of Parallel and Distributed Computing,
Volume 125,
2019,
Pages 31-44,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.10.010.
(https://www.sciencedirect.com/science/article/pii/S0743731518307810)
Abstract: Voltage scaling to values near the threshold voltage is a promising technique to hold off the many-core power wall. However, as voltage decreases, some SRAM cells are unable to operate reliably and show a behavior consistent with a hard fault. Block disabling is a micro-architectural technique that allows low-voltage operation by deactivating faulty cache entries, at the expense of reducing the effective cache capacity. In the case of the last-level cache, this capacity reduction leads to an increase in off-chip memory accesses, diminishing the overall energy benefit of reducing the voltage supply. In this work, we exploit the reuse locality and the intrinsic redundancy of multi-level inclusive hierarchies to enhance the performance of block disabling with negligible cost. The proposed fault-aware last-level cache management policy maps critical blocks, those not present in private caches and with a higher probability of being reused, to active cache entries. Our evaluation shows that this fault-aware management results in up to 37.3% and 54.2% fewer misses per kilo instruction (MPKI) than block disabling for multiprogrammed and parallel workloads, respectively. This translates to performance enhancements of up to 13% and 34.6% for multiprogrammed and parallel workloads, respectively.
Keywords: Near-threshold voltage; SRAM reliability; Fault-tolerance; On-chip caches; Cache management

Jianguo Chen, Qingying Deng, Xulei Yang,
Non-cooperative game algorithms for computation offloading in mobile edge computing environments,
Journal of Parallel and Distributed Computing,
Volume 172,
2023,
Pages 18-31,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.10.004.
(https://www.sciencedirect.com/science/article/pii/S074373152200212X)
Abstract: Mobile Edge Computing (MEC) has become a promising technology for 5G networks. Computation offloading is an essential issue of MEC, which enables mobile User Equipment (UE) to enjoy rich wireless resources and huge computing power anywhere. This paper considers the Quality-of-Experience (QoE) of UEs in 5G MEC systems and presents a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm for computation offloading of MEC applications. We establish an MEC computation offloading model by considering the QoE requirements of UEs, and discuss the communication overheads, computation cost, and energy consumption models to minimize the energy consumption and time delay of each UE. Considering that there are multiple UEs who want to offload their computation tasks to a resource-constrained MEC server, and each UE is selfish and competitive, we formulate the problem of computation offloading decision as a non-cooperative game model. We prove the existence of a Nash Equilibrium (NE) solution for the proposed game model. In addition, we propose an algorithm that jointly optimizes energy consumption and time delay under QoE preferences to achieve optimal offloading benefits for each UE. Moreover, we respectively propose a dynamic non-cooperative game (QCOG-DG) algorithm and a static non-cooperative game (QCOG-SG) algorithm to efficiently find the NE solution. Extensive simulation experiments are conducted to verify the effectiveness of the proposed MEC computation offloading model and the QCOG-DG and QCOG-SG algorithms. Simulation results show that the proposed QCOG-DG algorithm can efficiently find the NE solutions in the MEC scenarios with UEs of different sizes.
Keywords: Computation offloading; Dynamic game; 5G networks; Mobile edge computing; Non-cooperative game

Wei Tong, Xuewen Dong, Yulong Shen, Xiaohong Jiang, Zhiwei Zhang,
A blockchain-driven data exchange model in multi-domain IoT with controllability and parallelity,
Future Generation Computer Systems,
Volume 135,
2022,
Pages 85-94,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.04.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001558)
Abstract: With the extensive application of the Internet of Things (IoT), multi-domain IoT appears as the additional coordination and control ability of IoT. However, the emerging multi-domain IoT suffers from some security challenges, such as insider threats and cross-domain data exchange issues. In this paper, we propose a data exchange model for the multi-domain IoT environment driven by blockchain, addressing expensive cross-domain access control and low data exchange throughput issues. At the core of this model is a chaincode-based cross-domain access control scheme and a domain-as-a-shard (DaaS) high parallel throughput optimization technology. In particular, the access control scheme is maintained by multiple blockchain nodes to ensure access control strategy distributed storage and data cross-domain controllability. The data in this model cannot be exchanged across domains until all nodes jointly verify that the access control strategy is valid. Moreover, the throughput optimization technology sets the IoT domain as the blockchain shard to process data exchange in parallel and scale up the throughput dramatically. In each shard, data exchange is processed in the form of the blockchain transaction independently to improve the transaction throughput of our model. Finally, security analysis proves that our model successfully ensures access control strategy non-repudiation and cross-domain data controllability. Extensive experiments on Hyperledger Fabric show that the transaction throughput of our model is nearly three times that of the original Fabric v1.4.
Keywords: Multi-domain IoT; Data exchange; Cross-domain access control; Throughput optimization; Domain-as-a-shard; Blockchain

Krzysztof Ostrowski, Krzysztof Małecki, Piotr Dziurzański, Amit Kumar Singh,
Mobility-aware fog computing in dynamic networks with mobile nodes: A survey,
Journal of Network and Computer Applications,
Volume 219,
2023,
103724,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103724.
(https://www.sciencedirect.com/science/article/pii/S1084804523001431)
Abstract: Fog computing is an evolving paradigm that addresses the latency-oriented performance and spatio-temporal issues of the cloud services by providing an extension to the cloud computing and storage services in the vicinity of the service requester. In dynamic networks, where both the mobile fog nodes and the end users exhibit time-varying characteristics, including dynamic network topology changes, there is a need of mobility-aware fog computing, which is very challenging due to various dynamisms, and yet systematically uncovered. This paper presents a comprehensive survey on the fog computing compliant with the OpenFog (IEEE 1934) standardised concept, where the mobility of fog nodes constitutes an integral part. A review of the state-of-the-art research in fog computing implemented with mobile nodes is conducted. The review includes the identification of several models of fog computing concept established on the principles of opportunistic networking, social communities, temporal networks, and vehicular ad-hoc networks. Relevant to these models, the contributing research studies are critically examined to provide an insight into the open issues and future research directions in mobile fog computing research.
Keywords: Mobile fog computing; Mobility-aware; Dynamic networks

Xu Zhao, Guangqiu Huang, Jin Jiang, Ling Gao, Maozhen Li,
Research on lightweight anomaly detection of multimedia traffic in edge computing,
Computers & Security,
Volume 111,
2021,
102463,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2021.102463.
(https://www.sciencedirect.com/science/article/pii/S016740482100287X)
Abstract: With the rapid development of the multimedia Internet of Things, a large amount of multimedia data is generated at the edge of the network. These multimedia data occupies most of the network traffic, and they have higher real-time transmission requirements, so they bring higher detection requirements to network security equipment. However, due to the limited processing capabilities of edge nodes, the high-performance distributed intrusion detection system (DIDS) in cloud computing cannot be used directly at the edge of the network. To solve this problem, a DIDS identification and detection method for multimedia traffic in an edge computing (EC) environment is proposed. Firstly, according to the characteristics of multimedia sensors, a multimedia traffic identification scheme combining protocol analysis and session analysis is established. Then, the improved and pruned C4.5 decision tree algorithm is used to identify the traffic which is not easily identified by the previous method. Finally, a special rule base and rule linked list are established in DIDS for targeted detection of multimedia traffic to achieve the goal of low calculation and high accuracy of the system. In addition, a DIDS multimedia detection optimization algorithm based on the M/M/n/m model is also proposed, which enables DIDS to automatically adjust the detection intensity of multimedia packets according to the length of the waiting queue. Firstly, the state flow diagram is established according to the Markov chain and birth-death process. Then, the multimedia detection method is improved by establishing the service dependent model. Finally, the operating cost is minimized by calculating the optimal number of detection engines in the DIDS. Experimental results show that the proposed scheme can help DIDS to identify and pertinently detect multimedia traffic with the lowest operating cost in a resource-constrained edge computing environment, while the safety of DIDS has not been reduced.
Keywords: Edge computing; Multimedia; Intrusion detection; C4.5; Queuing theory

Byomakesh Mahapatra, Ashok Kumar Turuk, Sarat Kumar Patra,
Multi-tier delay-aware load balancing strategy for 5G HC-RAN architecture,
Computer Communications,
Volume 187,
2022,
Pages 144-154,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.02.012.
(https://www.sciencedirect.com/science/article/pii/S0140366422000536)
Abstract: The fifth-generation (5G) cellular network aim to provide ultra-low latency and enhance data rate service to its associated users. The centralized baseband processing of incoming traffic increases the computational complexity and end-to-end delays in a conventional C-RAN architecture. This paper proposes a multi-tier HC-RAN architecture using the edge computing concept at the Remote Radio Head (RRH) to overcome the above limitations. Further, to reduce load at the C-BBU and fronthaul, we have proposed a multi-tier delay-aware load balancing (MDALB) algorithm. The proposed algorithm is run over the EO to strategically distribute incoming flow between eRRH and C-BBU based on a load distribution ratio. The efficiency of the proposed strategy is analyzed through mathematical models and simulations based on various performance matrices. The performance parameters’ results show that the use of multi-tier processing in HC-RAN minimizes end-to-end delay and packet loss whereas increases the system throughput and packet delivery ratio.
Keywords: BBU-pool; End-to-delay; HC-RAN; eRRH; Load balancing

Chunlin Li, YaPing Wang, Hengliang Tang, Yujiao Zhang, Yan Xin, Youlong Luo,
Flexible replica placement for enhancing the availability in edge computing environment,
Computer Communications,
Volume 146,
2019,
Pages 1-14,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.07.013.
(https://www.sciencedirect.com/science/article/pii/S0140366418308296)
Abstract: As edge devices that make up the Internet become more powerful, these devices in edge computing environment will even perform better in improving the computing and storage capabilities of system. In this paper, to enhancing data availability and improve cloud storage performance in edge computing environment, an adaptive replica placement strategy in edge computing environment is proposed to study the relationship among user access characteristics, the number of replicas, and the replicas location. To deal with the volatility of user access, the strategy in this paper takes data block as the data unit and the dynamic replica creation algorithm (DRC-GM) is proposed. DRC-GM uses the relationship between access frequency of data blocks and the number of replicas to dynamically adjust the number of replicas to ensure data availability requirements. To find the optimal location for replica distributed placement, in this paper, the replica placement algorithm (RP-FNSG) is proposed. And replica number, node performance and replica consistency are considered. Experiment results show that DRC-GM and RP-FNSG algorithms in edge computing environment can greatly improve system performance in terms of better prediction accuracy, shorter access response, higher effective network and storage space usage, which enhancing the data availability.
Keywords: Replica placement; Data availability; Edge computing environment

Sebastián Risco, Caterina Alarcón, Sergio Langarita, Miguel Caballer, Germán Moltó,
Rescheduling serverless workloads across the cloud-to-edge continuum,
Future Generation Computer Systems,
Volume 153,
2024,
Pages 457-466,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.12.015.
(https://www.sciencedirect.com/science/article/pii/S0167739X23004764)
Abstract: Serverless computing was a breakthrough in Cloud computing due to its high elasticity capabilities and fine-grained pay-per-use model offered by the main public Cloud providers. Meanwhile, open-source serverless platforms supporting the FaaS (Function as a Service) model allow users to take advantage of many of their benefits while operating on the on-premises platforms of organizations. This opens the possibility to deploy and exploit them on the different layers of the cloud-to-edge continuum, either on IoT (Internet of Things) devices located at the Edge (i.e. next to data acquisition devices), in on-premises clusters closer to the data sources (i.e. Fog computing) or directly on the Cloud. This paper presents two strategies to mitigate the overload that disparate data ingestion rates may cause in low-powered devices at the Edge or Fog layers. To this end, it is proposed to delegate and reschedule serverless jobs between the different layers of the cloud-to-edge continuum using an open-source platform for event-driven file processing. To demonstrate the performance of these strategies, a use case for fire detection is proposed that includes processing in the Fog via minified Kubernetes clusters located near the Edge, in the private Cloud via on-premises elastic clusters and, finally, in the public Cloud by using the AWS (Amazon Web Services) Lambda FaaS service. The results indicate that these strategies can mitigate overloads in use cases involving processing across the cloud-to-edge continuum by coordinating several layers of computing resources.
Keywords: Cloud computing; Cloud-to-edge continuum; Containers; FaaS; Kubernetes; Serverless computing

Wei Wei, Haoyi Li, Weidong Yang,
Cost-effective stochastic resource placement in edge clouds with horizontal and vertical sharing,
Future Generation Computer Systems,
Volume 138,
2023,
Pages 213-225,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.08.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002771)
Abstract: To support distributed upper-level services such as those in edge or fog computing, a hierarchical and distributed multicloud architecture is introduced to replace the traditional centralized cloud architecture. The intra-time slot demand fluctuation with corresponding multidimensional resource sharing are neglected for efficiency reasons in the existing algorithms. By considering them, the service provider has the potential to obtain better solution, but also makes the scheduling problem a highly complicated stochastic optimization one. To effectively and efficiently solve the problem containing a nonlinear objective and constraints with heterogeneous nonlinear pricing functions, the general stochastic scheduling problem is formulated and its structure is exploited to obtain the base solution quickly, which is further applied with solution closure for fast convergence in the following configurable metaheuristic searching phrase. The experiments with many different problem settings show its superiority over existing algorithms with more than 30% revenue improvement. Therefore, the proposed algorithm can be used as a good supplement to the existing methods.
Keywords: Hierarchical and distributed clouds; Resource placement; Stochastic demand; Edge computing; Cloud computing; Metaheuristic

Yiming Miao, Gaoxiang Wu, Miao Li, Ahmed Ghoneim, Mabrook Al-Rakhami, M. Shamim Hossain,
Intelligent task prediction and computation offloading based on mobile-edge cloud computing,
Future Generation Computer Systems,
Volume 102,
2020,
Pages 925-931,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.035.
(https://www.sciencedirect.com/science/article/pii/S0167739X19320862)
Abstract: Edge computing overcomes the high communication delay shortcoming of traditional cloud computing and provides computing services with high reliability and high bandwidth for mobile devices. At present, edge computing has become the forefront and hotspot of mobile-edge cloud computing (MEC) research. However, with the increasing requirements and services of mobile users, offloading strategy of simple edge computing is no longer applicable to MEC architecture. This paper puts forward a new intelligent computation offloading based MEC architecture in combination with artificial intelligence (AI) technology. According to the data size of computation task from mobile users and the performance features of edge computing nodes, a computation offloading and task migration algorithm based on task prediction is proposed. The computation task prediction based on LSTM algorithm, computation offloading strategy for mobile device based on task prediction, and task migration for edge cloud scheduling scheme are used to assist in optimizing the edge computing offloading model. Experiments show that our proposed architecture and algorithm can effectively reduce the total task delay with the increasing data and subtasks.
Keywords: Artificial intelligence; Computation offloading; Edge computing; LSTM; Task migration

Amin Shahraki, Torsten Ohlenforst, Felix Kreyß,
When machine learning meets Network Management and Orchestration in Edge-based networking paradigms,
Journal of Network and Computer Applications,
Volume 212,
2023,
103558,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103558.
(https://www.sciencedirect.com/science/article/pii/S1084804522001990)
Abstract: Caused by the rising of new network types, e.g., Internet of Things (IoT), within the last decade and related challenges like Big Data and data processing delay, new paradigms such as Edge and Fog computing emerged. Although these paradigms can partially address those challenges, their performance can still be affected by various issues, such as faults or network inefficiencies. To establish efficient network infrastructures for these paradigms, Network Management and Orchestration (NMO) techniques are introduced to improve various aspects of networking e.g., Quality of Service (QoS) provisioning, resource management, task allocation, and many others. Therefore, NMO primarily uses various methods like statistical models, heuristic techniques or Artificial Intelligence (AI) to automate networking decision-making. In this study, we investigate NMO issues, related orchestration challenges and the usage of Machine Learning (ML) techniques as a sub-field of AI for NMO purposes. The focus rests on new Edge-based networking and computing paradigms that employ resource-constraint devices to perform different tasks in environments like Extreme Edge, Cloud-of-Things (CoT) or Mist. We provide a comprehensive survey including a state-of-the-art review, research challenges and future directions. The study shows the challenges of NMO in such paradigms and provides information on how ML-based techniques can improve the performance of Edge-based networking paradigms.
Keywords: Network management; Machine learning; NTMA; Edge computing; Extreme edge; Fog computing; Deep learning; Deep reinforcement learning; Cloud computing

Qiufen Xia, Zheng Lou, Wenzheng Xu, Zichuan Xu,
Near-optimal and learning-driven task offloading in a 5G multi-cell mobile edge cloud,
Computer Networks,
Volume 176,
2020,
107276,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107276.
(https://www.sciencedirect.com/science/article/pii/S1389128620301699)
Abstract: With development well underway, 5G is envisioned as an enabler of lighting fast mobile services, such as virtual reality, augmented reality, live video analytics, and etc. In particular, multi-cell Mobile Edge Clouds (MEC) with 5G base stations endowed with computing capability are able to promote the Quality of Services (QoS) of mobile users by executing tasks in the edge cloud. Due to the varying 5G network conditions and limited computation capacity of each base station in the multi-cell MEC, as well as the stringent QoS requirements, a fundamental and challenging problem is how to offload user tasks to the edge cloud, such that the energy consumption of mobile devices is minimized. In this paper, we first formulate the offline and online location-aware mobile task offloading problems in a multi-cell MEC. For the offline location-aware mobile task offloading problem, we then develop an exact solution and an approximation algorithm with an approximation ratio. For the online problem, we thirdly propose a novel deep reinforcement learning-based offloading algorithm for mobile users to obtain the optimal offloading policy. We finally conduct extensive experiments by simulations to evaluate the proposed algorithms against existing benchmarks. The experimental results show that the proposed algorithms are promising and outperform the benchmark algorithms by significantly reducing energy cost of mobile devices and delays experienced by mobile users.
Keywords: Task offloading; Mobile edge computing; Approximation algorithm

Xin Jian, Langyun Wu, Keping Yu, Moayad Aloqaily, Jalel Ben-Othman,
Energy-efficient user association with load-balancing for cooperative IIoT network within B5G era,
Journal of Network and Computer Applications,
Volume 189,
2021,
103110,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103110.
(https://www.sciencedirect.com/science/article/pii/S1084804521001302)
Abstract: As one of the key technologies of 5G wireless communication technology, cooperative multi-access edge computing allows one device to associate multiple edge nodes simultaneously, namely multi-association, which can provide scalable communication services with characteristics of high reliability, massive connectivity and low latency for promising Industrial Internet of Things (IIoT). Effective association between edge nodes and devices is the prerequisite for providing high quality communication services in dense deployed IIoT networks. Most of state of art researches focus on the user association problem in single-association scenario. There are rarely no solutions presented for the considered user association problem with multi-association. In this paper, user association, power allocation and edge node deployment are jointly considered for load balance and energy efficiency under the multi-association mechanism. The problem is formulated as a nested knapsack optimization problem (NKOP) with energy efficiency and load balancing as objective functions and power and signal quality as constraints. Differential evolution with Monte Carlo and sequential quadratic programming (DMS) algorithm is proposed to solve this problem, which decouples the problem into three parts, user association, power allocation and optimizing the location of edge nodes. Numerical results show that: (1) Compared with the single-association, multi-association with power allocation can provide better signal quality and improve energy efficiency; (2) Proposed DMS algorithm is feasible and stable for optimal deployment of edge nodes. These works together provide good reference for edge node deployment of high-density IIoT application scenarios.
Keywords: 5G wireless technology; Industrial Internet of Things; Multi-access edge computing; Cooperative networks; Multi-association; Load balancing

Yuting Liang, Reza Samavi,
Optimization-based k-anonymity algorithms,
Computers & Security,
Volume 93,
2020,
101753,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2020.101753.
(https://www.sciencedirect.com/science/article/pii/S0167404820300377)
Abstract: In this paper we present a formulation of k-anonymity as a mathematical optimization problem. In solving this formulated problem, k-anonymity is achieved while maximizing the utility of the resulting dataset. Our formulation has the advantage of incorporating different weights for attributes in order to achieve customized utility to suit different research purposes. The resulting formulation is a Mixed Integer Linear Program (MILP), which is NP-complete in general. Recognizing the complexity of the problem, we propose two practical algorithms which can provide near-optimal utility. Our experimental evaluation confirms that our algorithms are scalable when used for datasets containing large numbers of records.
Keywords: Anonymization; Optimization; Privacy; Security; Mixed Integer Linear Program

Hugo Santos, Derian Alencar, Rodolfo Meneguette, Denis Rosário, Jéferson Nobre, Cristiano Both, Eduardo Cerqueira, Torsten Braun,
A multi-tier fog content orchestrator mechanism with quality of experience support,
Computer Networks,
Volume 177,
2020,
107288,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107288.
(https://www.sciencedirect.com/science/article/pii/S1389128620304448)
Abstract: Video-on-Demand (VoD) services create a demand for content orchestrator mechanisms to support Quality of Experience (QoE). Fog computing brings benefits for enhancing the QoE for VoD services by caching the content closer to the user in a multi-tier fog architecture, considering their available resources to improve QoE. In this context, it is mandatory to consider network, fog node, and user metrics to choose an appropriate fog node to distribute videos with QoE support properly. In this article, we introduce a content orchestrator mechanism, called of Fog4Video, which chooses an appropriate fog node to download video content. The mechanism considers the available bandwidth, delay, and cost, besides the QoE metrics for VoD, namely number of stalls and stalls duration, to deploy VoD services in the opportune fog node. Decision-making acknowledges periodical reports of QoE from the clients to assess the video streaming from each fog node. These values serve as inputs for a real-time Analytic Hierarchy Process method to compute the influence factor for each parameter and compute the QoE improvement potential of the fog node. Fog4Video is executed in fog nodes organized in multiple tiers, having different characteristics to provide VoD services. Simulation results demonstrate that Fog4Video transmits adapted videos with 30% higher QoE and reduced monetary cost up to 24% than other content request mechanisms.
Keywords: Content orchestrator; Multi-Tier fog computing; Quality of experience

Yang Wang, Yutong Li, Ting Wang, Gang Liu,
Towards an energy-efficient Data Center Network based on deep reinforcement learning,
Computer Networks,
Volume 210,
2022,
108939,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108939.
(https://www.sciencedirect.com/science/article/pii/S1389128622001220)
Abstract: Data Center Network (DCN) plays a crucial role in orchestrating the physical or virtual resources in data centers to meet the requirements of Internet of Things and Cloud Computing. The energy efficiency should be seriously considered for DCNs with large-scale switch devices which support numerous realtime network flow demands, especially for enormous flow demands from IoT devices. Typically, the network energy conservation can be achieved by optimizing routing and flow scheduling with energy awareness, targeting at powering off as many idle and low-loaded network devices as possible. For energy efficiency objective, in this paper we address a combinatorial optimization problem, named Multi-Commodity Flow (MCF) problem which optimizes the bandwidth allocation and routing to reduce the energy consumption. We propose a framework which has the lookahead ability of predicting flow demands in DCNs to dynamically feed the MCF problem as inputs. A Long Short-Term Memory (LSTM) network is exploited for flow demand prediction in DCNs and a Deep Reinforcement Learning (DRL) algorithm is tailored for solving the MCF problem. In experiments, we evaluate the predicted flow demands which simulate real flow demands and conduct a comparison between our DRL scheme with the baseline and optimizer to show the advantage of the DRL solution in optimality and efficiency.
Keywords: Data center network; Power conservation; Deep reinforcement learning

Yue Shen, Bowen Liu, Xiaoyu Xia, Lianyong Qi, Xiaolong Xu, Wanchun Dou,
A game theory-based COVID-19 close contact detecting method with edge computing collaboration,
Computer Communications,
Volume 207,
2023,
Pages 36-45,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.04.029.
(https://www.sciencedirect.com/science/article/pii/S0140366423001469)
Abstract: People all throughout the world have suffered from the COVID-19 pandemic. People can be infected after brief contact, so how to assess the risk of infection for everyone effectively is a tricky challenge. In view of this challenge, the combination of wireless networks with edge computing provides new possibilities for solving the COVID-19 prevention problem. With this observation, this paper proposed a game theory-based COVID-19 close contact detecting method with edge computing collaboration, named GCDM. The GCDM method is an efficient method for detecting COVID-19 close contact infection with users’ location information. With the help of edge computing’s feature, the GCDM can deal with the detecting requirements of computing and storage and relieve the user privacy problem. Technically, as the game reaches equilibrium, the GCDM method can maximize close contact detection completion rate while minimizing the latency and cost of the evaluation process in a decentralized manner. The GCDM is described in detail and the performance of GCDM is analyzed theoretically. Extensive experiments were conducted and experimental results demonstrate the superior performance of GCDM over other three representative methods through comprehensive analysis.
Keywords: Game theory; COVID-19; Edge computing; Decentralized method

Zhifei Wang, Xiangming Wen, Zhaoming Lu, Wenpeng Jing, Yujing Zhang,
Random access optimization for initial access and seamless handover for 5G-satellite network,
Computer Networks,
Volume 214,
2022,
109176,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109176.
(https://www.sciencedirect.com/science/article/pii/S1389128622002791)
Abstract: Satellite communication is becoming an indispensable component of the 5G global networks. Considering the unique characteristics of the satellite communications such as long round trip delay, large cell radius and high satellite speed, initial access and handover are necessary to be improved to guarantee the stable communication for 5G-satellite network, which usually use the random access procedure in traditional terrestrial communication and is not suitable for satellite scenario. Therefore, a 2-step random access for 5G-satellite network (TRAS) and a seamless MC-controlled handover for 5G-satellite network (SMHOS) are proposed in this paper. In particular, the access delay can be reduced and the success rate of access can be improved since the access procedure is simplified and the contention occurs only when the UEs select the same Physical Random Access Channel (PRACH) occasion and Physical Uplink Shared Channel (PUSCH) occasion in TRAS. Moreover, in SMHOS, the management center (MC) can predict the handover of the UE and inform the next satellite to send a broadcast message with embedded information about the reserved PUSCH time–frequency resource. The UE can access to the target satellite immediately when the handover occurs, therefore, seamless handover is realized. Simulation results demonstrate the effectiveness of our TRAS and SMHOS.
Keywords: 5G-satellite; Initial access; Seamless handover

Xiangjun Zhang, Weiguo Wu, Song Liu, Jinyu Wang,
An efficient computation offloading and resource allocation algorithm in RIS empowered MEC,
Computer Communications,
Volume 197,
2023,
Pages 113-123,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.10.012.
(https://www.sciencedirect.com/science/article/pii/S0140366422004005)
Abstract: Mobile edge computing (MEC) enables mobile devices (MDs) to offload computation-intensive tasks to edge servers to support a variety of latency-sensitive emerging applications (such as the Internet of Vehicles, real-time video analytics, etc.). However, the time-varying communication link environment of signal occlusion and interference between MDs and edge servers often leads to disappointing offloading benefits. Reconfigurable intelligent surface (RIS) is recognized as a promising technology in sixth-generation communication networks, with great potential to intelligently adjust the phase shift and amplitude of reflective elements to enhance wireless network capabilities. This paper proposes a novel computation offloading algorithm for RIS empowered MEC networks. Specifically, we comprehensively consider the optimization problems of delay, energy consumption, and operator cost in the process of computation offloading, and model it as a Markov decision process. To overcome the continuous action space challenge, we propose a computation offloading algorithm based on Deep Deterministic Policy Gradient (DDPG) to jointly optimize the phase shift and amplitude of RIS, offloading decision, and MEC resource allocation strategy. Finally, compared with various other benchmark algorithms, our proposed algorithm has a significant performance improvement over non-RIS learning algorithms and other classical algorithms, and maintains the optimal performance.
Keywords: Mobile edge computing; Reconfigurable intelligent surface; Computation offloading; Resource allocation; Deep Deterministic Policy Gradient; 6G

Juan Zhao, Xiaolong Xu, Wei-Ping Zhu,
Adaptive delay-constrained resource allocation in mobile edge computing for Internet of Things communications networks,
Computer Communications,
Volume 160,
2020,
Pages 607-613,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.06.031.
(https://www.sciencedirect.com/science/article/pii/S0140366420306770)
Abstract: The traffic burden at each node in Internet-of-Things (IoT) communication networks becomes prohibitively high especially when involving exhaustive computation. Mobile edge computing (MEC) makes this complicated computation feasible while alleviates the traffic burden by providing the corresponding node with powerful computing resources through wireless transmission between the node and the MEC for offloading computation. However, the transmission via the varying wireless channel requires considerable energy consumption and imposes delay. In this paper, we study the trade-off between the energy consumption and the delay performance in IoT network due to the offloading computation and the wireless communication. An optimization problem involving the offloading ratio for MEC as unknown parameter is established by minimizing the total energy consumption subject to a delay constraint. The problem is then solved by analyzing the convexity of the cost function and the constraint. Moreover, the scaling law of both energy cost and delay performance of IoT networks is investigated with respect to the number of nodes employing the MEC. It is discovered that the delay performance decreases in the logarithm with increasing the number of nodes while the energy cost grows linearly with the increase of the number of nodes. Numerical simulations verifying the performances of the proposed method in the studied IoT networks with MEC are provided.
Keywords: Mobile edge computing (MEC); Internet-of-Things (IoT); Computation offloading; Energy and delay trade-off; Adaptive resources allocation

Lejun Zhang, Yanfei Zou, Weizheng Wang, Zilong Jin, Yansen Su, Huiling Chen,
Resource allocation and trust computing for blockchain-enabled edge computing system,
Computers & Security,
Volume 105,
2021,
102249,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2021.102249.
(https://www.sciencedirect.com/science/article/pii/S0167404821000730)
Abstract: In order to meet various needs of people, different Internet of Things (IoT) devices have been developed and applied successfully in recent years. However, the consequent challenges in terms of search efficiency, reliable requirements, and resource allocation appear followed, which attract attention from both academia and industry. Facing this circumstance, it is necessary to establish a new scheme to realize data processing and sharing better. Therefore, a reliable and efficient system based on edge computing and blockchain is proposed in this paper. First, a new group-agent strategy with trust computing is designed to ensure the reliability of edge devices during interactions and improve transmission efficiency. Second, we introduce a stacked task sorting and ranking mechanism which improves resource allocation in each edge device. Third, this paper creates a new content model that uses Zipf distribution to predict context popularity of keywords and encrypt hot data with symmetric searchable encryption (SSE) technology. Finally, simulation results show that the proposed scheme has better computational efficiency and higher reliability compared with existing methods.
Keywords: Edge computing; Blockchain; Trust computing; Resource allocation; Content model

Bertrand Le Gal, Christophe Jego, Vincent Pignoly,
High-performance hard-input LDPC decoding on multi-core devices for optical space links,
Journal of Systems Architecture,
Volume 137,
2023,
102832,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102832.
(https://www.sciencedirect.com/science/article/pii/S1383762123000115)
Abstract: LDPC codes are a family of error-correcting codes that are present in most space communication standards. Thanks to their large processing power and their parallelization capabilities, prevailing multicore devices facilitate real-time implementations of digital communication systems, which were previously implemented into dedicated hardware devices. Previous works were done over the last decade on the implementation of Gbps decoders on programmable devices. However, these works focus on the soft input LDPC decoding algorithms. But, hard-input LDPC decoders are also required to design and prototype for instance next optical-based satellite communication systems. These systems should provide high throughput (≥10 Gbps) and low latency (≤1ms) internet links. In this article, the first software-based implementation of a hard-input multi-Gbps LDPC decoder is detailed. Thanks to different parallelization strategies and deeply optimized SIMD codes, throughput up to 7.5 Gbps is achieved when 10 Gallager-E iterations are executed onto an INTEL Xeon device making possible the design of software base station systems providing throughputs of tens of Gbps for optical system evaluation or base station design.
Keywords: LDPC; Gallager-E; Multicore; SIMD; Optical space links

Abu Sufian, Anirudha Ghosh, Ali Safaa Sadiq, Florentin Smarandache,
A Survey on Deep Transfer Learning to Edge Computing for Mitigating the COVID-19 Pandemic,
Journal of Systems Architecture,
Volume 108,
2020,
101830,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101830.
(https://www.sciencedirect.com/science/article/pii/S1383762120301223)
Abstract: Global Health sometimes faces pandemics as are currently facing COVID-19 disease. The spreading and infection factors of this disease are very high. A huge number of people from most of the countries are infected within six months from its first report of appearance and it keeps spreading. The required systems are not ready up to some stages for any pandemic; therefore, mitigation with existing capacity becomes necessary. On the other hand, modern-era largely depends on Artificial Intelligence(AI) including Data Science; and Deep Learning(DL) is one of the current flag-bearer of these techniques. It could use to mitigate COVID-19 like pandemics in terms of stop spread, diagnosis of the disease, drug & vaccine discovery, treatment, patient care, and many more. But this DL requires large datasets as well as powerful computing resources. A shortage of reliable datasets of a running pandemic is a common phenomenon. So, Deep Transfer Learning(DTL) would be effective as it learns from one task and could work on another task. In addition, Edge Devices(ED) such as IoT, Webcam, Drone, Intelligent Medical Equipment, Robot, etc. are very useful in a pandemic situation. These types of equipment make the infrastructures sophisticated and automated which helps to cope with an outbreak. But these are equipped with low computing resources, so, applying DL is also a bit challenging; therefore, DTL also would be effective there. This article scholarly studies the potentiality and challenges of these issues. It has described relevant technical backgrounds and reviews of the related recent state-of-the-art. This article also draws a pipeline of DTL over Edge Computing as a future scope to assist the mitigation of any pandemic.
Keywords: AI for Good; COVID-19; Deep Learning; Edge Computing; Pandemic; Review; Transfer Learning

Michele Girolami, Piergiorgio Vitello, Andrea Capponi, Claudio Fiandrino, Luca Foschini, Paolo Bellavista,
A mobility-based deployment strategy for edge data centers,
Journal of Parallel and Distributed Computing,
Volume 164,
2022,
Pages 133-141,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.03.007.
(https://www.sciencedirect.com/science/article/pii/S0743731522000600)
Abstract: The main objective of Multi-access Edge Computing (MEC) is to bring computational capabilities at the edge of the network to better support low-latency applications. Such capabilities are typically offered by Edge Data Centers (EDC). The MEC paradigm is not tied to a single radio technology, rather it embraces both cellular and other radio access technologies such as WiFi. Distributed intelligence at the edge for AI purposes requires careful spatial planning of computing and storage resources. The problem of EDC deployment in urban environments is challenging and, to the best of our knowledge, it has been explored only for cellular connectivity so far. In this paper, we study the possibility of deploying EDC without analyzing the expected data traffic load of the cellular network, a kind of information rarely shared by network operators. To this purpose, we propose in this work CLUB, CLUstering-Based strategy tailored on the analysis of urban mobility. We analyze two experimental mobility data sets, and we analyze some mobility features in order to characterize their properties. Finally, we compare the performance of CLUB against state-of-the-art techniques in terms of the outage probability, namely the probability an EDC is not able to serve a request. Our results show that the CLUB strategy is always comparable with respect to our benchmarks, but without using any information related to network traffic.
Keywords: Edge data center; Multi-access edge computing; Mobility; Mobile CrowdSensing

Xianglong Li, Kaiwei Mo, Yeqiao Hou, Zongpeng Li, Hong Xu, Chun Jason Xue,
An auction approach to aircraft bandwidth scheduling in non-terrestrial networks,
Computer Networks,
Volume 247,
2024,
110424,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110424.
(https://www.sciencedirect.com/science/article/pii/S1389128624002561)
Abstract: Internet Service has witnessed only limited deployment on commercial flights, where network infrastructure is lacking and subscription fees are high. It is natural to design an efficient scheduling strategy for Internet service access on flights, while maximizing social welfare. There are challenges to use either antennas on the ground or geostationary (GEO) satellites in space as Internet Service Provider (ISP) equipment. First, preemptive-purchase and resource-allocation in post-paid plans may not satisfy the dynamic demand for large amounts of flight-ISP machine links. Second, finding an optimal, dynamic, online solution based on flight demands is NP-hard. Third, existing solutions often fail to maximize either seller profit or social welfare. To address the aforementioned challenges, we formulate an online bandwidth scheduling integer linear program (ILP) among flights, antennas, and GEO satellites in a Non-Terrestrial Network (NTN). We transform the ILP into an exponential optimization problem, which is then relaxed, with its dual formulated. We iteratively solve a series of dual subproblems in polynomial time, towards social welfare maximization with a good competitive ratio in typical network settings. In empirical studies, our algorithm achieved a much better competitive ratio than the theoretical worst-case guarantee, and clearly outperforms alternative algorithms.
Keywords: Auction; Bandwidth; Optimizing; Scheduling; Air-to-ground; Non-terrestrial network

Eslam Hussein, Bernd Waschneck, Christian Mayr,
Automating application-driven customization of ASIPs: A survey,
Journal of Systems Architecture,
Volume 148,
2024,
103080,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103080.
(https://www.sciencedirect.com/science/article/pii/S1383762124000171)
Abstract: The rapid advancements and stringent requirements of modern embedded computing systems have led to a surge in the demand for customized processors that can efficiently cater to specific application needs. This survey paper delves into the realm of automating application-driven customization of extensible processors, offering insights into the challenges, advancements, and trends in this domain. It explores the trade-offs between fine-grained and coarse-grained customization, discussing Custom Instructions (CIs) identification and optimization techniques, while emphasizing the shift towards larger accelerators that target complex control sequences in the application. It scrutinizes the balance between speedup and reusability, addressing the challenges of efficient design approaches to manage area and power consumption. The agile nature of early Design Space Exploration (DSE) is discussed, where rapid evaluation of area and communication costs plays a pivotal role. In essence, this survey serves as a valuable guide for researchers and practitioners in the field of processor customization, aiding designers in navigating this complex landscape to optimize performance in a rapidly evolving computing paradigm.
Keywords: Extensible processors; RISC; ISA; Custom instructions; Custom accelerators; Application-specific functional units; Resource sharing; Hardware/software codesign; Design space exploration

Mostafa Rezazad, Y.C. Tay,
Decoupling NDN caches via CCndnS: Design, analysis, and application,
Computer Communications,
Volume 151,
2020,
Pages 338-354,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.12.053.
(https://www.sciencedirect.com/science/article/pii/S0140366418307370)
Abstract: In-network caching is considered to be a vital part of the Internet for future applications (e.g., Internet of Things). One proposal that has attracted interest in recent years, Named Data Networking (NDN), aims to facilitate in-network caching by locating content by name. However, the efficiency of in-network caching has been questioned by experts. Data correlation among caches builds strong dependencies between caches at the edge and in the core. That dependency makes analyzing network performance difficult. This paper proposes CCndnS (Content Caching strategy for NDN with Skip), a caching policy to break the dependencies among caches, thus facilitating the design of an efficient data placement algorithm. Specifically, each cache – regardless of its location in the network – should receive an independent set of requests; otherwise, only misses from downstream caches make their way to the upstream caches, i.e. a filtering effect that induces a correlation among the caches. CCndnS breaks a file into smaller segments and spreads them in the path between requester and publisher in a way that the head of the file (the first segment) should be cached at the edge router close to the requester and the tail far from the requester and towards the content provider. Requests for a segment skip searching caches in its path, to search only the cache with the segment of interest. That reduces the number of futile checks on caches, and thus the delay from memory accesses. This mechanism also decouples the caches, so there is a simple analytical model for cache performance in the network. We illustrate an application of the model to enforce a Service Level Agreement (SLA) between a content provider and the caching system proposed in this paper. The model can be used for cache provisioning for two purposes: (1) To specify the cache size to be reserved for specific contents to reach some desired performance. For instance, if the client of an SLA requires a 50% cache hit for its content at each router, the model can be used to determine the cache size that needs to be reserved to reach the 50% hit rate. (2) To calculate the effect of such reservations on other contents that use the routers covered by the SLA. The design, analysis, and application are tested with extensive simulations.
Keywords: Information-centric networking; Named Data Networking; Network of caches; Cache policy; Performance modeling; Service Level Agreement

Miłosz Ciżnicki, Krzysztof Kurowski, Jan Wȩglarz,
Energy and performance improvements in stencil computations on multi-node HPC systems with different network and communication topologies,
Future Generation Computer Systems,
Volume 115,
2021,
Pages 45-58,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.08.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X19326433)
Abstract: Energy and performance improvements in stencil computations are relevant for both application developers and data center administrators. They appear as the fundamental scheme in many large-scale scientific simulations and workloads. Many research efforts have focused on some estimation techniques of the energy usage of HPC systems based on specific characteristics of parallel applications. In case of stencils, we have previously concentrated on detailed estimations of energy consumption and the energy-aware distribution of stencil computations on heterogeneous processors. However, we have restricted our comprehensive studies to a single heterogeneous computing node only. In this paper, we show how scheduling and optimization techniques can be applied for energy and performance improvements of stencil computations on multi-node HPC systems using different network topologies. We formulate a scheduling model together with a new Tabu Search algorithm, called Task Movement (TM), taking into account the communication hierarchies, to minimize the overall energy usage and the execution time of stencil computations. Experimental studies show that this algorithm solves the considered problem more efficiently comparing to other, simpler heuristics. We present computational experiments for a reference 7 point stencil computation pattern on three commonly used low-diameter network topologies: Fat-tree, Dragonfly, and Torus. According to our studies, the most promising multi-node HPC architecture for stencil computations is based on the Torus network concept. Finally, we argue that the proposed scheduling model and TM algorithm can be easily adopted within existing high-level parallel execution environments for stencils automatic performance tuning.
Keywords: Stencil computations; Performance analysis; Topology-aware scheduling; Energy modeling; GPUs; HPC

Chunlin Li, Chengyi Wang, Hengliang Tang, Youlong Luo,
Scalable and dynamic replica consistency maintenance for edge-cloud system,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 590-604,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.05.014.
(https://www.sciencedirect.com/science/article/pii/S0167739X18331558)
Abstract: With the increasing number of edge devices in the Internet of things (IoT), data storage in edge cloud environment better meet the real-time requirements of IoT. In order to improve the data reliability, reduce the overhead and the response latency, and make more reasonable use of storage resources in edge cloud environment, we conduct an in-depth study on the replica creation strategy and consistency maintenance strategy. In order to reduce the waste of storage resources, a dynamic replica creation strategy based on block popularity and node load (DRC-BN) is proposed. DRC-BN strategy considers the storage cost of edge cloud and the communication cost between central cloud and edge cloud when optimizing the number of replicas, and improves the data reliability. In order to solve the problem of data inconsistency caused by multi-user concurrency, a replica consistency maintenance strategy based on Fast Paxos algorithm (RC-FP) is proposed. RC-FP strategy considers the comprehensive performance of nodes and selects the node with better performance as the leader according to the comprehensive performance of nodes, and reduces the time of consistency maintenance. Experimental results show that the proposed strategies perform well in terms of the consistency maintenance time and the node throughput. At the same time, the proposed strategies also reduce the overall overhead.
Keywords: Edge-cloud; Consistency maintenance; Latency-sensitive application

Hala Ajmi, Fakhreddine Zayer, Amira Hadj Fredj, Hamdi Belgacem, Baker Mohammad, Naoufel Werghi, Jorge Dias,
Efficient and lightweight in-memory computing architecture for hardware security,
Journal of Parallel and Distributed Computing,
Volume 190,
2024,
104898,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2024.104898.
(https://www.sciencedirect.com/science/article/pii/S0743731524000625)
Abstract: This paper introduces an innovative solution for improving the efficiency and speed of the Advanced Encryption Standard (AES) based cryptographic algorithm. The approach leverages in-memory computing (IMC) and is versatile for application across a broad spectrum of IoT applications, including robotic autonomous vehicles and various other scenarios. To achieve this goal, memristor (MR) designs are proposed to emulate the arithmetic operations required for different phases of the AES algorithm, enabling efficient in-memory processing. The key contributions of this work include; 1) The development of a 4 bit-MR state element for implementing different arithmetic operations in an AES hardware prototype; 2) The creation of a pipeline AES design for massive parallelism and MR integration compatibility; and 3) The hardware implementation of the AES-IMC based architecture using the MR emulator. The results show that AES-IMC performs better than existing architectures in terms of higher throughput and energy efficiency. Compared to conventional AES hardware, AES-IMC achieves a 30% power enhancement with comparable throughput. Additionally, when compared to state-of-the-art AES-based NVM engines, AES-IMC demonstrates comparable power dissipation and a 62% increase in throughput. The IMC architecture enables cost-effective real-time deployment of AES, leading to high-performance computing. By leveraging the power of in-memory computing, this system is able to provide improved computational efficiency and faster processing speeds, making it a promising solution for a wide range of applications in the field of autonomous driving and robotics. The potential benefits of this system include improved safety and security of unmanned devices, as well as enhanced performance and cost-effectiveness in a variety of computing environments.
Keywords: AES Algorithm; Hardware Security; Memristor Technology; In-memory Computing; Memristor Hardware

Jiayi Liu, Wenbin Yao, Chen Wang, Qinghai Yang,
Provisioning network slice for mobile content delivery in uncertain MEC environment,
Computer Networks,
Volume 224,
2023,
109629,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109629.
(https://www.sciencedirect.com/science/article/pii/S1389128623000749)
Abstract: Network Slicing (NS) and Mobile Edge Computing (MEC) are recognized as a promising paradigm for mobile Content Delivery Network (CDN) by instantiating Virtual Network Functions (VNFs) on MEC platforms. Mobile network exhibits intrinsic dynamics, where user requests patterns vary in time and space due to human behaviors. The requirements and distribution of users are arbitrary spatio-temporal stochastic variables, which makes the deployment and reconfiguration of a CDN slice a challenging task. In this work, by adopting a periodical reconfiguration strategy, we consider a CDN slice system which operates in discrete time intervals. Within each time interval, we formalize the CDN-MEC-VNFs Planning (CMVP) problem based on uncertain programming to tackle the dynamic and uncertain mobile network environment, where user requests are treated as random variables with arbitrary pattern. Then, we propose a data-driven and learning-based algorithm which integrates Stochastic Simulation (SS), Deep Neural Networks (DNN) and meta-heuristic algorithm to determine the provisioning of the CDN slice from historical user requests information. The practicability of the proposed mechanism in offline training and online running are also discussed. Finally, we conducted intensive real-trace driven simulations to demonstrate the effectiveness of our approach on planning CDN slice with higher QoS under dynamic system environment and arbitrary user requests by comparing towards several baselines.
Keywords: Learning-based network optimization; Network slicing; Content Delivery Network; Mobile Edge Computing; Uncertain programming

Hao Li, Weimin Lei, Wei Zhang, Yunchong Guan,
A joint optimization method of coding and transmission for conversational HD video service,
Computer Communications,
Volume 145,
2019,
Pages 243-262,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.07.001.
(https://www.sciencedirect.com/science/article/pii/S014036641930088X)
Abstract: Multipath transmission is considered to be a promising approach for supporting data delivery of conversational high-definition (HD) video service as it is able to aggregate more transmission resources, increase the reliability of data transmission, and thereby improve the user experience of service. However, most research achievements of multipath video transmission have mainly concentrated on the efficiency of video delivery while less attention has been paid to the dynamic video bit rate adjustment which is also important to improving the quality of such video service. To take full advantage of multipath video transmission, this paper presents a joint optimization method for conversational HD video service, taking into account the linkage between video coding and transmission. In the aspect of the video coding, a region rate based perceptual coding scheme is applied by the combination usage of the characteristics of such video service, which aims to adapt the output bit rate to match the dynamic network conditions. The scheme allocates different coding frequency or rate for different regions in a video frame according to their perceptual importance dynamically, and then skips some unimportant regions and encodes the key regions. Moreover, a multipath load distribution method is adopted to optimize the transmission process which schedules the coded frames over multiple paths by employing the capacity-limited water filling (CLWF) algorithm, to meet the stringent delay requirements of conversational HD video service. Furthermore, the transmission quality of each path is assessed and the coding parameters are adjusted periodically according to the data transmission status and feedback messages. Experiments are carried out with a simulation in OMNeT++ and the results demonstrate that the proposed method performs much better than the existing schemes in terms of data transmission and playback quality.
Keywords: Conversational video service; Joint optimization; Multipath transmission; Rate control

Li Yu, Zongpeng Li, Jiangchuan Liu, Ruiting Zhou,
Online and energy-efficient task-processing for distributed edge networks,
Computer Networks,
Volume 193,
2021,
107875,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107875.
(https://www.sciencedirect.com/science/article/pii/S138912862100044X)
Abstract: User equipment produces a series of tasks that are processed locally or remotely, falling into three categories: (i) local computing only, (ii) a fraction of the task is computed locally and the remaining task unprocessed is offloaded for remote computation, and (iii) the entire task is offloaded. Each case has attracted substantial attention in recent studies, where a delay-constrained non-linear optimization problem is often formulated. The solutions employed are either based on Lagrange duality, heuristic search, or dynamic programming. To our knowledge, there is no unifying task-processing orchestrator that is an online tailored solver for learning the model-free problems, encapsulating the three cases above. We fill this gap and present the first attempt on an innovative actor-critic reinforcement learning approach in consideration of the energy-efficiency, to compute the asymptotically optimal solutions via decomposing the comprehensive optimization into sub-problems. Rigorous theoretical analyses and experience-driven simulations demonstrate significant advantages over the benchmark approaches, in terms of task-processing delay, power efficiency, and convergence time.
Keywords: Online learning; Internet of Things; Task offloading; Energy efficiency; Mobile edge computing

Pablo Gimeno Sarroca, Marc Sánchez-Artigas,
MLLess: Achieving cost efficiency in serverless machine learning training,
Journal of Parallel and Distributed Computing,
Volume 183,
2024,
104764,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104764.
(https://www.sciencedirect.com/science/article/pii/S074373152300134X)
Abstract: Function-as-a-Service (FaaS) has raised a growing interest in how to “tame” serverless computing to enable domain-specific use cases such as data-intensive applications and machine learning (ML), to name a few. Recently, several systems have been implemented for training ML models. Certainly, these research articles are significant steps in the correct direction. However, they do not completely answer the nagging question of when serverless ML training can be more cost-effective compared to traditional “serverful” computing. To help in this endeavor, we propose MLLess, a FaaS-based ML training prototype built atop IBM Cloud Functions. To boost cost-efficiency, MLLess implements two innovative optimizations tailored to the traits of serverless computing: on one hand, a significance filter, to make indirect communication more effective, and on the other hand, a scale-in auto-tuner, to reduce cost by benefiting from the FaaS sub-second billing model (often per 100 ms). Our results certify that MLLess can be 15X faster than serverful ML systems [27] at a lower cost for sparse ML models that exhibit fast convergence such as sparse logistic regression and matrix factorization. Furthermore, our results show that MLLess can easily scale out to increasingly large fleets of serverless workers.
Keywords: Serverless computing; Function-as-a-Service; Machine learning

Congfeng Jiang, Tiantian Fan, Honghao Gao, Weisong Shi, Liangkai Liu, Christophe Cérin, Jian Wan,
Energy aware edge computing: A survey,
Computer Communications,
Volume 151,
2020,
Pages 556-580,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.004.
(https://www.sciencedirect.com/science/article/pii/S014036641930831X)
Abstract: Edge computing is an emerging paradigm for the increasing computing and networking demands from end devices to smart things. Edge computing allows the computation to be offloaded from the cloud data centers to the network edge and edge nodes for lower latency, security and privacy preservation. Although energy efficiency in cloud data centers has been broadly investigated, energy efficiency in edge computing is largely left uninvestigated due to the complicated interactions between edge devices, edge servers, and cloud data centers. In order to achieve energy efficiency in edge computing, a systematic review on energy efficiency of edge devices, edge servers, and cloud data centers is required. In this paper, we survey the state-of-the-art research work on energy-aware edge computing, and identify related research challenges and directions, including architecture, operating system, middleware, applications services, and computation offloading.
Keywords: Edge computing; Energy efficiency; Computing offloading; Benchmarking; Computation partitioning

Shiyou Chen, Lanlan Rui, Zhipeng Gao, Yang Yang, Xuesong Qiu, Shaoyong Guo,
Resource sharing for collaborative edge learning: A privacy-aware incentive mechanism combined with demand prediction,
Computer Networks,
Volume 243,
2024,
110302,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110302.
(https://www.sciencedirect.com/science/article/pii/S1389128624001348)
Abstract: Edge intelligence benefits from ubiquitous resource capabilities by numerous edge devices. However, interest concerns and privacy issues caused by the openness of the edge network make smart devices reluctant to participate in collaborative edge learning. This work proposes a privacy-aware incentive mechanism based on combinatorial double auction (PIMCDA) to facilitate idle resource sharing of diversified edge devices. First, we establish an auction model based on blockchain to form a trusted sharing market in a multi-resource binding manner. Then, we propose a two-stage auction solution combined with differential privacy: The privacy-aware winner selection stage matches the optimal winner candidate to the task that is the closest in probability to maximize the revenue of resource providers with the obfuscated location information. The probabilistic pricing decision stage is designed based on a uniform pricing method and exponential mechanism, which can ensure bid privacy from inference attacks. Furthermore, to ensure the effectiveness of PIMCDA for long-term participation, we design a supply and demand balance mechanism with a learning-based resource prediction method. Theoretical and simulation analysis demonstrate that the proposed mechanism achieves privacy-preserving in location and bid information while ensuring effective incentive properties. Our approach effectively motivates the sharing of multiple resources in edge computing.
Keywords: Edge computing; Combinatorial double auction; Differential privacy; Supply–demand balance

Zhao Yang, Shengbing Zhang, Chuxi Li, Miao Wang, Haoyang Wang, Meng Zhang,
Efficient knowledge management for heterogeneous federated continual learning on resource-constrained edge devices,
Future Generation Computer Systems,
Volume 156,
2024,
Pages 16-29,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.02.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000633)
Abstract: Federated learning (FL) is a promising and privacy-preserving distributed learning method that is widely deployed on edge devices. However, in practical applications, the data collected by edge devices exhibits temporal variations. This leads to the problem of FL models adapting to new data while forgetting knowledge from old data, resulting in a catastrophic forgetting issue. Continual learning methods can be used to address this problem. However, when deploying these methods in FL on edge devices, it is challenging to adapt to the limited resources and heterogeneous data of the deployed devices, which reduces the efficiency and effectiveness of federated continual learning (FCL). Therefore, this article proposes a resource-efficient heterogeneous FCL framework. This framework divides the global model into an adaptation part for new knowledge and a preservation part for old knowledge. The preservation part is used to address the catastrophic forgetting problem. Only the adaptation part is trained when learning new knowledge on a new task, reducing resource consumption. Additionally, the framework mitigates the impact of heterogeneous data through an aggregation method based on feature representation. Experimental results show that our method performs well in mitigating catastrophic forgetting in a resource-efficient manner.
Keywords: Federated learning; Catastrophic forgetting; Continual learning; Resource-efficient; Edge device

Francesco Valente, Vincenzo Eramo, Francesco G. Lavacca,
Optimal bandwidth and computing resource allocation in low earth orbit satellite constellation for earth observation applications,
Computer Networks,
Volume 232,
2023,
109849,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109849.
(https://www.sciencedirect.com/science/article/pii/S1389128623002943)
Abstract: The next step in Earth Observation (EO) constellations will be leveraging Inter-Satellite Links (ISLs) to form a network where information generated by the EO application can be transmitted, in such a way that, by endowing spacecrafts with processing capacity, observation data may be processed directly in orbit by any satellite of the constellation. However, since bandwidth and on-board processing capacity are valuable resources, strategies to appropriately routing the information and deciding on which node it has to be processed shall be defined. In this work, we formalize and solve an optimal bandwidth and computing resource allocation problem in Low Earth Orbit (LEO) satellite constellation for EO applications. In order to deal with the complexity of the proposed optimization problem, we also present two heuristics requiring different computational effort. In the proposed problem formalization, processing can happen on any node of the network (i.e., either on the data source satellite, on any other satellite of the constellation or on ground station). After having validated the proposed heuristics by comparing their results to the optimization problem ones, we apply them to a real orbital scenario, showing their ability to reduce both total cost and data delivery delay to ground with respect to state-of-the-art solutions.
Keywords: Orbital edge computing; Earth observation; Inter-satellite networks; Satellite constellations

Mohammed Laroui, Boubakr Nour, Hassine Moungla, Moussa A. Cherif, Hossam Afifi, Mohsen Guizani,
Edge and fog computing for IoT: A survey on current research activities & future directions,
Computer Communications,
Volume 180,
2021,
Pages 210-231,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.09.003.
(https://www.sciencedirect.com/science/article/pii/S0140366421003327)
Abstract: The Internet of Things (IoT) allows communication between devices, things, and any digital assets that send and receive data over a network without requiring interaction with a human. The main characteristic of IoT is the enormous quantity of data created by end-user’s devices that needs to be processed in a short time in the cloud. The current cloud-computing concept is not efficient to analyze very large data in a very short time and satisfy the users’ requirements. Analyzing the enormous quantity of data by the cloud will take a lot of time, which affects the quality of service (QoS) and negatively influences the IoT applications and the overall network performance. To overcome such challenges, a new architecture called edge computing — that allows to decentralize the process of data from the cloud to the network edge has been proposed to solve the problems occurred by using the cloud computing approach. Furthermore, edge computing supports IoT applications that require a short response time and consequently enhances the consumption of energy, resource utilization, etc. Motivated by the extensive research efforts in the edge computing and IoT applications, in this paper, we present a comprehensive review of edge and fog computing research in the IoT. We investigate the role of cloud, fog, and edge computing in the IoT environment. Subsequently, we cover in detail, different IoT use cases with edge and fog computing, the task scheduling in edge computing, the merger of software-defined networks (SDN) and network function virtualization (NFV) with edge computing, security and privacy efforts. Furthermore, the Blockchain in edge computing. Finally, we identify open research challenges and highlight future research directions.
Keywords: Internet of Things (IoT); Edge computing; Cloud computing

Faria Khandaker, Sharief Oteafy, Hossam S. Hassanein, Hesham Farahat,
A functional taxonomy of caching schemes: Towards guided designs in information-centric networks,
Computer Networks,
Volume 165,
2019,
106937,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106937.
(https://www.sciencedirect.com/science/article/pii/S1389128619301616)
Abstract: Information Centric Networking (ICN) is a developing paradigm, poised to transform the Internet's architecture. At its core, ICN focuses on efficient content dissemination and retrieval, regardless of storage location and physical representation of content. Thus, content caching schemes play a pivotal role in providing fast, reliable, and scalable content distribution and delivery. Given the multitude of caching schemes that have evolved over the past few years, recent developments have been often hampered by a fixed set of design primitives. In this paper, we present a functional-based taxonomy of ICN caching schemes, detailing each functional component, and depicting the functional mandates of these schemes to aid in contrasting their operations. The goal of this survey is to guide the design and development of ICN protocols, building on insights from caching schemes and their inherent tradeoffs. We present a comprehensive benchmark for future caching schemes, coupled with a quantitative as well as qualitative analysis of leading caching schemes, encompassing cross-scheme performance metrics. We highlight the impact of ICN caching schemes in the development of the Internet of Things (IoT) and Vehicular Ad-Hoc Networks (VANETs). This work concludes by presenting insights for future developments in ICN, with a dedicated discussion on guided development for researchers in this domain; building on the aforementioned taxonomy, as well as quantitative and qualitative analyses.
Keywords: Cache designs; Cache valuation; Guided designs; Information-Centric Networking (ICN); ICN caching taxonomy; ICN performance evaluation

Simone Bolettieri, Dinh Thai Bui, Raffaele Bruno,
Towards end-to-end application slicing in Multi-access Edge Computing systems: Architecture discussion and proof-of-concept,
Future Generation Computer Systems,
Volume 136,
2022,
Pages 110-127,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.05.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001984)
Abstract: Network slicing is one of the most critical 5G pillars. It allows for sharing a 5G infrastructure among different tenants leading to improved service customisation and increased operators’ revenues. Concurrently, introducing the Multi-access Edge Computing (MEC) into 5G to support time-critical applications raises the need to integrate this distributed computing infrastructure to the 5G network slicing framework. Indeed, end-to-end latency guarantees require the end-to-end management of slice resources. For this purpose, after discussing the main gaps in the state-of-the-art with regards to such an objective, we propose a novel slicing architecture that enables the management and orchestration of slice segments that span over all the domains of an end-to-end application service, including the MEC. We also show how this general management architecture can be instantiated into a multi-tenant MEC infrastructure. A preliminary implementation of the proposed architecture focusing on the MEC domain is also provided, together with performance tests to validate the feasibility and efficacy of our design approach.
Keywords: Edge computing; MEC; NFV; 3GPP; Network slicing; Latency-sensitive applications

Zhaohui Huang, Vasilis Friderikos,
Optimal service decomposition for mobile augmented reality with edge cloud support,
Computer Communications,
Volume 202,
2023,
Pages 97-109,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.02.002.
(https://www.sciencedirect.com/science/article/pii/S0140366423000397)
Abstract: Mobile augmented reality (MAR) applications are starting to attract significant attention due to the enhanced capabilities stemming from both the network and the end devices that propel their realization. However, despite the progress on the end user devices, MAR applications are inherently hugely demanding in terms of computational and memory requirements since they combine, inter alia, video streams, computer generated images, intense computer vision algorithms and geolocation. To this end, edge cloud computing is envisioned as a key technology for supporting such applications where part of the computationally demanding algorithms could be offloaded to suitably selected edge clouds. Within that context the inherent user mobility should be considered to allow an efficient service continuum between edge and the end-terminal. To this end, in this paper, an optimal edge cloud resource MAR service decomposition is presented that takes explicitly into account the AR service composition as well as the inherent user mobility to proactively allocate resources to satisfy the required strict latency and frame accuracy requirements of MAR applications. In addition to the optimal decision making using mathematical programming, and as a mean to provide real-time decision making two advanced heuristic techniques are proposed. A Simulated Annealing based mobility aware AR algorithm (SAMAR) is developed to enhance computing efficiency and a Long Short-Term Memory (LSTM) neural network which is trained offline with optimal solutions. Numerical investigations reveal that significant gains can be achieved by the proposed schemes compare to a number of baseline previously proposed techniques.
Keywords: 5G; Augmented reality; Mobility; Edge cloud computing; Resource management; Services and applications

Roberto Morabito, Riccardo Petrolo, Valeria Loscrì, Nathalie Mitton,
Reprint of : LEGIoT: A Lightweight Edge Gateway for the Internet of Things,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 1157-1171,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.10.020.
(https://www.sciencedirect.com/science/article/pii/S0167739X18325123)
Abstract: The stringent latency together with the higher bandwidth requirements of current Internet of Things (IoT) applications, are leading to the definition of new network-infrastructures, such as Multi-access Edge Computing (MEC). This emerging paradigm encompasses the execution of many network tasks at the edge and in particular on constrained gateways that have also to deal with the plethora of disparate technologies available in the IoT landscape. To cope with these issues, we introduce a Lightweight Edge Gateway for the Internet of Things (LEGIoT) architecture. It relies on the modular characteristic of microservices and the flexibility of lightweight virtualization technologies to guarantee an extensible and flexible solution. In particular, by combining the implementation of specific frameworks and the benefits of container-based virtualization, our proposal enhances the suitability of edge gateways towards a wide variety of IoT protocols/applications (for both downlink and uplink) enabling an optimized resource management and taking into account requirements such as energy efficiency, multi-tenancy, and interoperability. LEGIoT is designed to be hardware agnostic and its implementation has been tested within a real sensor network. Achieved results demonstrate its scalability and suitability to host different applications meant to provide a wide range of IoT services.
Keywords: Internet of Things; Edge computing; Gateway; Virtualization; Container; Sensor network

Baoshan Lu, Junli Fang, Xuemin Hong, Jianghong Shi,
Energy-efficient task scheduling for mobile edge computing with virtual machine I/O interference,
Future Generation Computer Systems,
Volume 148,
2023,
Pages 538-549,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.06.020.
(https://www.sciencedirect.com/science/article/pii/S0167739X23002431)
Abstract: Mobile edge computing (MEC) is expected to support the computation-intensive and delay-sensitive applications of mobile internet users. In this paper, we investigate the resource allocation of MEC with the effect of I/O interference among parallel virtual machines (VMs) while satisfying the quality of service (QoS) of tasks. Different from existing works, we propose a flexible task scheduling approach that combining parallel and sequential computing to minimize the computing energy consumption of MEC server. We formulate the task scheduling problem as a mixed-integer nonlinear programming (MINLP) and decompose it as a CPU resource allocation subproblem, a computing time slot subproblem, and a VM selection subproblem. We show the first subproblem is a convex problem and propose a CPU frequency allocation (CFA) algorithm based on the Karush–Kuhn–Tucker (KKT) conditions to obtain the optimal CPU frequency resource allocation. For the time slot allocation and VM selection subproblems, we propose the three step allocation (TSA) and urgency based adjusting (UBA) algorithms to obtain the near-optimal solutions, respectively. Simulation results show that compared with several time slot allocations and VM selections, the proposed TSA and UBA algorithms can save up to 21.7% and 95.8% of energy consumption, respectively.
Keywords: Mobile edge computing; Virtual machine; I/O interference; Task scheduling; Mixed-integer nonlinear programming; Karush–Kuhn–Tucker

Shah Zeb, Aamir Mahmood, Syed Ali Hassan, MD. Jalil Piran, Mikael Gidlund, Mohsen Guizani,
Industrial digital twins at the nexus of NextG wireless networks and computational intelligence: A survey,
Journal of Network and Computer Applications,
Volume 200,
2022,
103309,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103309.
(https://www.sciencedirect.com/science/article/pii/S1084804521002988)
Abstract: By amalgamating recent communication and control technologies, computing and data analytics techniques, and modular manufacturing, Industry 4.0 promotes integrating cyber–physical worlds through cyber–physical systems (CPS) and digital twin (DT) for monitoring, optimization, and prognostics of industrial processes. A DT enables interaction with the digital image of the industrial physical objects/processes to simulate, analyze, and control their real-time operation. DT is rapidly diffusing in numerous industries with the interdisciplinary advances in the industrial Internet of things (IIoT), edge and cloud computing, machine learning, artificial intelligence, and advanced data analytics. However, the existing literature lacks in identifying and discussing the role and requirements of these technologies in DT-enabled industries from the communication and computing perspective. In this article, we first present the functional aspects, appeal, and innovative use of DT in smart industries. Then, we elaborate on this perspective by systematically reviewing and reflecting on recent research trends in next-generation (NextG) wireless technologies (e.g., 5G-and-Beyond networks) and design tools, and current computational intelligence paradigms (e.g., edge and cloud computing-enabled data analytics, federated learning). Moreover, we discuss the DT deployment strategies at different communication layers to meet the monitoring and control requirements of industrial applications. We also outline several key reflections and future research challenges and directions to facilitate industrial DT’s adoption.
Keywords: Industry 4.0; Digital twin; Industrial Internet of things; Cyber–physical systems; Machine learning; Artificial intelligence; Computational intelligence; Multi-access edge computing; 5G-and-Beyond/6G; Green communication; Age of information

Waleed Ejaz, Mehak Basharat, Salman Saadat, Asad Masood Khattak, Muhammad Naeem, Alagan Anpalagan,
Learning paradigms for communication and computing technologies in IoT systems,
Computer Communications,
Volume 153,
2020,
Pages 11-25,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.043.
(https://www.sciencedirect.com/science/article/pii/S0140366419312411)
Abstract: Wireless communication and computation technologies are becoming increasingly complex and dynamic due to the sophisticated and ubiquitous Internet of things (IoT) applications. Therefore, future wireless networks and computation solutions must be able to handle these challenges and dynamic user requirements for the success of IoT systems. Recently, learning strategies (particularly deep learning and reinforcement learning) are explored immensely to deal with the complexity and dynamic nature of communication and computation technologies for IoT systems, mainly because of their power to predict and efficient data analysis. Learning strategies can significantly enhance the performance of IoT systems at different stages, including at IoT node level, local communication, long-range communication, edge gateway, cloud platform, and corporate data centers. This paper presents a comprehensive overview of learning strategies for IoT systems. We categorize learning paradigms for communication and computing technologies in IoT systems into reinforcement learning, Bayesian algorithms, stochastic learning, and miscellaneous. We then present research in IoT with the integration of learning strategies from the optimization perspective where the optimization objectives are categorized into maximization and minimization along with corresponding applications. Learning strategies are discussed to illustrate how these strategies can enhance the performance of IoT applications. We also identify the key performance indicators (KPIs) used to evaluate the performance of IoT systems and discuss learning algorithms for these KPIs. Lastly, we provide future research directions to further enhance IoT systems using learning strategies
Keywords: Computation; Internet of Things; Key performance indicators; Learning paradigms; Optimization; Wireless communication

Lucas Perotin, Sandhya Kandaswamy, Hongyang Sun, Padma Raghavan,
Multi-resource scheduling of moldable workflows,
Journal of Parallel and Distributed Computing,
Volume 184,
2024,
104792,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104792.
(https://www.sciencedirect.com/science/article/pii/S0743731523001624)
Abstract: Resource scheduling plays a vital role in High-Performance Computing (HPC) systems. Most scheduling research in HPC has focused on only a single type of resource (e.g., computing cores or I/O resource). With the advancement in hardware architectures and the increase in data-intensive HPC applications, there is a need to simultaneously consider a diverse set of resources (e.g., computing cores, cache, memory, I/O, and network resources) in the design of runtime schedulers for improving the overall application performance. In this paper, we study multi-resource scheduling to minimize the makespan of computational workflows comprised of moldable parallel jobs. Moldable jobs allow the scheduler to flexibly select a variable set of resources before execution, thus can adapt to the available system resources (as compared to rigid jobs) while staying easy to design and implement (as compared to malleable jobs). We propose a Multi-Resource Scheduling Algorithm (MRSA), which combines a novel resource allocation strategy and an extended list scheduling scheme to schedule the jobs. We prove that, on a system with d types of schedulable resources, MRSA achieves an approximation ratio of 1.619d+2.545d+1 for any d≥1, and a ratio of d+3d23+O(d3) when d is large (i.e., d≥22). We also present improved approximation results for workflows comprised of jobs with special precedence constraints (e.g., series-parallel graphs, trees, and independent jobs). Further, we prove a lower bound of d on the approximation ratio of any list-based scheduling algorithm with local priority considerations. Finally, we conduct a comprehensive set of simulations to evaluate the performance of the algorithm using synthetic workflows of different structures and moldable jobs following different speedup models. The results show that MRSA fares better than the theoretical bound predicts, and that it consistently outperforms two baseline heuristics under a variety of parameter settings, illustrating its robust practical performance.
Keywords: Makespan scheduling; Multi-resources scheduling; Moldable jobs; Workflows; Approximation ratio

Zhipeng Cheng, Minghui Liwang, Ning Chen, Lianfen Huang, Xiaojiang Du, Mohsen Guizani,
Deep reinforcement learning-based joint task and energy offloading in UAV-aided 6G intelligent edge networks,
Computer Communications,
Volume 192,
2022,
Pages 234-244,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.06.017.
(https://www.sciencedirect.com/science/article/pii/S0140366422002195)
Abstract: Edge networks are expected to play an important role in 6G where machine learning-based methods are widely applied, which promote the concept of Edge Intelligence. Meanwhile, Unmanned Aerial Vehicle (UAV)-enabled aerial network is significant in 6G networks to achieve seamless coverage and super-connectivity. To this end, a joint task and energy offloading problem is studied under a UAV-aided and energy-constrained intelligent edge network, consisting of a high altitude platform (HAP), multiple UAVs, and on-ground fog computing nodes (FCNs). To guarantee the energy supply of UAVs and FCNs, both simultaneous wireless information and power transfer (SWIPT), as well as laser charging techniques are considered. Specifically, we investigate a scenario where each UAV needs to execute a computation-intensive task during each time slot and can be powered by the laser beam transmitted from the HAP. Due to the limited computation resources, each UAV can offload part of the task and energy to the FCNs for collaborative computing, to reduce local energy consumption and the overall task execution delay by adopting SWIPT. Considering the dynamics of the network, e.g., the time-varying locations of UAVs and available computation resources of FCNs, the problem is formulated as a cooperative multi-agent Markov game for UAVs, which aims to maximize the total system utility, by optimizing the task partitioning and power allocation strategies of each UAV, regarding task size, average delay and energy consumption of task execution. To tackle this problem, we propose a multi-agent soft actor–critic (MASAC)-based approach to resolve the problem. Numerical simulation results prove the superiority of our proposed approach as compared with benchmark methods.
Keywords: UAV; Task offloading; SWIPT; Laser; Multi-agent Markov game; MASAC

Xinglong Pei, Penghao Sun, Yuxiang Hu, Dan Li, Bo Chen, Le Tian,
Enabling efficient routing for traffic engineering in SDN with Deep Reinforcement Learning,
Computer Networks,
Volume 241,
2024,
110220,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110220.
(https://www.sciencedirect.com/science/article/pii/S1389128624000525)
Abstract: Traffic Engineering (TE) is applied to optimize network transmission efficiency by managing the routing of complicated traffic. Emerging Deep Reinforcement Learning (DRL) and Software-Defined Networking (SDN) provide flexible ability for traffic management and congestion control. However, existing methods either cannot reroute the network-wide traffic in an accurate way or encounter too much calculation latency due to the inference of mathematical optimization techniques. In this paper, we propose a flexible and link-reconfigurable TE solution called EfficientTE that effectively adjusts the traffic routing based on real-time traffic demands. By analyzing the characteristics of network topology and traffic, EfficientTE selects a few links that are critical for congestion in the network. Then, we propose the idea of virtual capacity that helps the DRL algorithm adjust to different link bandwidths. Based on the traffic demand and topology information collected by the SDN controller, the DRL algorithm is used to dynamically adjust the virtual capacity of the critical links to reshape the network. To ensure network performance with low disturbance, we selectively reroute the Top-K critical flows using the weighted K-shortest path algorithm, while forwarding the major flows with default rerouting. Experiments show that EfficientTE optimizes maximum link utilization and outperforms existing TE solutions by improving the load-balancing performance ratio by at least 6.13%, 18.78%, 16.20%, and 22.81% respectively in four network topologies.
Keywords: Traffic engineering; Deep reinforcement learning; Software-defined networking; Load balancing; Routing optimization

Peizhe Ma, Saurabh Garg, Mutaz Barika,
Research allocation in mobile volunteer computing system: Taxonomy, challenges and future work,
Future Generation Computer Systems,
Volume 154,
2024,
Pages 251-265,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.015.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000128)
Abstract: The rise of mobile devices and the Internet of Things has generated vast data which require efficient processing methods. Volunteer Computing (VC) is a distributed network that utilises idle resources from diverse devices for task completion. VC offers a cost-effective and scalable solution for computation resources. Mobile Volunteer Computing (MVC) capitalises on the abundance of mobile devices as participants. However, managing a large number of participants in the network presents a challenge in scheduling resources. Various resource allocation algorithms and MVC platforms have been developed, but there is a lack of survey papers summarising these systems and algorithms. This paper aims to bridge the gap by delivering a comprehensive survey of MVC, including related technologies, MVC architecture, and major finding in taxonomy of resource allocation in MVC.
Keywords: Distributed computing; Resource management; Edge computing; Cloud computing; Volunteer computing; IoT

Gabriele Russo Russo, Daniele Ferrarelli, Diana Pasquali, Valeria Cardellini, Francesco Lo Presti,
QoS-aware offloading policies for serverless functions in the Cloud-to-Edge continuum,
Future Generation Computer Systems,
Volume 156,
2024,
Pages 1-15,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.02.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000645)
Abstract: Function-as-a-Service (FaaS) paradigm is increasingly attractive to bring the benefits of serverless computing to the edge of the network, besides traditional Cloud data centers. However, FaaS adoption in the emerging Cloud-to-Edge Continuum is challenging, mostly due to geographical distribution and heterogeneous resource availability. This emerging landscape calls for effective strategies to trade off low latency at the edge of the network with Cloud resource richness, taking into account the needs of different functions and users. In this paper, we present QoS-aware offloading policies for serverless functions running in the Cloud-to-Edge continuum. We consider heterogeneous functions and service classes, and aim to maximize utility given a monetary budget for resource usage. Specifically, we introduce a two-level approach, where (i) FaaS nodes rely on a randomized policy to schedule every incoming request according to a set of probability values, and (ii) periodically, a linear programming model is solved to determine the probabilities to use for scheduling. We show by extensive simulation that our approach outperforms alternative approaches in terms of generated utility across multiple scenarios. Moreover, we demonstrate that our solution is computationally efficient and can be adopted in large-scale systems. We also demonstrate the functionality of our approach through a proof-of-concept experiment on an open-source FaaS framework.
Keywords: Serverless computing; Offloading; Edge computing; Quality of service

Nazma Akther, Kingshuk Dhar, Shahid Md. Asif Iqbal, Mohammed Nurul Huda,  Asaduzzaman,
Interest forwarding strategy in Named Data Networks (NDN) using Thompson Sampling,
Journal of Network and Computer Applications,
Volume 205,
2022,
103458,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103458.
(https://www.sciencedirect.com/science/article/pii/S1084804522001084)
Abstract: Optimizing Interest forwarding and Data delivery has been among the top dissected problems in NDN for the last decade; however, only a few contributions thrive to minimize communication cost and delay concurrently. In NDN, a receiver-driven forwarding strategy is considered resource-consuming as the routers incur computation to find the best path to the desired item, specified by an Interest’s name. On the other hand, a source-driven forwarding strategy, a scheme that suppresses the sub-optimal sources, experiences increased delay when no source answers in the exploration phase. The confluence of the two strategies can counteract the drawbacks of each one, which, however, has never been investigated. In this work, a reinforcement learning-based, namely Thompson Sampling, strategy is proposed that operates in a receiver-cum-source-driven fashion to optimize Interest forwarding and answering. The proposed method introduces a ’Beam’ concept coupled with adaptive scoped-flooding to optimize Interest forwarding, and the sources adopt Thompson Sampling to suppress the sub-optimal responses. When hit by an Interest, an optimal source sends back the desired Data to the consumer whereas a sub-optimal source remains Silent. Together, the ’Beam’ and the scoped-flooding adapt the Interest forwarding range based on cache hit/miss ratio. The adaptation optimizes communication cost and delay, and contributes to scheming the proposed strategy resource-savvy. The proof-of-concept implementation in software (simulation) reveals that the proposed system outperforms the counterpart benchmarks by reducing the communication costs and delay in NDN (by around 350% and 10%, respectively) without negotiating packet delivery ratio.
Keywords: Interest forwarding; Receiver-driven; Source-driven; Reinforcement learning; Scoped-flooding

Roberto Bruschi, Jane Frances Pajo, Franco Davoli, Chiara Lombardo,
Managing 5G network slicing and edge computing with the MATILDA telecom layer platform,
Computer Networks,
Volume 194,
2021,
108090,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108090.
(https://www.sciencedirect.com/science/article/pii/S1389128621001766)
Abstract: The fifth generation (5G) of mobile networks is seen as a key enabler to support the introduction of digital technologies in multiple sectors, empowering different verticals and enabling new business models. Thanks to native capabilities of 5G networks, especially network slicing, edge computing and multi-tenancy, highly integrated telecom infrastructures with end-to-end flexibility will be realized to lower the barriers for creating 5G-ready applications that are able to satisfy business and user necessities. In this context, the MATILDA Project has established a holistic framework that unifies the development, deployment and operation for this new kind of applications. Since Edge Computing is not natively supported by 4G, in order to anticipate the technological improvements foreseen with the advent of 5G, as well as to smoothen the transition to the new technology, this paper describes the design of the end-point between the mobile and edge environments that has been integrated in the MATILDA telecom layer platform. Such end-point, designed in a Virtual Network Function (VNF), allows intercepting and forwarding data and control traffic towards external Data Networks. Instances of this VNF can be horizontally scaled according to a decision policy, which determines the minimum number of instances required for the current load. Results show that the latency ascribable to the VNF processing is sufficiently low to satisfy the delay budget for all 5G use-cases up to 10 ms and that a QCI-based decision policy allows scaling with the traffic load, while still fulfilling the performance requirements of each application.
Keywords: Edge computing; Virtual network functions; 5G; Quality of service

Yanghao Xie, Yuyang Kong, Lin Huang, Sheng Wang, Shizhong Xu, Xiong Wang, Jing Ren,
Resource allocation for network slicing in dynamic multi-tenant networks: A deep reinforcement learning approach,
Computer Communications,
Volume 195,
2022,
Pages 476-487,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.09.015.
(https://www.sciencedirect.com/science/article/pii/S0140366422003607)
Abstract: To support the wide range of 5G use cases in a cost-efficient way, network slicing has been considered a promising solution, which makes it possible to serve multiple customized and isolated network services on a common physical infrastructure. In this paper, we investigate the resource allocation problem of network slicing in multi-tenant networks where network resources can be used by low-priority tenants change dynamically due to the preemption of high-priority tenants. We formulate the problem as an energy-minimizing mathematical optimization problem considering practical constraints. Due to the dynamic characteristics of the problem, the complexity of the optimization problem is exceptionally high, making it impossible to solve the problem in real-time using traditional optimization approaches. With discovering the special structure of the problem, we propose a Dueling-Deep Q Network (DQN)-based algorithm to solve the problem efficiently. The experimental results show that the proposed algorithm outperforms compared algorithms in terms of total energy cost, runtime, and robustness.
Keywords: Network slicing; Multi-tenant networks; Dynamic networks; Resource allocation; Deep reinforcement learning

Ashkan Yousefpour, Caleb Fung, Tam Nguyen, Krishna Kadiyala, Fatemeh Jalali, Amirreza Niakanlahiji, Jian Kong, Jason P. Jue,
All one needs to know about fog computing and related edge computing paradigms: A complete survey,
Journal of Systems Architecture,
Volume 98,
2019,
Pages 289-330,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.02.009.
(https://www.sciencedirect.com/science/article/pii/S1383762118306349)
Abstract: With the Internet of Things (IoT) becoming part of our daily life and our environment, we expect rapid growth in the number of connected devices. IoT is expected to connect billions of devices and humans to bring promising advantages for us. With this growth, fog computing, along with its related edge computing paradigms, such as multi-access edge computing (MEC) and cloudlet, are seen as promising solutions for handling the large volume of security-critical and time-sensitive data that is being produced by the IoT. In this paper, we first provide a tutorial on fog computing and its related computing paradigms, including their similarities and differences. Next, we provide a taxonomy of research topics in fog computing, and through a comprehensive survey, we summarize and categorize the efforts on fog computing and its related computing paradigms. Finally, we provide challenges and future directions for research in fog computing.
Keywords: Fog computing; Edge computing; Cloud computing; Internet of things (IoT); Cloudlet; Mobile edge computing; Multi-access edge computing; Mist computing

Lina Su, Ne Wang, Ruiting Zhou, Zongpeng Li,
Dynamic service placement and request scheduling for edge networks,
Computer Networks,
Volume 213,
2022,
108997,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108997.
(https://www.sciencedirect.com/science/article/pii/S1389128622001633)
Abstract: Multi-access computing (MEC) represents an effective technology to satisfy low-latency demands by capacitating the execution of data-intensive and latency-sensitive tasks at the edge of mobile networks. While recent studies have investigated cost-awareness regarding where to execute service tasks and how to schedule user requests to edge servers, most have primarily leveraged deep reinforcement learning or concentrated on less-complicated assumptions. In this paper, we design an online algorithm for joint service placement and request scheduling in MEC networks, subjected to multi-dimensional constraints, aiming to minimize the operational cost. However, the problem is non-trivial since it involves time-correlated service placement cost and future information. Our proposed online algorithm employs the online learning technique and rounding method to address these challenges, consisting of the following basic modules: (1) a regularization approach to decouple the offline cost-minimization problem into multiple convex sub-problems, each to be efficiently solved in each time slot; (2) a rounding method to transform an optimal fractional solution of the convex sub-problem into an integer solution of the original cost-minimization problem, with provable the performance guarantee. Our analytical results and simulations verify the effectiveness of the proposed online algorithm.
Keywords: MEC; Service placement; Request scheduling; Online algorithm

Thiarles S. Medeiros, Luan Pereira, Fábio D. Rossi, Marcelo C. Luizelli, Antonio Carlos S. Beck, Arthur F. Lorenzon,
Mitigating the processor aging through dynamic concurrency throttling,
Journal of Parallel and Distributed Computing,
Volume 156,
2021,
Pages 86-100,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.05.006.
(https://www.sciencedirect.com/science/article/pii/S0743731521001118)
Abstract: The increase in the number of cores in a single chip brings better capabilities to exploit thread-level parallelism (TLP). However, since power dissipated per area rises at each new node generation, higher temperatures are achieved, speeding up the aging of hardware components, which may provoke undesired system behavior. Considering that many applications do not scale with the number of cores, their execution with the maximum TLP available will not only degrade performance, but also unnecessarily increase temperature, further accelerating aging. Given that, we propose Hebe, a dynamic concurrency throttling approach that learns at run-time the degree of TLP that reduces the aging for OpenMP applications. Hebe is totally transparent, needing no modifications in the original binary code. With a set of extensive experiments (fifteen benchmarks and four multicore platforms), we show that Hebe outperforms state-of-the-art approaches with very close results from the best possible solution given by an exhaustive search.
Keywords: Thread-level parallelism; Aging; Optimization; Runtime system

Camille Monière, Bertrand Le Gal, Emmanuel Boutillon,
Real-time energy-efficient software and hardware implementations of a QCSP communication system,
Journal of Systems Architecture,
Volume 141,
2023,
102933,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102933.
(https://www.sciencedirect.com/science/article/pii/S1383762123001121)
Abstract: In the Internet of Things, the energy efficiency of communications is crucial. Thus, using data-less preambles to detect frames is suboptimal. A preamble-less physical layer named “Quasi-Cyclic Short Packet (QCSP)” proposed a solution. However, the high computational complexity of the receiver makes the implementation challenging. This paper studies the feasibility and efficiency of real-time software and hardware QCSP systems. Several highly efficient transmitter implementations are presented, demonstrating the relevance of the approach. The parallelization and optimization strategies are elaborated for both software and hardware receiver implementations. They allowed software implementations with throughputs above 50 kb/s (maximum for the LoRa standard), consuming as low as 27 μJ/b. Thanks to detailed algorithm refinements, the throughtput even exceeded 250 kb/s on FPGA using High-Level Synthesis methodology, for an estimated consumption of 3.9 μJ/b. The results support the viability of the QCSP approach for cloud-RAN and edge computing in Low-Power Wide-Area Networks.
Keywords: Real-time implementation; CCSK; Small packets; Hardware; Software; Low power wide area network

Cha Hwan Song, Xin Zhe Khooi, Dinil Mon Divakaran, Mun Choon Chan,
DySO: Enhancing application offload efficiency on programmable switches,
Computer Networks,
Volume 224,
2023,
109607,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109607.
(https://www.sciencedirect.com/science/article/pii/S138912862300052X)
Abstract: Application offloads on modern high-speed programmable switches have been proposed in a variety of systems (e.g., key–value store systems and network middleboxes) so as to efficiently scale up the traditional server-oriented deployments. However, they largely achieve sub-optimal offloading efficiency due to the lack of (1) capability to perform control actions at sufficient rates, and (2) adaptability to workload changes. In this paper, we scrutinize the common stumbling blocks of existing frameworks with performance evaluations on real workloads. We present DySO (Dynamic State Offloading), a framework which enables expeditious on-demand control actions and self-tuning of management rules. DySO’s key insight is to perform control actions via a data-path instead of the switch control channel which is the bottleneck to read/write states into data plane. Our software simulations show up to 100% performance improvement compared to existing systems for various real world traces. On top of that, we implement and evaluate DySO on a commodity programmable switch, showing two orders of magnitude faster responsiveness to sudden workload changes compared to the existing systems.
Keywords: Programmable networks; P4; Framework; Application acceleration; In-network caching

Sparsh Mittal,  Vibhu,
A survey of accelerator architectures for 3D convolution neural networks,
Journal of Systems Architecture,
Volume 115,
2021,
102041,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102041.
(https://www.sciencedirect.com/science/article/pii/S1383762121000400)
Abstract: 3D convolution neural networks (CNNs) have shown excellent predictive performance on tasks such as action recognition from videos. Since 3D CNNs have unique characteristics and extremely high compute/memory-overheads, executing them on accelerators designed for 2D CNNs provides sub-optimal performance. To overcome these challenges, researchers have recently proposed architectures for 3D CNNs. In this paper, we present a survey of hardware accelerators and hardware-aware algorithmic optimizations for 3D CNNs. We include only those CNNs that perform 3D convolution and not those that perform only 2D convolution on 2D or 3D data. We highlight their key ideas and underscore their similarities and differences. We believe that this survey will spark a great deal of research towards the design of ultra-efficient 3D CNN accelerators of tomorrow.
Keywords: 3D CNNs; 3D convolution neural networks; Artificial intelligence; Artificial neural networks; Image processing

Partha Pratim Ray, Dinesh Dash, Debashis De,
Edge computing for Internet of Things: A survey, e-healthcare case study and future direction,
Journal of Network and Computer Applications,
Volume 140,
2019,
Pages 1-22,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.05.005.
(https://www.sciencedirect.com/science/article/pii/S1084804519301651)
Abstract: The world has recently witnessed the emergence of huge technological growth in the field of data transmission and smart living through various modes of information and communication technology. For example, edge computing has taken a leading role to embark upon the problems related to the internetwork bandwidth minimization and service latency reduction. Inclusion of small microcontroller chips, smart sensors and actuators in the existing socio-economic sectors have paved the Internet of Things (IoT) to act upon the dissemination of smart services to the end users. Thus, a strong need of understating of the industrial elements of edge computing has become necessary that can share the mutual goal while assimilating with the IoT. This paper advocates the crucial role of industrial standards and elements of the edge computing for the dissemination of overwhelming augmented user experience with conjunction with the IoT. First, we present the taxonomical classification and review the industrial aspects that can benefit from both of the IoT and edge computing scenario, then discuss about each of the taxonomical components in detail. Second, we present two practically implemented use cases that have recently employed the edge-IoT paradigm together to solve urban smart living problems. Third, we propose a novel edge-IoT based architecture for e-healthcare i.e. EH-IoT and developed a demo test-bed. The test results showed promising results towards minimizing dependency over IoT cloud analytics or storage facility. We conclude with discussion on the various parameters such as, architecture, requirement capability, functional issues, and selection criteria, related to the survival of edge-IoT ecosystem incorporation.
Keywords: Edge computing; Internet of Things; e-healthcare; Industrial protocol

Jordi Garcia, Francesc Aguiló, Adrià Asensio, Ester Simó, Marisa Zaragozá, Xavi Masip-Bruin,
Data-flow driven optimal tasks distribution for global heterogeneous systems,
Future Generation Computer Systems,
Volume 125,
2021,
Pages 792-805,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.07.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X21002806)
Abstract: As a result of advances in technology and highly demanding users expectations, more and more applications require intensive computing resources and, most importantly, high consumption of data distributed throughout the environment. For this reason, there has been an increasing number of research efforts to cooperatively use geographically distributed resources, working in parallel and sharing resources and data. In fact, an application can be structured into a set of tasks organized through interdependent relationships, some of which can be effectively executed in parallel, notably speeding up the execution time. In this work a model is proposed aimed at offloading tasks execution in heterogeneous environments, considering different nodes computing capacity connected through distinct network bandwidths, and located at different distances. In the envisioned model, the focus is on the overhead produced when accessing remote data sources as well as the data transfer cost generated between tasks at run-time. The novelty of this approach is that the mechanism proposed for tasks allocation is data-flow aware, considering the geographical location of both, computing nodes and data sources, ending up in an optimal solution to a highly complex problem. Two optimization strategies are proposed, the Optimal Matching Model and the Staged Optimization Model, as two different approaches to obtain a solution to the task scheduling problem. In the optimal model approach a global solution for all application’s tasks is considered, finding an optimal solution. Differently, the staged model approach is designed to obtain a local optimal solution by stages. In both cases, a mixed integer linear programming model has been designed intended to minimizing the application execution time. In the studies carried out to evaluate this proposal, the staged model provides the optimal solution in 76% of the simulated scenarios, while it also dramatically reduces the solving time with respect to optimal. Both models have pros and cons and, in fact, can be used together to complement each other. The optimal model finds the global optimal solution at high running time cost, which makes this model unpractical on some scenarios. The staged model instead, is faster enough to be used on those scenarios; however, the given solution might not be optimal in some cases.
Keywords: Edge computing; Distributed computing; Heterogeneous systems; Task distribution; Task offloading; Resources allocation

Claudio Fiandrino, Alejandro Blanco Pizarro, Pablo Jiménez Mateo, Carlos Andrés Ramiro, Norbert Ludant, Joerg Widmer,
openLEON: An end-to-end emulation platform from the edge data center to the mobile user,
Computer Communications,
Volume 148,
2019,
Pages 17-26,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.08.024.
(https://www.sciencedirect.com/science/article/pii/S0140366419301392)
Abstract: To support next generation services, 5G mobile network architectures are increasingly adopting emerging technologies like software-defined networking (SDN) and network function virtualization (NFV). Core and radio access functionalities are virtualized and executed in edge data centers, in accordance with the Multi-Access Edge Computing (MEC) principle. While testbeds are an essential research tool for experimental evaluation in such environments, the landscape of data center and mobile network testbeds is fragmented. In this work, we aim at filling this gap by presenting openLEON, an open source muLti-access Edge cOmputiNg end-to-end emulator that operates from the edge data center to the mobile users. openLEON bridges the functionalities of existing emulators for data centers and mobile networks, i.e., Containernet and srsLTE, and makes it possible to evaluate and validate research ideas on all the components of an end-to-end mobile edge architecture.
Keywords: Edge Computing; Data center networks; Mobile networks; Emulation

S.S. Chalapathi G., Vinay Chamola, Chen-Khong Tham, Gurunarayanan S., Nirwan Ansari,
An optimal delay aware task assignment scheme for wireless SDN networked edge cloudlets,
Future Generation Computer Systems,
Volume 102,
2020,
Pages 862-875,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.003.
(https://www.sciencedirect.com/science/article/pii/S0167739X19301232)
Abstract: Over the past decade, there has been an increasing demand for mobile devices to perform computationally intensive tasks. However, the computational capability of these devices is limited due to memory, power and portability constraints. One of the feasible and attractive ways to enhance the performance of the resource-limited mobile devices is to offload their computationally intensive tasks on to the cloud servers when internet connectivity is available. However, when cloud servers are involved in processing, the latency and cost of computation increases. To mitigate these problems, devices with high computational resources, called cloudlets, can be deployed in the locations close to the mobile users/devices. The mobile devices can then offload their computationally intensive tasks on to them. Due to easier access and nearness of the cloudlets, the cost and latency in processing the tasks decreases. In this work, we focus on task assignment problem in a multi-cloudlet network connected via a wireless SDN network, which services the task offload requests from mobile devices in a given locality. The aim of the proposed solution is to minimize latency and thus enhance the quality of service for mobile devices. We prove the optimality of the proposed solution mathematically and employ an admission control policy to maintain this optimality even in heavily loaded networks. We also perform numerical simulations for two scenarios of small and large networks and evaluate the performance for varying traffic and network parameters. The results demonstrate that the proposed task assignment method offers reduced latency compared to state-of-the-art task assignment approaches and hence improves the quality of service offered to mobile devices.
Keywords: Edge computing; Load balancing; Quality of service; Task assignment; Wireless SDN

Dawei Sun, Yijing Cui, Minghui Wu, Shang Gao, Rajkumar Buyya,
An energy efficient and runtime-aware framework for distributed stream computing systems,
Future Generation Computer Systems,
Volume 136,
2022,
Pages 252-269,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.06.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002187)
Abstract: Task scheduling in distributed stream computing systems is an NP-complete problem. Current scheduling schemes usually have a pause or slow start process due to the fluctuation of input data stream, which affects the performance stability, especially the high throughput and low latency goals. In addition, idle compute nodes at runtime may result in large idle load energy consumption. To address these problems, we propose an energy efficient and runtime-aware framework (Er-Stream). This paper thoroughly discusses the framework from the following aspects: (1) The communication between real-time data streaming tasks is investigated; stream application, resource and energy consumption are modeled to formalize the scheduling problem. (2) After an initial topology is submitted to the cluster, task pairs with high communication cost are processed on the same compute node through a lightweight task partitioning strategy, minimizing the communication cost between nodes and avoiding frequent triggering of runtime scheduling. (3) At runtime, reliable task migration is performed based on node communication and resource usage, which in turn helps the dynamic adjustment of the node energy consumption. (4) Metrics including latency, throughput, resource load and energy consumption are evaluated in a real distributed stream computing environment. With a comprehensive evaluation of variable-rate input scenarios, the proposed Er-Stream system provides promising improvements on throughput, latency and energy consumption compared to the existing Storm’s scheduling strategies.
Keywords: Stream computing; Runtime-aware; Reliable migration; Energy consumption; High throughput; Low latency

Kihan Choi, Hyungseok Seo, Hyuck Han, Minsoo Ryu, Sooyong Kang,
CredsCache: Making OverlayFS scalable for containerized services,
Future Generation Computer Systems,
Volume 147,
2023,
Pages 44-58,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.04.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X2300167X)
Abstract: OverlayFS, a Linux kernel driver for the Docker container, supports access control in Linux. User processes in the container must have appropriate privileges to access files in the backing file system. To that end, OverlayFS temporarily overrides credentials of a process with the mounter’s credentials whenever the process accesses a file, inode, or directory. However, we found that this mechanism incurs severe overhead owing to contention in updating a shared reference counter for the mounter’s credentials, which hinders OverlayFS scalability. In this paper, we propose a credential caching scheme, CredsCache, which greatly relieves contention by maintaining a per-process cached version of the mounter’s credentials. In CredsCache, each process performs credential overriding and reverting using its own cached version without updating the shared reference counter. We implemented CredsCache in OverlayFS and evaluated its performance using micro- and macro-benchmarks. Using solid-state drive (SSD) backend storage, the micro-benchmark results showed that CredsCache achieved up to 23.3× and 54.7× higher performance for data and metadata I/O, respectively, than vanilla OverlayFS. Moreover, the macro-benchmark and real-world benchmark results showed that CredsCache produced up to 5.6× and 2.9× better performance for respective database and web server benchmarks than vanilla OverlayFS.
Keywords: Container; OverlayFS; Credentials; Filesystem scalability; Reference counting; Credentials management

Tanvi Chawla, Girdhari Singh, Emmanuel S. Pilli, M.C. Govil,
Storage, partitioning, indexing and retrieval in Big RDF frameworks: A survey,
Computer Science Review,
Volume 38,
2020,
100309,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2020.100309.
(https://www.sciencedirect.com/science/article/pii/S1574013720304093)
Abstract: Resource Description Framework (RDF) is increasingly being used to model data on the web. RDF model was designed to support easy representation and exchange of information on the web. RDF is queried using SPARQL, a standard query language recommended by W3C. The growth in acceptance of RDF format can be attributed to its flexible and reusable nature. The size of RDF data is steadily increasing as many government organizations and companies are using RDF for data representation and exchange. This resulted in the need for developing distributed RDF frameworks that can efficiently manage RDF data on large scale i.e. Big RDF data. These scalable distributed RDF data management systems competent enough to handle Big RDF data can also be termed as Big RDF frameworks. The proliferation of RDF data has made RDF data management a difficult task. In this survey, we provide an extensive literature on Big RDF frameworks from the aspect of storage, partitioning, indexing, query optimization and processing. A taxonomy of the tools and technologies used for storage and retrieval of Big RDF data in these systems has been presented. The comparative evaluation of some Big RDF frameworks based on query performance and our observations from this evaluation are described. The research challenges identified during the study of these systems are elaborated to suggest promising directions for future research.
Keywords: Storage; Partitioning; Indexing; Query optimization; Query processing; Semantic web; RDF; SPARQL

Simran Preet Kaur, Manojit Ghose, Ananya Pathak, Rutuja Patole,
A survey on mapping and scheduling techniques for 3D Network-on-chip,
Journal of Systems Architecture,
Volume 147,
2024,
103064,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103064.
(https://www.sciencedirect.com/science/article/pii/S1383762124000018)
Abstract: Network-on-chips (NoCs) have been widely employed in the design of multiprocessor system-on-chips (MPSoCs) as a scalable communication solution. NoCs enable communications between on-chip Intellectual Property (IP) cores to perform a task seamlessly collaborating among them. Mapping and Scheduling methodologies are key elements in assigning application tasks, allocating the tasks to the IPs, and organizing communication among them to achieve some specified objectives. The goal of this paper is to present detailed state-of-the-art research in the field of mapping and scheduling of applications on 3D NoC, classify the works based on several parameters, and provide some potential research directions.
Keywords: Survey; 3D network on chip; 3D NoC; Mapping; Scheduling; 3D system-on-chip; 3D SoC

Tuo Cao, Yibo Jin, Xiongfeng Hu, Sheng Zhang, Zhuzhong Qian, Baoliu Ye, Sanglu Lu,
Adaptive provisioning for mobile cloud gaming at edges,
Computer Networks,
Volume 205,
2022,
108704,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108704.
(https://www.sciencedirect.com/science/article/pii/S138912862100565X)
Abstract: Mobile cloud gaming (MCG), which is proposed to deliver high-quality gaming experience to users anywhere and anytime, suffers from tremendous wide-area traffic and long network delays. In order to shorten the delays and provide the gaming services in close proximity to end users, the mobile edge computing (MEC) is envisioned as a promising approach to enable related computations at the edge of the network. Unfortunately, the performance of MEC-enabled MCG highly depends on the provisioning of both gaming services and bandwidth, since the quality of gaming experience (QoE) is easily impaired by both delays and video frame rates. Furthermore, due to the erratic mobility of users, migrating the services accordingly actually decreases the QoE impairment, but incurs extra system cost, leading to the performance-cost tradeoff. In order to address these challenges, we jointly investigate adaptive service placement and bandwidth allocation for MEC-enabled MCG. Considering the system dynamics, we propose to minimize the QoE impairment in a long-term scope, under the constrained cost for migrations. We design an online two-layer iterative algorithm (OnTrial) to solve the problem. Theoretical analysis demonstrates that OnTrial achieves a near-optimal performance and the potential violation of the migration constraint is bounded. Extensive experiments with real-world traces confirm that OnTrial outperforms other algorithms regarding the long-term QoE impairment for games.
Keywords: Mobile cloud gaming; Online provisioning; Mobile edge computing; QoE impairment

Pu Wang, Tao Ouyang, Qiong Wu, Qianyi Huang, Jie Gong, Xu Chen,
Hydra: Hybrid-model federated learning for human activity recognition on heterogeneous devices,
Journal of Systems Architecture,
Volume 147,
2024,
103052,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.103052.
(https://www.sciencedirect.com/science/article/pii/S138376212300231X)
Abstract: Federated Learning (FL) has recently received extensive attention in enabling privacy-preserving edge AI services for Human Activity Recognition (HAR). However, users’ mobile and wearable devices in the HAR scenario usually possess dramatically different computing capability and diverse data distributions, making it very challenging for such heterogeneous HAR devices to conduct effective collaborative training (co-training) with the traditional FL schemes. To address this issue, we present Hydra, a Hybrid-model federated learning mechanism that facilitates the co-training among heterogeneous devices by allowing them to train models that well fit their own computing capability. Specifically, Hydra leverages BranchyNet to design a large-small global hybrid-model and enables heterogeneous devices to train the proper parts of the model tailored to their computing capability. Hydra drives co-training among the devices and clusters them based on model similarity to mitigate the impact of HAR data heterogeneity on model accuracy. In order to deal with the issue that large model may lack sufficient training data due to the limited number of high-performance devices in FL, we introduce a pairing scheme between high and low performance devices for effective co-training, and further propose sample selection approach to select more valuable samples to participate in co-training. We then formulate a constrained co-training problem within a cluster that is proved to be NP-hard and devise a fast greedy-based heuristic algorithm to solve it. In addition, to address the low accuracy of small models, we also propose a Large-to-Small knowledge distillation algorithm for resource-constrained devices to optimize the efficiency of transferring knowledge from large models to small models. We conduct extensive experiments on three HAR datasets and the experimental results demonstrate the superior performance of Hydra for achieving outstanding model accuracy improvement compared with other state-of-the-art schemes.
Keywords: Edge AI; Federated learning; Human activity recognition

Lucas Leandro Nesi, Arnaud Legrand, Lucas Mello Schnorr,
Asynchronous multi-phase task-based applications: Employing different nodes to design better distributions,
Future Generation Computer Systems,
Volume 147,
2023,
Pages 119-135,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.05.005.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001796)
Abstract: HPC infrastructures often present intra-node (multi-core CPUs and multiple GPUs) and system-level heterogeneity (different nodes arranged into partitions). HPC applications with several phases, each with distinct resource necessities, can exploit the task-based programming paradigm to overlap phases and take advantage of inter-node heterogeneity to improve performance, provided their workload is correctly distributed. We study two applications with these characteristics, a machine learning framework for geostatistics data, ExaGeoStat, and a multivariate data analysis library, Diodon. We show how to both (1) organize the application to improve runtime and scheduling decisions that impact the asynchronous phases overlap with performance gains between 31% and 46% in ExaGeoStat and 29% to 40% in Diodon when running on homogeneous nodes; and (2) create a distribution per phase over heterogeneous nodes considering overlap and reducing redistribution overhead, improving performance up to 69% in ExaGeoStat and 73% in Diodon compared to a block-cyclic distribution, thereby taming the diversity in supercomputers.
Keywords: Task-based; Scheduling; Partitioning; Load Balancing; Heterogeneous; Distribution

Brian Hildebrand, Mohamed Baza, Tara Salman, Simra Tabassum, Bharath Konatham, Fathi Amsaad, Abdul Razaque,
A comprehensive review on blockchains for Internet of Vehicles: Challenges and directions,
Computer Science Review,
Volume 48,
2023,
100547,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100547.
(https://www.sciencedirect.com/science/article/pii/S157401372300014X)
Abstract: Internet of Vehicles (IoVs) consists of smart vehicles, Autonomous Vehicles (AVs) as well as roadside units (RSUs) that communicate wirelessly to provide enhanced transportation services such as improved traffic efficiency and reduced traffic congestion and accidents. Unfortunately, current IoV networks suffer from security, privacy, and trust issues. Blockchain technology emerged as a decentralized approach for enhanced security without depending on trusted third parties to run services. Blockchain offers the benefits of trustworthiness and immutability and mitigates the problem of a single point of failure and other attacks. In this work, we present the state-of-the-art Blockchain-enabled IoVs (BIoVs) with a particular focus on their applications, such as crowdsourcing-based applications, energy trading, traffic congestion reduction, collision, accident avoidance, infotainment, and content caching. We also present in-depth applications of federated learning (FL) for BIoVs. The key challenges of integrating Blockchain with IoV are investigated in several domains, such as edge computing, machine learning, and Federated Learning (FL). Lastly, we present several open issues, challenges, and future opportunities in AI-enabled BIoV, hardware-assisted security for BIoV, and quantum computing attacks on BIoV applications.
Keywords: Blockchain; Federated learning; Vehicular networks; Internet of Vehicles (IoV); Autonomous vehicles; Security; Privacy; Smart contracts

Ning Zhang, Chenfei Zhang, Dengpan Wu,
Construction of a smart management system for physical health based on IoT and cloud computing with big data,
Computer Communications,
Volume 179,
2021,
Pages 183-194,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.08.018.
(https://www.sciencedirect.com/science/article/pii/S0140366421003121)
Abstract: In response to the needs of physical health data management in the context of the Internet of Everything, this article first uses cloud computing, big data, mobile Internet and other technologies to build a physical health smart management system. When the system is deployed, edge nodes are introduced in each data collection area, and the system is composed of data collection, transmission, and query and analysis modules. Secondly, it uses convolutional neural network to learn features from body measurement data unsupervised. Then, based on the Gaussian mixture distribution, a three-level physical fitness assessment model was established. Finally, input the learned features into the evaluation model to get the result of physical fitness evaluation. The results show that the system not only has a better response to the family, but also can reduce operating costs and improve work efficiency. Moreover, the algorithm in this paper is not affected by individual physical fitness assessment methods and results, and provides new ideas and methods for physical fitness assessment.
Keywords: Internet of Things; Cloud computing; Big data; Physical health; Smart management

Francesco Giannone, Pantelis A. Frangoudis, Adlen Ksentini, Luca Valcarenghi,
Orchestrating heterogeneous MEC-based applications for connected vehicles,
Computer Networks,
Volume 180,
2020,
107402,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107402.
(https://www.sciencedirect.com/science/article/pii/S1389128620301997)
Abstract: In the near future, 5G-connected vehicles will be able to exchange messages with each other, with the roadside infrastructure, with back-end servers, and with the Internet. They will do so with reduced latency, increased reliability, and large throughput under high mobility and user density. Different services with different requirements, such as Advanced Driving Assistance (ADA) and High Definition (HD) Video Streaming, will share the same physical resources, such as the wireless channel. Thus, a rigid orchestration among them becomes necessary to prioritize network resource allocation. This study proposes a Connected Vehicle Service Orchestrator (CVSO) which optimizes the Quality of Experience (QoE) of an in-vehicle infotainment video delivery service, while taking into account the required bandwidth for coexisting high priority services, such as ADA. To this end, we provide an Integer Linear Programming (ILP) formulation for the problem of optimally assigning a video streaming bitrate/quality per user to maximize the overall QoE, considering information from the video service and the Radio Access Network (RAN) levels. Our system takes advantage of recent developments in the area of Multi-access Edge Computing (MEC). In particular, we have implemented the CVSO and other service-level components and have deployed them on top of a standards-compliant MEC platform that we have developed. We exploit MEC-native services such as the Radio Network Information Service (RNIS) to offer the CVSO the necessary level of RAN awareness. Experiments on a full LTE network testbed featuring our MEC platform demonstrate the performance improvements our system brings in terms of video QoE. Furthermore, we propose and evaluate different algorithms to solve the ILP, which exhibit different trade-offs between solution quality and execution time.
Keywords: Multi-access edge computing; 4G/5G systems; Connected vehicles; Video streaming; Quality of experience

Feifei Zhang, Jidong Ge, Chifong Wong, Chuanyi Li, Xingguo Chen, Sheng Zhang, Bin Luo, He Zhang, Victor Chang,
Online learning offloading framework for heterogeneous mobile edge computing system,
Journal of Parallel and Distributed Computing,
Volume 128,
2019,
Pages 167-183,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.02.003.
(https://www.sciencedirect.com/science/article/pii/S0743731519301492)
Abstract: Cloud of Things (CoT) is a significant paradigm for bridging cloud resource and mobile terminals. Mobile edge computing (MEC) is a supporting architecture for CoT. The objectives of this paper are to describe and evaluate a method to handle the computation offloading problem during user mobility which minimizes the offloading failure rate in heterogeneous network. Furthermore, users’ mobility and their choices for offloading lead to the everchanging condition of wireless network and opportunistic resource available. By modeling such dynamic mobile edge environment, quantizing the user cost, failure penalty and diversified QoS requirements, computation offloading problem is converted into an online decision-making problem in a stochastic process. We divide the decision-making into two phases: offloading planning phase and offloading running phase. In both phases the learning agent can continuously improve the control policy. We also conduct a failure recovery policy to tackle different types of failure and is included in the decision-making process. The numerical results show that the proposed online learning offloading method for mobile users can derive the optimal offloading scheme compared with the baseline algorithms.
Keywords: Mobile edge computing; Cloudlet; Computation offloading; Offloading failure; Reinforcement learning

Juan M. Jurado, Emilio J. Padrón, J. Roberto Jiménez, Lidia Ortega,
An out-of-core method for GPU image mapping on large 3D scenarios of the real world,
Future Generation Computer Systems,
Volume 134,
2022,
Pages 66-77,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.03.022.
(https://www.sciencedirect.com/science/article/pii/S0167739X22000978)
Abstract: Image mapping on 3D huge scenarios of the real world is one of the most fundamental and computational expensive processes for the integration of multi-source sensing data. Recent studies focused on the observation and characterization of Earth have been enhanced by the proliferation of Unmanned Aerial Vehicle (UAV) and sensors able to capture massive datasets with a high spatial resolution. Despite the advances in manufacturing new cameras and versatile platforms, only a few methods have been developed to characterize the study area by fusing heterogeneous data such as thermal, multispectral or hyperspectral images with high-resolution 3D models. The main reason for this lack of solutions is the challenge to integrate multi-scale datasets and high computational efforts required for image mapping on dense and complex geometric models. In this paper, we propose an efficient pipeline for multi-source image mapping on huge 3D scenarios. Our GPU-based solution significantly reduces the run time and allows us to generate enriched 3D models on-site. The proposed method is out-of-core and it uses available resources of the GPU’s machine to perform two main tasks: (i) image mapping and (ii) occlusion testing. We deploy highly-optimized GPU-kernels for image mapping and detection of self-hidden geometry in the 3D model, as well as a GPU-based parallelization to manage the 3D model considering several spatial partitions according to the GPU capabilities. Our method has been tested on 3D scenarios with different point cloud densities (66M, 271M, 542M) and two sets of multispectral images collected by two drone flights. We focus on launching the proposed method on three platforms: (i) System on a Chip (SoC), (ii) a user-grade laptop and (iii) a PC. The results demonstrate the method’s capabilities in terms of performance and versatility to be computed by commodity hardware. Thus, taking advantage of GPUs, this method opens the door for embedded and edge computing devices for 3D image mapping on large-scale scenarios in near real-time.
Keywords: Parallel computing; GPGPU; Image mapping; 3D model; Multi-source data fusion

Mahdi Sharara, Francesca Fossati, Sahar Hoteit, Véronique Vèque, Francesca Bassi,
Minimizing energy consumption by joint radio and computing resource allocation in Cloud-RAN,
Computer Networks,
Volume 234,
2023,
109870,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109870.
(https://www.sciencedirect.com/science/article/pii/S1389128623003158)
Abstract: Cloud-RAN is a key 5G enabler; it centralizes the baseband processing of several base stations by executing the baseband functions in a centralized, virtualized, and shared entity known as the Base Band Unit (BBU)-Pool. Cloud-RAN paves the way for joint management of the radio and computing resources of multiple base stations. In fact, centralization and virtualization allow for decreasing energy consumption which decreases Capital Expenditure (CAPEX) and Operational Expenditure (OPEX). Cloud-RAN architecture permits jointly allocating the radio and computing resources of multiple base stations. The radio resources include the Resource Blocks (RBs), the transmission power, and the Modulation Coding Scheme (MCS), whereas the computing resources include the CPUs resources. This paper investigates the potential benefits that could be scored thanks to the joint allocation of these two types of resources, with respect to energy consumption and overall throughput, when radio resources are finite and computing resources are not. The latter is an effect of the C-RAN architecture, which allows scalability and fast computing resource provisioning. Due to the unconstrained availability of computing resources, the joint allocation of radio and computing resources has a negligible impact when the objective is throughput maximization. However, it is highly beneficial when the target is energy consumption minimization in comparison to the sequential allocation that consists of allocating radio resources first, and then computing resources are allocated. For that, we formulate a Mixed Integer Linear Programming (MILP) problem having the objective of minimizing energy consumption. When the goal is to minimize energy consumption, the joint allocation of radio and computing resources reduces the total energy consumption by up to 21.3% when compared to the case where radio and computing resources in the BBU pool are allocated sequentially. Furthermore, given the NP-hardness of solving a MILP problem, we propose a two-step low-complexity matching game-based algorithm with a transmission power adjustment mechanism that aims at performing close to the MILP solver. The results show that our proposed matching game algorithm is a good alternative for solving the joint-allocation MILP problem, producing results that are very close to the MILP optimal solutions.
Keywords: Cloud-RAN; Joint radio and computing resource allocation; Energy consumption minimization; Mixed integer linear programming (MILP); Matching game

Serap Ergun, Ibrahim Sammour, Gerard Chalhoub,
A survey on how network simulators serve reinforcement learning in wireless networks,
Computer Networks,
Volume 234,
2023,
109934,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109934.
(https://www.sciencedirect.com/science/article/pii/S1389128623003791)
Abstract: Rapid adoption of mobile devices, coupled with the increase in prominence of mobile applications and services, resulted in unprecedented infrastructure requirements for mobile and wireless networks. To improve user experience, future 5G and wireless network systems evolve to support increased mobile traffic, real-time precision analysis, and adaptable network resource management. As mobile environments become more complex, heterogeneous, and evolving, these tasks become more difficult. In order to solve these problems, many researchers rely on reinforcement learning. The success of reinforcement learning stems from its support for new and powerful tools that solve problems. Nodes mobility, instability of wireless connections, the coexistence of multiple wireless technologies, and resource sharing among users are a few examples of what makes a wireless network a dynamic system. Learning, which is the main feature of reinforcement learning, enables wireless nodes to adapt to the dynamics of the system over time. For the learning to be efficient, it should be done over realistic and varied conditions. This is where network simulation tools can be useful. Network simulators are extensively used when it comes to studying wireless network protocols. They offer the advantage of scaling up scenarios at minimum cost and the ability to test many possible configurations quicker under a controlled environment. The main purpose of this survey is to show how network simulators help in developing reinforcement learning techniques in wireless networks. We emphasize how these tools can be used in the learning process and which problems they can solve. In the end, we discuss open issues related to this topic and highlight some best practice guidelines when it comes to mixing network simulators, reinforcement learning, and wireless protocols.
Keywords: Wireless networks; Reinforcement learning; Network simulators

Lei Zhang, Panyue Lin,
Reinforcement learning based energy-neutral operation for hybrid EH powered TBAN,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 311-320,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.037.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003600)
Abstract: The aging population, outbreak of new infectious diseases and shortage of medical resources promote rapid development of telemedicine. Wireless textile body area network (TBAN), which combines functional textile and wireless body area network (WBAN), is gaining great attention as an efficient medium of remote medical care. This is because of its unique materials and application scenario, as well as its convenience and friendliness to the elderly. Moreover, it is an effective application for integrating edge computing with next generation of wearable technology. Nonetheless, it is unavoidable that TBAN has to deal with reliability and energy issues. Given these deficiencies and challenges, this paper focuses on the feasibility of achieving wearable energy neutral operation (ENO) in TBAN while maintaining robustness. In addition to adding user posture factors regarding network specifics, we combine hybrid energy harvesting (EH) techniques and duty cycle schemes. A hybrid radio frequency (RF) energy and Triboelectric nanogenerator (TENG) EH-assisted TBAN system is built in this work. We analyze and discuss the delay, data rate and packet error rate (PER) under five typical daily activities (standing, sitting, lying, walking, and running). To optimize the ENO problem, two reinforcement learning (Q-learning and Deep Q-Network (DQN)) based algorithms are proposed. According to numerical results, both algorithms ultimately lead to stable power levels compared to the continuous decline of battery power without optimization. DQN-based optimization performs better than Q-Learning. For instance, 14% and 56% improvements in PER and battery power, respectively.
Keywords: TBAN; ENO; Energy harvesting; Optimization; Reinforcement learning

Tahseen Khan, Wenhong Tian, Guangyao Zhou, Shashikant Ilager, Mingming Gong, Rajkumar Buyya,
Machine learning (ML)-centric resource management in cloud computing: A review and future directions,
Journal of Network and Computer Applications,
Volume 204,
2022,
103405,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103405.
(https://www.sciencedirect.com/science/article/pii/S1084804522000649)
Abstract: Cloud computing has rapidly emerged as a model for delivering Internet-based utility computing services. Infrastructure as a Service (IaaS) is one of the most important and rapidly growing models in cloud computing. Scalability, quality of service, optimum utility, decreased overheads, higher throughput, reduced latency, specialised environment, cost-effectiveness, and a streamlined interface are some of the essential elements of cloud computing for IaaS. Traditionally, resource management has been done through static policies, which impose certain limitations in various dynamic scenarios, prompting cloud service providers to adopt data-driven, machine-learning-based approaches. Machine learning is being used to handle various resource management tasks, including workload estimation, task scheduling, VM consolidation, resource optimisation, and energy optimisation, among others. This paper provides a detailed review of machine learning-based resource management solutions. We begin by introducing background concepts of cloud computing like service models, deployment models, and machine learning use in cloud computing. Then we look at resource management challenges in cloud computing, categorise them based on various aspects of resource management types such as workload prediction, VM consolidation, resource provisioning, VM placement and thermal management, review current techniques for addressing these challenges, and evaluate their key benefits and drawbacks. Finally, we propose prospective future research directions based on observed resource management challenges and shortcomings in current approaches for solving these challenges.
Keywords: Resource management; Cloud computing; Data centres; Machine learning

Waleed Ejaz, Shree K. Sharma, Salman Saadat, Muhammad Naeem, Alagan Anpalagan, N.A. Chughtai,
A comprehensive survey on resource allocation for CRAN in 5G and beyond networks,
Journal of Network and Computer Applications,
Volume 160,
2020,
102638,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102638.
(https://www.sciencedirect.com/science/article/pii/S1084804520301120)
Abstract: The diverse service requirements coming with the advent of sophisticated applications as well as a large number of connected devices demand for revolutionary changes in the traditional distributed radio access network (RAN). To this end, Cloud-RAN (CRAN) is considered as an important paradigm to enhance the performance of the upcoming fifth generation (5G) and beyond wireless networks in terms of capacity, latency, and connectivity to a large number of devices. Out of several potential enablers, efficient resource allocation can mitigate various challenges related to user assignment, power allocation, and spectrum management in a CRAN, and is the focus of this paper. Herein, we provide a comprehensive review of resource allocation schemes in a CRAN along with a detailed optimization taxonomy on various aspects of resource allocation. More importantly, we identity and discuss the key elements for efficient resource allocation and management in CRAN, namely: user assignment, remote radio heads (RRH) selection, throughput maximization, spectrum management, network utility, and power allocation. Furthermore, we present emerging use-cases including heterogeneous CRAN, millimeter-wave CRAN, virtualized CRAN, Non-Orthogonal Multiple Access (NoMA)-based CRAN and full-duplex enabled CRAN to illustrate how their performance can be enhanced by adopting CRAN technology. We then classify and discuss objectives and constraints involved in CRAN-based 5G and beyond networks. Moreover, a detailed taxonomy of optimization methods and solution approaches with different objectives is presented and discussed. Finally, we conclude the paper with several open research issues and future directions.
Keywords: Cloud RAN; Optimization; Resource allocation; 5G and beyond networks

Wei-Che Chien, Shih-Yun Huang, Chin-Feng Lai, Han-Chieh Chao, M. Shamim Hossain, Ghulam Muhammad,
Multiple contents offloading mechanism in AI-enabled opportunistic networks,
Computer Communications,
Volume 155,
2020,
Pages 93-103,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.02.084.
(https://www.sciencedirect.com/science/article/pii/S0140366419314239)
Abstract: With the rapid growth of mobile devices and the emergence of 5G applications, the burden of cellular and the use of the licensed band have enormous challenges. In order to solve this problem, opportunity communication is regarded as a potential solution. It can use unlicensed bands to forward content to users under delay-tolerance constraints, as well as reduce cellular data traffic. Since opportunity communication is easily interrupted when User Equipment (UE) is moving, we adopt Artificial Intelligence (AI) to predict the location of the mobile UE. Then, the meta-heuristic algorithm is used to allocate multiple contents. In addition, deep learning-based methods almost need a lot of training time. Based on real-time requirements of the network, we propose AI-enabled opportunistic networks architecture, combined with Mobile Edge Computing (MEC) to implement edge AI applications. The simulation results show that the proposed multiple contents offloading mechanism can reduce cellular data traffic through UE location prediction and cache allocation.
Keywords: Opportunistic networks; MEC; Offloading; Content caching

Jose Nunez-Yanez, Neil Howard,
Energy-efficient neural networks with near-threshold processors and hardware accelerators,
Journal of Systems Architecture,
Volume 116,
2021,
102062,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102062.
(https://www.sciencedirect.com/science/article/pii/S1383762121000515)
Abstract: Hardware for energy-efficient AI has received significant attention over the last years with both start-ups and large corporations creating products that compete at different levels of performance and power consumption. The main objective of this hardware is to offer levels of efficiency and performance that cannot be obtained with general-purpose processors or graphics processing units. In parallel, innovative hardware techniques such as near- and sub-threshold voltage processing have been revisited, capitalizing on the low-power requirements of deploying AI at the network edge. In this paper, we evaluate recent developments in hardware for energy-efficient AI, focusing on inference in embedded systems at the network edge. We then explore a heterogeneous configuration that deploys a neural network that processes multiple independent inputs and deploys convolutional and LSTM (Long Short-Term Memory) layers. This heterogeneous configuration uses two devices with different performance/power characteristics connected with a feedback loop. It obtains energy reductions measured at 75% while simultaneously maintaining the level of inference accuracy.
Keywords: Sub-threshold processor; Energy efficient; Edge computing; Neural network; Heterogeneous computing

Juan Marcelo Parra-Ullauri, Hari Madhukumar, Adrian-Cristian Nicolaescu, Xunzheng Zhang, Anderson Bravalheri, Rasheed Hussain, Xenofon Vasilakos, Reza Nejabati, Dimitra Simeonidou,
kubeFlower: A privacy-preserving framework for Kubernetes-based federated learning in cloud–edge environments,
Future Generation Computer Systems,
Volume 157,
2024,
Pages 558-572,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.03.041.
(https://www.sciencedirect.com/science/article/pii/S0167739X24001134)
Abstract: Federated Learning (FL) enables collaborative model training across edge devices while preserving data locally. Deploying FL faces challenges due to device heterogeneity. Using cloud technologies like Kubernetes (K8s) can offer computational elasticity, yet may compromise FL privacy principles. K8s can jeopardise FL privacy by potentially allowing malicious FL clients to access other resources given its flat networking approach. This paper introduces the privacy-preserving K8s operator kubeFlower. It addresses privacy risks via isolation-by-design and differential privacy for data management. Isolation ensures secure resource sharing, while differential privacy safeguards individual data privacy. We introduce the Privacy Preserving Persistent Volume Claimer (P3-VC), which adds noise to data while managing a privacy budget. kubeFlower simplifies FL system management in K8s while ensuring privacy. We tested our approach on a network testbed composed of different geo-located cloud and edge nodes where FL clients are deployed. Our results demonstrate the approach’s efficacy in preserving privacy in K8s-based FL compared to benchmarks for cloud–edge environments.
Keywords: Federated learning; Cloud; Edge; Kubernetes; Networking; Privacy preservation

Soyi Jung, Jae-Hyun Kim, David Mohaisen, Joongheon Kim,
Truthful and performance-optimal computation outsourcing for aerial surveillance platforms via learning-based auction,
Computer Networks,
Volume 225,
2023,
109651,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109651.
(https://www.sciencedirect.com/science/article/pii/S1389128623000968)
Abstract: This paper proposes a novel truthful computing algorithm for learning task outsourcing decision-making strategies in edge-enabled unmanned aerial vehicle (UAV) networks. In our considered scenario, a single UAV performs face identification in a monitored target area. The execution of the identification requires a certain computing power, and its complexity and time are dependent on the number of faces in the recorded images. As a consequence, the task cannot be fully executed by a single UAV under high image arrivals or with images that have a high density of faces. In those conditions, UAV can outsource the task to one of the nearby edges. Importantly, the computing task distribution should be energy-efficient and delay-minimal due to the constraints imposed by the UAV platform characteristics and applications. Based on those fundamental requirements, our proposed algorithm conducts sequential decision-making for image sharing with one selected edge. The edge is selected based on a second price auction for truthfulness. Besides the truthfulness guarantees, deep learning based approximation for the auction solution is used for revenue-optimality. Our evaluation demonstrates that the proposed algorithm achieves the desired performance.
Keywords: Unmanned aerial networks (UAVs); Surveillance; Auction

Mohamed Said Frikha, Sonia Mettali Gammar, Abdelkader Lahmadi, Laurent Andrey,
Reinforcement and deep reinforcement learning for wireless Internet of Things: A survey,
Computer Communications,
Volume 178,
2021,
Pages 98-113,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.07.014.
(https://www.sciencedirect.com/science/article/pii/S0140366421002681)
Abstract: Nowadays, many research studies and industrial investigations have allowed the integration of the Internet of Things (IoT) in current and future networking applications by deploying a diversity of wireless-enabled devices ranging from smartphones, wearables, to sensors, drones, and connected vehicles. The growing number of IoT devices, the increasing complexity of IoT systems, and the large volume of generated data have made the monitoring and management of these networks extremely difficult. Numerous research papers have applied Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) techniques to overcome these difficulties by building IoT systems with effective and dynamic decision-making mechanisms, dealing with incomplete information related to their environments. The paper first reviews pre-existing surveys covering the application of RL and DRL techniques in IoT communication technologies and networking. The paper then analyzes the research papers that apply these techniques in wireless IoT to resolve issues related to routing, scheduling, resource allocation, dynamic spectrum access, energy, mobility, and caching. Finally, a discussion of the proposed approaches and their limits is followed by the identification of open issues to establish grounds for future research directions proposal.
Keywords: Internet of Things; Reinforcement learning; Deep reinforcement learning; Wireless Networks

Dawei Wei, Jianfeng Ma, Linbo Luo, Yunbo Wang, Lei He, Xinghua Li,
Computation offloading over multi-UAV MEC network: A distributed deep reinforcement learning approach,
Computer Networks,
Volume 199,
2021,
108439,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108439.
(https://www.sciencedirect.com/science/article/pii/S1389128621003984)
Abstract: Unmanned aerial vehicle (UAV)-assisted computation offloading allows mobile devices (MDs) to process computation-intensive and latency-sensitive tasks with limited or no-available infrastructures. To achieve long-term performance under changing environment, deep reinforcement-based methods have been applied to solve the UAV-assisted computation offloading problem. However, the deployment of multiple UAVs for computation offloading in mobile edge computing (MEC) network still faces the challenge of lacking flexible learning scheme to efficiently adjust computation offloading policy according to dynamic UAV mobility pattern and UAV failure. To this end, a distributed deep reinforcement learning (DRL)-based method with the cooperative exploring and prioritized experience replay (PER) is proposed in this paper. Our distributed exploring process achieves flexible learning scheme under UAV failure by allowing MDs to learning cost-efficient offloading policy cooperatively. Furthermore, PER allows MDs can explore the transitions with high TD-error, which can improve the performance under dynamic UAV mobility patterns. The efficiency of the proposed method is demonstrated by comparing with the existing computation offloading methods, and results show that the proposed method outperforms the compared methods in terms of convergence rate, energy-task efficiency and average processing time.
Keywords: Multi-UAV assisted MEC-enabled network; Computation offloading; Distributed reinforcement learning

Rihab Chaâri, Omar Cheikhrouhou, Anis Koubâa, Habib Youssef, Tuan Nguyen Gia,
Dynamic computation offloading for ground and flying robots: Taxonomy, state of art, and future directions,
Computer Science Review,
Volume 45,
2022,
100488,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2022.100488.
(https://www.sciencedirect.com/science/article/pii/S1574013722000296)
Abstract: The collaboration between ground and flying robots can take advantage of the capabilities of each system to enhance the performance of robotic applications, including military, healthcare, disaster management, etc. Unfortunately, this cooperation is restricted by the physical constraint that all computation should be performed on robots. The concept of computation offloading has great potential to improve the performance of both ground and flying robots. Unfortunately, external conditions like the network conditions, the robot’s mobility, and the availability of the processing resources may lead to new challenges. Accordingly, these challenges should be addressed from different perspectives, like security, network communication, response time, and energy consumption. Recently, most computation offloading solutions aim to optimize further the robot’s energy consumption. Several research works are designed 1.) to satisfy the real-time requirements of robotic applications and 2.) to solve the trade-off between the energy consumed by computation and communication. To better understand these concepts, we present a comprehensive overview of the computation offloading process for ground and flying robots. We also devised a taxonomy explaining the factors affecting the offloading decision in a robotic scenario. The taxonomy presents guidelines to recognize the scope of research in offloading decisions that were designed for robots. Then, we discuss the state-of-the-art techniques of computation offloading from an architectural point of view, and we survey works related to offloading decisions for robots.
Keywords: Computation offloading; Offloading decision; Robots; Optimization

Hans Jakob Damsgaard, Aleksandr Ometov, Md Munjure Mowla, Adam Flizikowski, Jari Nurmi,
Approximate computing in B5G and 6G wireless systems: A survey and future outlook,
Computer Networks,
Volume 233,
2023,
109872,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109872.
(https://www.sciencedirect.com/science/article/pii/S1389128623003171)
Abstract: As modern 5G systems are being deployed, researchers question whether they are sufficient for the oncoming decades of technological evolution. Growing numbers of interconnected intelligent devices put these networks under tremendous pressure, demanding their development. Paving the way for beyond 5G and 6G systems, commonly denoted by B5G herein, therefore means seeking enablers to increase efficiency from different perspectives. One novel look on this is the application of inexact computations where nine 9s reliability is not needed, for example, in non-critical mobile broadband traffic. The paradigm of Approximate Computing (AxC) focuses on such areas where constrained quality degradation results in savings that benefit the users and operators. This paper surveys the state-of-the-art publications on the intersection of AxC and B5G systems, identifying and emphasizing trends and tendencies in existing work and directions for future research. The work highlights resource allocation algorithms as particularly mesmerizing in the former, while research related to Intelligent Reflective Surfaces appears the most prominent in the latter. In both, problems are often NP-hard and, thus, only solvable using heuristics or approximations, Successive Convex Approximation and Reinforcement Learning are most frequently applied.
Keywords: Approximate computing; Beyond 5G; Mobile networks

János Czentye, Balázs Sonkoly,
Serverless application composition leveraging function fusion: Theory and algorithms,
Future Generation Computer Systems,
Volume 153,
2024,
Pages 403-418,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.12.010.
(https://www.sciencedirect.com/science/article/pii/S0167739X23004648)
Abstract: Serverless computing is a novel cloud computing paradigm enabling flexible, cost-efficient and fine-granular development of cloud-native applications, although without any guarantees on scheduling or execution times. Thus, various high-level solutions have been proposed in recent years to find proper configurations of individual FaaS functions, while considering user-given QoS requirements. However, the ever-increasing complexity of invocation patterns among stateless functions, externalized management of intermediate states, and diverse public cloud resources pose new challenges to the composition of highly data-intensive serverless applications. In this paper, we fill this gap by proposing novel algorithms based on the emerging function fusion technique, along with the related cost/performance models of composite functions supporting implicit instance parallelization and internal state propagation. We prove the NP-completeness of the underlying latency-constrained tree partitioning problem, and design a bicriteria approximation scheme and a greedy heuristic to derive cost-efficient deployment configurations in polynomial time. With the help of extensive simulations using synthetic call graphs generated from public cloud traces, we demonstrate the applicability and superior runtime performance of our proposed methods compared to state-of-the-art solutions. In addition, we showcase that further cost-reduction of up to 3–6 % can be achieved compared to the optimal partitioning with the allowance of tolerable latency violations.
Keywords: Serverless; FaaS; Cloud native; Function fusion; Tree partitioning

Diogo Lima, Hugo Miranda,
A geographical-aware state deployment service for Fog Computing,
Computer Networks,
Volume 216,
2022,
109208,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109208.
(https://www.sciencedirect.com/science/article/pii/S1389128622002948)
Abstract: Today’s distributed mobile applications assume a permanent network connectivity to mediate the interactions between a large number of users, coordinated by resourceful servers on cloud datacenters. Cloud-based distributed applications penalize performance with the unavoidable latency and jitter resulting from the geographical distance between clients and application servers. Fog Computing architectures mitigate this impact by deploying state fragments and providing computing power on surrogate servers, located at the network edge. Performance improves with the ability to deploy each fragment at the most convenient surrogate, and with efficient consistency procedures even when fragments at different locations are used. This paper presents a self-configuring geographical-aware state deployment service that combines a state deployment algorithm with a scalable distribution support framework.
Keywords: Mobile distributed applications; Fog Computing; Geographical state deployment; Consistency; Lightweight groups

Germán T. Eizaguirre, Marc Sánchez-Artigas,
A Seer knows best: Auto-tuned object storage shuffling for serverless analytics,
Journal of Parallel and Distributed Computing,
Volume 183,
2024,
104763,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104763.
(https://www.sciencedirect.com/science/article/pii/S0743731523001338)
Abstract: Serverless platforms offer high resource elasticity and pay-as-you-go billing, making them a compelling choice for data analytics. To craft a “pure” serverless solution, the common practice is to transfer intermediate data between serverless functions via serverless object storage (IBM COS; AWS S3). However, prior works have led to inconclusive results about the performance of object storage systems, since they have left large margin for optimization. To verify that object storage has been underrated, we devise a novel shuffle manager for serverless data analytics called Seer. Specifically, Seer dynamically chooses between two shuffle algorithms to maximize performance. The algorithm choice is made online based on some predictive models, and very importantly, without end users having to specify intermediate shuffle data sizes at the time of the job submission. We integrate Seer with PyWren-IBM [31], a well-known serverless analytics framework, and evaluate it against both serverful (e.g., Spark) and serverless systems (e.g., Google BigQuery, Caerus [46] and SONIC [22]). Our results certify that our new shuffle manager can deliver performance improvements over them.
Keywords: Serverless computing; Shuffle; I/O optimization; Object storage

Min Jia, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Shouzhen Gu,
Transient computing for energy harvesting systems: A survey,
Journal of Systems Architecture,
Volume 132,
2022,
102743,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102743.
(https://www.sciencedirect.com/science/article/pii/S1383762122002284)
Abstract: Battery-powered, ultra-low-power embedded devices are often limited by the size and maintenance costs of batteries, giving rise to battery-less devices and the emergence of energy harvesting systems. Energy harvesters obtain enough energy from the environment in order to satisfy program execution. However, the difference in the harvesting source and the size of the energy storage makes the program not execute continuously due to frequent interruptions due to power failures. Frequent power failures make the program lose volatile state, inconsistent data, and non-termination, so the energy harvesting system has to preserve the storage of volatile logic, maintain data consistency, and avoid non-termination. In this paper, we show the transient computing techniques for energy harvesting systems. We hope that this research will provide researchers with insights into transient computing and help them address the remaining challenges.
Keywords: Transient computing; Energy harvesting systems; Checkpoint; Atomic task; Idempotency

Wenlan Diao, Jianping An, Tong Li, Chao Zhu, Yu Zhang, Xiaotian Wang, Zhoujie Liu,
Low delay fragment forwarding in LEO satellite networks based on named data networking,
Computer Communications,
Volume 211,
2023,
Pages 216-228,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.08.022.
(https://www.sciencedirect.com/science/article/pii/S0140366423003080)
Abstract: The LEO satellite network with cache and computing capabilities has become an important evolution of the next generation (6G) mobile communication network. The satellite networks employing Named Data Networking (NDN) can achieve swift content access through in-network caches and data multicast. The data packet is the basic unit of transmission in NDN. In order to adapt to the limited MTU of links, large data packets must be transmitted in fragments. However, due to the issues such as large transmission delay, redundant reassembly, and infeasible fragment reuse, the existing fragmentation methods in NDN cannot be applied to satellite communication. In this paper, we propose a low delay fragment forwarding method in NDN: Direct Forwarding and Reuse of Fragments (DFRF). Specifically, the content in the large data packet will be split and packaged into fragments that conform to NDN semantics. Once the fragment is received by the satellite router, it can be forwarded directly without reassembly, thus saving the forwarding delay. In addition, we design a new fragment format and the list-based cache, so that all fragments generated from the same data packet can be cached intact in a list, and each fragment can be reused as a complete packet in the content store (CS). The proposed approach has been further evaluated and the simulation results show that compared with the NDN Link Protocol (NDNLP), using the DFRF method, the end-to-end transmission delay of large content is reduced by up to 51%, and the content acquisition delay is reduced by up to 60% through fragment reuse.
Keywords: LEO satellite network; Named data networking; Direct fragment forwarding; List-based cache; Fragment reuse

Maxime Gonthier, Loris Marchal, Samuel Thibault,
Taming data locality for task scheduling under memory constraint in runtime systems,
Future Generation Computer Systems,
Volume 143,
2023,
Pages 305-321,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.01.024.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000328)
Abstract: A now-classical way of meeting the increasing demand for computing speed by HPC applications is the use of GPUs and/or other accelerators. Such accelerators have their own memory, which is usually quite limited, and are connected to the main memory through a bus with bounded bandwidth. Thus, particular care should be devoted to data locality in order to avoid unnecessary data movements. Task-based runtime schedulers have emerged as a convenient and efficient way to use such heterogeneous platforms. When processing an application, the scheduler has the knowledge of all tasks available for processing on a GPU, as well as their input data dependencies. Hence, it is possible to produce a tasks processing order aiming at reducing the total processing time through three objectives: minimizing data transfers, overlapping transfers and computation and optimizing the eviction of previously-loaded data. In this paper, we focus on how to schedule tasks that share some of their input data (but are otherwise independent) on a single GPU. We provide a formal model of the problem, exhibit an optimal eviction strategy, and show that ordering tasks to minimize data movement is NP-complete. We review and adapt existing ordering strategies to this problem, and propose a new one based on task aggregation. We prove that the underlying problem of this new strategy is NP-complete, and prove the reasonable complexity of our proposed heuristic. These strategies have been implemented in the StarPU runtime system. We present their performance on tasks from tiled 2D, 3D matrix products, Cholesky factorization, randomized task order, randomized data pairs from the 2D matrix product as well as a sparse matrix product. We introduce a visual way to understand these performance and lower bounds on the number of data loads for the 2D and 3D matrix products. Our experiments demonstrate that using our new strategy together with the optimal eviction policy reduces the amount of data movement as well as the total processing time.
Keywords: Memory-aware scheduling; Eviction policy; Tasks sharing data; GPUs; Runtime systems; Memory constraint

M.R. Nived, Vinayak Eswaran,
A massively parallel implicit 3D unstructured grid solver for computing turbulent flows on latest distributed memory computational architectures,
Journal of Parallel and Distributed Computing,
Volume 182,
2023,
104750,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104750.
(https://www.sciencedirect.com/science/article/pii/S074373152300120X)
Abstract: An implicit unstructured grid density-based solver–PRAVAHA based on a parallel variant of the lower upper symmetric Gauss-Seidel (LUSGS) method is developed to compute large-scale engineering problems. A four-layered parallel algorithm is designed to efficiently compute three-dimensional turbulent flows on massively parallel modern multiple instruction multiple data-stream (MIMD) computational hardware. This data parallel approach achieves multiple layers of parallelism including continuity of flow solution, transfer of solution gradients, and calculation of drag/lift/solution residuals, right up to the innermost implicit LUSGS solver sub-routine, which is relatively less explored in the literature. Domain decomposition is performed using the METIS software based on multi-level graph partitioning algorithms. Non-blocking message-passing interface library functions are used to manage inter-processor communication through explicit message passing, efficiently. Super-linear scalability of the parallel solver is established on the current state-of-the-art supercomputing facility, the 838 teraflops PARAM seva on up to 6144 cores. Linear or even super-linear speedup on problems of significant size is observed even on ad-hoc parallel computing platforms like workstations and multi-node clusters, for turbulent flow simulations.
Keywords: Parallel code; Unstructured grid; Message passing; Scalability; Implicit LUSGS

Amartya Mukherjee, Nilanjan Dey, Debashis De,
EdgeDrone: QoS aware MQTT middleware for mobile edge computing in opportunistic Internet of Drone Things,
Computer Communications,
Volume 152,
2020,
Pages 93-108,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.039.
(https://www.sciencedirect.com/science/article/pii/S0140366419315750)
Abstract: Internet of Things is a crucial research empire in the current era which enables a wide diversity of applications. One of the emerging classes of technology in recent age dominate the universe of IoT is the concept of the Internet of Drones Thing (IoDT). The following work fundamentally emphasizes the key concepts of the Internet of Drones architecture in the context of the Edge computing perspective. A state of the art “EdgeDrone” concept has been engineered where the standard message transfer strategy of IoT and the opportunistic Ad-Hoc network have been amalgamated. Parameters like message transfer latency, overhead ratio, message delivery chances under several QoS values and resource usage, energy performance for the protocol have been analyzed. A hardware testbed has been implemented with the JavaScript platform along with the simulation which has been executed after tuning the performance of the protocols. The result shows 30% improvement in average packet delivery for enhanced MQTT-SN and about 17% improvement for enhanced MQTT in comparing to standard strategy. The study also reveals the minimum value of the message delivery latency of 15.5 msec for enhanced MQTT-SN in QoS-1. In memory usage point of view, the result shows that the MQTT broker takes a maximum of 5.6 MB of memory during enhanced MQTT-SN takes to publish–subscribe operation which is significantly less. Another two major parameters which report the maximum message transfer bandwidth of 620 msg/sec for enhanced MQTT-SN at QoS 1 and about 1800 msg/sec for QoS 0 as well as the enhanced MQTT-SN within 802.15.4 interface ensures minimum energy consumption amongst all other strategies
Keywords: IoDT; EdgeDrone; QoS; Latency; Energy; Publish time

Ayaz Ali Khan, Muhammad Zakarya,
Energy, performance and cost efficient cloud datacentres: A survey,
Computer Science Review,
Volume 40,
2021,
100390,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100390.
(https://www.sciencedirect.com/science/article/pii/S1574013721000307)
Abstract: In major Information Technology (IT) companies such as Google, Rackspace and Amazon Web Services (AWS), virtualization and containerization technologies are usually used to execute customers’ workloads and applications — as part of their cloud computing services offering. The computational resources are provided through large-scale datacentres, which consume substantial amount of energy and, consequently, affect our environment with global warming. Cloud datacentres have become a backbone for today’s business and economy, which are the fastest-growing electricity consumers, globally. Numerous studies suggest that ∼30% of the US datacentres are comatose and the others are grossly less-utilized, which make it possible to save energy through technologies like virtualization and containerization. These technologies provide support for allocation and consolidation of workloads on appropriate resources. However, consolidation comprises migrations of virtual machines (VMs), containers and/or applications, depending on the underlying virtualization method; that are expensive in terms of energy consumption, performance degradation, and therefore, costs which is mostly not accounted for in many existing models, and, possibly, it could be more energy and performance efficient not to consolidate. This paper describes energy consumption and performance, therefore, cost issues of large-scale datacentres. Besides, we cover various methods for energy and performance efficient distributed systems, clouds and datacentres. We elaborate energy efficiency methods at three different levels: hardware; resource management; and applications. Besides these, different performance management techniques are mapped onto taxonomies and described in details. In last, energy, performance and cost management techniques, at geographically distributed and multi-access edge computing platforms, are described along with critical discussion.
Keywords: Clouds; Datacentres; Resource management; Energy efficiency; Performance

Zahra Jalali Khalil Abadi, Najme Mansouri, Mahshid Khalouie,
Task scheduling in fog environment — Challenges, tools & methodologies: A review,
Computer Science Review,
Volume 48,
2023,
100550,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100550.
(https://www.sciencedirect.com/science/article/pii/S1574013723000175)
Abstract: Even though cloud computing offers many advantages, it can be a poor choice sometimes because of its slow response to existing requests, leading to the need for fog computing. Scheduling tasks in a fog environment is a major challenge. It is important that IoT clients execute their tasks in a timely manner and obtain lower-cost services; however, they are also looking for tasks to be executed in a secure manner. In this paper, we review the advantages, limitations, and issues associated with scheduling algorithms proposed by a number of different researchers for fog environments. For fog computing developers, we compare different simulation tools to help them choose the product that is most appropriate and flexible for simulating the application they are considering. Finally, open issues and promising research directions associated with task scheduling in fog computing are discussed.
Keywords: Fog computing; Cloud computing; Task scheduling; Literature review

Hameedur Rahman, Uzair Muzamil Shah, Syed Morsleen Riaz, Kashif Kifayat, Syed Atif Moqurrab, Joon Yoo,
Digital twin framework for smart greenhouse management using next-gen mobile networks and machine learning,
Future Generation Computer Systems,
Volume 156,
2024,
Pages 285-300,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.03.023.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000931)
Abstract: Due to the increase in world population, arable land has been reduced. Consequently, the concept of urban greenhouses is on the rise. Smart greenhouses need to monitor physical parameters for the healthy growth of plants from remote locations. A digital twin is a representation of physical assets in the digital world, and this emerging technology has opened up opportunities for efficient system development for Industry 4.0. The digital twin receives real-time operational data to monitor the asset in the digital domain. It performs real-time processing, data analysis, and machine learning to predict optimized decisions. In the era of next-generation mobile networks, IoT devices can communicate and perform their remote operations in a timely manner. In smart greenhouse technology, the digital twin could be a revolutionary substitute for real-time remote monitoring and process management. However, there has been limited work on digital twin-driven smart greenhouse technology. In this paper, a process management framework is developed that can be interpreted as a machine learning and cloud-based data-driven digital twin for smart greenhouses. The proposed framework consists of three layers: the physical, fog, and cloud layers. The physical greenhouse measurements are monitored using a highly immersive cloud-based, real-time 3D environment. We present an example architecture using commercial cloud and open-source tools to verify the proof of concept. Additionally, different ML techniques are utilized to predict the operational requirements for smart greenhouses.
Keywords: IoT; Smart greenhouse; Digital twin; Machine learning; NGMN; Fog layer

Federica Paganelli, Paola Cappanera, Giovanni Cuffaro,
Tenant-defined service function chaining in a multi-site network slice,
Future Generation Computer Systems,
Volume 121,
2021,
Pages 1-18,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.03.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X21000820)
Abstract: Service Function Chaining (SFC) enables dynamic and adaptive network service provisioning. However, virtual infrastructure providers are reluctant to disclose SFC APIs and infrastructure monitoring information to tenants, therefore undermining tenants’ capability in dynamically and flexibly managing application-tailored network services. In this work, we provide a two-fold contribution to the problem of tenant-defined service function chaining on a multi-site virtual infrastructure, which has not been widely investigated yet. First, we propose a VNF Selection algorithm that relies on an abstracted network model thus minimizing topological and monitoring information required by the infrastructure. Our algorithm selects a set of VNF instances that minimizes the end-to-end latency (accounting for network and processing delay) and also guarantees that already established chains do not violate their maximum latency constraint due to the additional load at VNFs brought by new chain requests. Second, we describe an SFC solution composed of an SFC application that uses our VNF Selection algorithm and a forwarding mechanism that manages chains and controls traffic steering within the tenant network without leveraging infrastructure control APIs. Validation and comparative evaluations are performed through simulations. Finally, the proposed approach is assessed through a set of experiments carried out on a multi-site testbed infrastructure.
Keywords: Network Function Virtualization; Service Function Chaining; Optimization; Tenant network; Software Defined Networking; Intent interface

Rui He, Bangbang Ren, Junjie Xie, Deke Guo, Yuwen Zhou, Laiping Zhao, Yong Li,
A reinforcement learning method for scheduling service function chains with multi-resource constraints,
Computer Networks,
Volume 235,
2023,
109985,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109985.
(https://www.sciencedirect.com/science/article/pii/S1389128623004309)
Abstract: Traditional networks are usually equipped with many dedicated middleboxes to provide various network services. Though these hardware-based devices certainly improve network performance, they are usually expensive and difficult to upgrade. To overcome this shortcoming, network function virtualization (NFV), which accomplishes network services in the form of virtual network functions (VNF) has been presented. Compared to middleboxes, the VNFs are easy to deploy and migrate. Usually, multiple VNFs are chained in a specified order as a service function chain (SFC) to serve a given flow. There are many works to schedule SFCs to minimize the average flow completion time. However, they only consider single resource limitation. In this paper, we are committed to addressing the problem of multi-resource SFC scheduling (MR-SFCS) and minimizing the average flow completion time. We formulate this problem with an Integer Linear Programming (ILP) model and prove its NP-hardness. To well tackle this problem, we propose an approach based on deep reinforcement learning (DRL), which has specific reward design and state representations. Besides, we extend the offline approach to online SFC scheduling. The experiment results demonstrate that our DRL method can significantly reduce the average flow completion time and achieves a cost saving of 69.07% against the benchmark method.
Keywords: Service function chain; Deep reinforcement learning; Scheduling

Xing Liu, Qi Wang, Chengming Zou, Mei Yu, Denghong Liao,
Edge intelligence for smart airport runway: Architectures and enabling technologies,
Computer Communications,
Volume 195,
2022,
Pages 323-333,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.09.003.
(https://www.sciencedirect.com/science/article/pii/S0140366422003437)
Abstract: To improve the management efficiency and reduce the operational risks, a modern airport should not only be able to track the vehicles driving on the runway, but also be capable of detecting the deformation on the runway. To build such a smart airport, this paper takes measures from four perspectives: data, algorithm, computing power and platform. In terms of data, the optic fiber sensors (OFSs) rather than the traditional electromechanical sensors are used to collect the airport environmental data (AED). Compared with the electromechanical sensors, OFSs are cheaper, more reliable, and easier to be deployed. In terms of algorithms, the intelligent algorithms such as Convolutional Neural Networks (CNN), fast Fourier transform (FFT) and K-means are applied to analyze the AED. Compared with the traditional algorithms which detect the vehicle traces and runway deformation directly by signal pressure and amplitude, these algorithms are more precise and adaptable. In terms of computing power, the domain-specific architecture (DSA) technique is applied to increase the computing performance while keeping high energy efficiency. By designing several specific FPGA accelerators dedicated to the algorithms, the large quantity of AEDs can be processed quickly in real time. In terms of platform, a real-world edge-cloud collaborative platform based on the improved KubeEdge and Huawei openLooKeng is built. This platform can provide low-latency and high-performance computing, as well as data fusion for the AED processing in the airport. The work of this article has been practically applied to the Ezhou Huahu International Airport, and the real-world experimental results show that the proposed approaches have high detection accuracy, real-time data processing capability, low cost and also high energy efficiency.
Keywords: Edge intelligence; Airport; Optic fiber sensors; Real-time; Energy efficiency

Ali Gohar, Gianfranco Nencioni,
An online cost minimization of the slice broker based on deep reinforcement learning,
Computer Networks,
Volume 241,
2024,
110198,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110198.
(https://www.sciencedirect.com/science/article/pii/S1389128624000306)
Abstract: The fifth generation of mobile networks can provide differentiated services enabled by network slicing and multi-access edge computing. A new business actor, called Slice Broker (SB), is emerging as an intermediate entity that buys networking and computing resources from Infrastructure Providers (InPs) and provides network slices to Slice Tenants (STs). This paper addresses the problem of online jointly allocating all the network slices requested by the STs and selecting the resources to purchase from the InPs. The target of the problem is the minimization of the SB costs. We assume that a network slice is implemented as a sequence of virtual network functions and that the InPs sell the networking and computing resources on predetermined configurations. The addressed problem is solved by using deep reinforcement learning with a model-free policy gradient-based algorithm. The proposed solution is evaluated and compared with benchmark solutions in various scenarios. The results show that the benchmark solutions have a cost that is from 10% to more than twice higher than the proposed solution in all scenarios.
Keywords: Cost minimization; Slice Broker; Multi-access Edge Computing; Network slicing; Deep Reinforcement Learning

Fenghua Li, Yongjun Li, Siyuan Leng, Yunchuan Guo, Kui Geng, Zhen Wang, Liang Fang,
Dynamic countermeasures selection for multi-path attacks,
Computers & Security,
Volume 97,
2020,
101927,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2020.101927.
(https://www.sciencedirect.com/science/article/pii/S0167404820302030)
Abstract: Multi-step attacks have been widely adopted by attackers, resulting in privacy leakage. Although many cost-sensitive approaches have been proposed to respond to the multi-step attacks, most studies have lack global optimization and ignored the fact that attackers may take multiple paths to launch multi-step attacks, which may lead to an over-response or an under-response. To address this problem, we formulate a response to multi-path attacks as an optimization problem and prove it is NP-hard. To obtain a feasible solution to the problem, we first identify suspicious attack paths and evaluate several metrics (i.e., security benefit, deployment cost, and negative impact on the quality of services) of the countermeasures. Specifically, by considering the compositions and cover degrees of atomic attacks, we define Attacks Surface Coverage to accurately evaluate the security benefit of countermeasures. Then, we propose an improved greedy algorithm to select reasonable countermeasures. Experimental results demonstrate the effectiveness and feasibility of our approach.
Keywords: Intrusion response; Countermeasures selection; Multi-path attack; Probabilistic attack-response tree; Attacks surface coverage

Ge Wang, Fangmin Xu, Hengsheng Zhang, Chenglin Zhao,
Joint resource management for mobility supported federated learning in Internet of Vehicles,
Future Generation Computer Systems,
Volume 129,
2022,
Pages 199-211,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.11.020.
(https://www.sciencedirect.com/science/article/pii/S0167739X2100457X)
Abstract: In recent years, the powerful combination of Multi-access Edge Computing (MEC) and Artificial Intelligence (AI), called edge intelligence, promotes the development of Intelligent Transportation Systems (ITS). However, there is a mismatch between the ever-increasing consumer privacy awareness and the data leakage risk in centralized AI training solutions in vehicular edge scenarios, which has become a new obstacle to satisfying the user experience. As a promising privacy-preserving paradigm, federated learning synthesizes a global model only with the parameters of decentralized trained local models, avoiding the exposure of sensitive data. Given this, we introduce federated learning into the proposed two-level MEC-assisted vehicular network framework. This paper aims to address the challenges posed by adopting federated learning into the Internet of Vehicles (IoV) scenario. Firstly, as the entity of the participant (the local model training node of federated learning), vehicles have high mobility. We design a mobility supported federated learning participant decision algorithm to pick out participants from candidate vehicles. Secondly, federated learning is rather resource-consuming, inevitably incurring considerable costs to participants. We focus on the joint resource allocation problem to optimize the federated learning cost. Finally, considering the limitations of centralized resource allocation, we propose a fully distributed resource allocation method inspired by multi-agent deep reinforcement learning. Simulation results are presented to demonstrate the feasibility and effectiveness of the proposed schemes.
Keywords: Internet of Vehicles; Federated learning; Multi-access Edge Computing; Distributed dynamic resource management; Multi-agent deep reinforcement learning

Yongsheng Hao, Qi Wang, Tinghuai Ma, Jinglin Du, Jie Cao,
Energy allocation and task scheduling in edge devices based on forecast solar energy with meteorological information,
Journal of Parallel and Distributed Computing,
Volume 177,
2023,
Pages 171-181,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.03.005.
(https://www.sciencedirect.com/science/article/pii/S0743731523000412)
Abstract: Offloading tasks from edge devices to the cloud is an important method to enhance the performance of the edge device. With the help of EH (Energy Harvesting) technology, the edge device can use the collected green energy to support its operations. Most offloading scheduling methods use as much green energy as the edge device collected. Unlike prior research, we consider the long-term benefits of energy. In this paper, we forecast the solar energy supply with meteorological methods which are based on the weather forecast data. Then, we use quadratic programming to allocate energy based on the forecast energy to maximize energy efficiency. Finally, we use NSGA (Non-dominated Sorting Genetic Algorithms) to offload tasks in the edge device. Simulations show that our proposed method not only minimizes the execution time and the energy consumption of clouds, but also enhances the QoE of users.
Keywords: Edge device; Offloading task; Weather forecast; NGSA

Zina Chkirbene, Ridha Hamila, Aiman Erbad, Serkan Kiranyaz, Nasser Al-Emadi,
D2DLive: Iterative live video streaming algorithm for D2D networks,
Computer Networks,
Volume 229,
2023,
109734,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109734.
(https://www.sciencedirect.com/science/article/pii/S1389128623001792)
Abstract: Smart healthcare system incorporates new technologies making healthcare more efficient and more convenient. Virtual consultations provide interactions with healthcare professionals via video technology, which lessens the number of patients visiting health facilities and consequently reduces the risk of infections. Recently, device to device (D2D) communication showed extraordinary abilities to save the available resources and improve the network quality of service (QoS). Some of the existing algorithms for multimedia services over D2D networks consider only the signal to noise ratio (SNR) and ignore temporal requirements, which do not provide optimum performances. So, more efficient transmission mechanisms that improve the users’ QoS are needed to respond quickly to the ever-changing application demands. In response to these challenges, the research community began exploring novel solutions for video streaming, namely: Quantile-based Carrier-sense multiple access (CSMA), Flexible video transmission (Flexi) and Distributed Random approach (DR). However, these solutions introduce higher throughput, and jitter, especially in the case of live streaming. Other solutions are also available in the literature, but most of them cannot efficiently scale to large multi videos, multi-hops multi-user live video streaming. In particular, in most of these techniques, the chunk number, node transmission capacity or specific video delay, and time requirements are not taken into consideration in the path optimization.To overcome these shortcomings, we propose a novel D2DLive video streaming algorithm as a new solution for a higher QoS to improve medical data delivery. For each requested video, the proposed algorithm divides its content into small playable units called chunks that are transmitted using a new network selection algorithm. The proposed algorithm computes iteratively a suboptimal set of paths to be used to forward all the video chunks cooperatively to their destinations. Furthermore, we develop a new weight algorithm to evaluate the performance of the proposed algorithm in adhering to the upload and download capacities for each device while minimizing the transmission delay. Simulation results show that the proposed algorithm outperforms other established methods in terms of full delay, average upload, and download capacities.
Keywords: Smart healthcare; Virtual consultations; Video live transmission; D2D; Upload and download capacities

Guolin Sun, Daniel Ayepah-Mensah, Li Lu, Wei Jiang, Guisong Liu,
Delay-aware content distribution via cell clustering and content placement for multiple tenants,
Journal of Network and Computer Applications,
Volume 137,
2019,
Pages 112-126,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.04.004.
(https://www.sciencedirect.com/science/article/pii/S108480451930116X)
Abstract: The introduction of 5G will see exponential growth in the amount of data generated in mobile networks. This huge growth in data volume will put great pressure on not only the wireless access network but also the backhaul. In-network caching as a key component of 5G targets faster download speeds and reduction in latency through efficient content placement to avoid contents being transmitted repeatedly. In addition, the reduction in latency will require an effective resource allocation scheme to improve radio resource utilization. This paper investigates the problem of delay-aware content distribution in a multi-tenant network. We propose a content placement scheme to minimize the average visiting time of all users and a novel heuristic graph-partitioning algorithm via cell clustering to maximize the user transmission rates. Finally, simulations are conducted to evaluate the proposed scheme with QoE satisfaction and resource utilization for multi-tenants.
Keywords: Network slicing; Caching virtualization; Cell clustering; Content placement

Miaojiang Chen, Tian Wang, Shaobo Zhang, Anfeng Liu,
Deep reinforcement learning for computation offloading in mobile edge computing environment,
Computer Communications,
Volume 175,
2021,
Pages 1-12,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.04.028.
(https://www.sciencedirect.com/science/article/pii/S0140366421001729)
Abstract: Recently, in order to distribute computing, networking resources, services, near terminals, mobile fog is gradually becoming the mobile edge computing (MEC) paradigm. In a mobile fog environment, the quality of service affected by offloading speeds and the fog processing, however the traditional fog method to solve the problem of computation resources allocation is difficult because of the complex network states distribution environment (that is, F-AP states, AP states, mobile device states and code block states). In this paper, to improve the fog resource provisioning performance of mobile devices, the learning-based mobile fog scheme with deep deterministic policy gradient (DDPG) algorithm is proposed. An offloading block pulsating discrete event system is modeled as a Markov Decision Processes (MDPs), which can realize the offloading computing without knowing the transition probabilities among different network states. Furthermore, the DDPG algorithm is used to solve the issue of state spaces explosion and learn an optimal offloading policy on distributed mobile fog computing. The simulation results show that our proposed scheme achieves 20%, 37%, 46% improvement on related performance compared with the policy gradient (PG), deterministic policy gradient (DPG) and actor–critic (AC) methods. Besides, compared with the traditional fog provisioning scheme, our scheme shows better cost performance of fog resource provisioning under different locations number and different task arrival rates.
Keywords: Internet of things (IoT); Reinforcement learning; Markov decision process; Computation offloading; Deep learning; Mobile edge computing

Anitha P., H.S. Vimala, Shreyas J.,
Comprehensive review on congestion detection, alleviation, and control for IoT networks,
Journal of Network and Computer Applications,
Volume 221,
2024,
103749,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103749.
(https://www.sciencedirect.com/science/article/pii/S1084804523001686)
Abstract: Context:
The Internet of Things (IoT) comprises various computing devices that operate on a non-standard platform and can connect to wireless networks to transmit data. These devices typically have limited storage capacity, restricted network bandwidth, and a lower level of computing power, which can cause congestion in the network. Hence, it is crucial to have a congestion control mechanism in place to facilitate efficient data transfer in IoT networks.
Objective:
To address congestion in the IoT, this research attempts to offer an overview of several congestion detections, avoidance, and control-based routing protocol techniques.
Method:
A systematic mapping study was carried out to pinpoint relevant literature. From this process, 102 publications were identified as the most relevant studies of congestion detection, congestion avoidance, congestion control, routing protocol, congestion control in 6LoWPAN, and learning-based congestion control.
Results:
Most relevant articles are clustered based on congestion detection (10%), congestion avoidance (12%), congestion control (23%), avoiding congestion through routing protocol (14%), congestion control in 6LoWPAN (19%), and controlling the congestion through learning based methods (24%).
Conclusion:
Congestion control is necessary for IoT to maintain network stability, reliability, and performance. It helps to ensure that critical applications can operate seamlessly and that IoT devices can communicate efficiently without overwhelming the network. Congestion control algorithms ensure that the network operates within its capacity, preventing network overload and maintaining network performance.
Keywords: IoT; Congestion; Routing protocol; 6LoWPAN; IoT simulators; Learning-based congestion control

Luning Liu, Zhaoming Lu, Luhan Wang, Xin Chen, Xiangming Wen,
Large-volume data dissemination for cellular-assisted automated driving with edge intelligence,
Journal of Network and Computer Applications,
Volume 155,
2020,
102535,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102535.
(https://www.sciencedirect.com/science/article/pii/S1084804520300096)
Abstract: Automated driving emerges as a potential technology for safe, convenient, and efficient transportation systems. In order to assist autonomous vehicles with large-volume data dissemination such as high definition (HD) map, cellular network becomes more inevitable to provide vehicles with the ability of accessing Internet. However, due to the bottleneck of wireless bandwidth and the high mobility of vehicles, efficient data dissemination assisted by cellular network is a challenging task. In this work, we investigate the large-volume data dissemination problem for cellular-assisted automated driving within given delay limit. Firstly, a data dissemination scheme is designed that transmits data from the base stations to the target vehicle via a collection of relay vehicles. Secondly, a novel user satisfaction model is derived by considering the vehicular mobility patterns and erasure coding, and the data dissemination is formulated into an optimization problem. Then, to solve this NP-hard problem, a low-complexity dynamic programming based data dissemination solution with edge intelligence is proposed. Finally, by carrying out extensive simulation, the effectiveness of proposed data dissemination scheme for cellular-assisted automated driving is demonstrated, in comparison with the optimal solution based on CPLEX as well as other conventional schemes.
Keywords: Cellular-assisted automated driving; Edge computing; Data dissemination; Relay vehicles

Xiaojiang Zuo, Yaxin Luopan, Rui Han, Qinglong Zhang, Chi Harold Liu, Guoren Wang, Lydia Y. Chen,
FedViT: Federated continual learning of vision transformer at edge,
Future Generation Computer Systems,
Volume 154,
2024,
Pages 1-15,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.11.038.
(https://www.sciencedirect.com/science/article/pii/S0167739X23004879)
Abstract: Deep Neural Networks (DNNs) have been ubiquitously adopted in internet of things and are becoming an integral part of our daily life. When tackling the evolving learning tasks in real world, such as classifying different types of objects, DNNs face the challenge to continually retrain themselves according to the tasks on different edge devices. Federated continual learning (FCL) is a promising technique that offers partial solutions but yet to overcome the following difficulties: the significant accuracy loss due to the limited on-device processing, the negative knowledge transfer caused by the limited communication of non-IID (non-Independent and Identically Distributed) data, and the limited scalability on the tasks and edge devices. Moreover, existing FCL techniques are designed for convolutional neural networks (CNNs), which have not utilized the full potential of newly emerged powerful vision transformers (ViTs). Considering ViTs depend heavily on training data diversity and volume, we hypothesize ViTs are well-suited for FCL where data arrives continually. In this paper, we propose FedViT, an accurate and scalable federated continual learning framework for ViT models, via a novel concept of signature task knowledge. FedViT is a client-side solution that continuously extracts and integrates the knowledge of signature tasks which are highly influenced by the current task. Each client of FedViT is composed of a knowledge extractor, a gradient restorer and, most importantly, a gradient integrator. Upon training for a new task, the gradient integrator ensures the prevention of catastrophic forgetting and mitigation of negative knowledge transfer by effectively combining signature tasks identified from the past local tasks and other clients’ current tasks through the global model. We implement FedViT in PyTorch and extensively evaluate it against state-of-the-art techniques using popular federated continual learning benchmarks. Extensive evaluation results on heterogeneous edge devices show that FedViT improves model accuracy by 88.61% without increasing model training time, reduces communication cost by 61.55%, and achieves more improvements under difficult scenarios such as large numbers of tasks or clients, and training different complex ViT models.
Keywords: Catastrophic forgetting; Continual learning; Edge computing; Federated learning; Knowledge transfer negative; Vision transformer

Malte Brunn, Naveen Himthani, George Biros, Miriam Mehl, Andreas Mang,
Fast GPU 3D diffeomorphic image registration,
Journal of Parallel and Distributed Computing,
Volume 149,
2021,
Pages 149-162,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.11.006.
(https://www.sciencedirect.com/science/article/pii/S074373152030407X)
Abstract: 3D image registration is one of the most fundamental and computationally expensive operations in medical image analysis. Here, we present a mixed-precision, Gauss–Newton–Krylov solver for diffeomorphic registration of two images. Our work extends the publicly available CLAIRE library to GPU architectures. Despite the importance of image registration, only a few implementations of large deformation diffeomorphic registration packages support GPUs. Our contributions are new algorithms to significantly reduce the run time of the two main computational kernels in CLAIRE: calculation of derivatives and scattered-data interpolation. We deploy (i) highly-optimized, mixed-precision GPU-kernels for the evaluation of scattered-data interpolation, (ii) replace Fast-Fourier-Transform (FFT)-based first-order derivatives with optimized 8th-order finite differences, and (iii) compare with state-of-the-art CPU and GPU implementations. As a highlight, we demonstrate that we can register 2563 clinical images in less than 6 s on a single NVIDIA Tesla V100. This amounts to over 20× speed-up over the current version of CLAIRE and over 30× speed-up over existing GPU implementations.
Keywords: GPU computing; Parallel optimization; Diffeomorphic image registration; Mixed-precision solver; Gauss–Newton–Krylov method

Chunlin Li, Chengyi Wang, Hengliang Tang, Youlong Luo,
Scalable and dynamic replica consistency maintenance for edge-cloud system,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 590-604,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.05.014.
(https://www.sciencedirect.com/science/article/pii/S0167739X18331558)
Abstract: With the increasing number of edge devices in the Internet of things (IoT), data storage in edge cloud environment better meet the real-time requirements of IoT. In order to improve the data reliability, reduce the overhead and the response latency, and make more reasonable use of storage resources in edge cloud environment, we conduct an in-depth study on the replica creation strategy and consistency maintenance strategy. In order to reduce the waste of storage resources, a dynamic replica creation strategy based on block popularity and node load (DRC-BN) is proposed. DRC-BN strategy considers the storage cost of edge cloud and the communication cost between central cloud and edge cloud when optimizing the number of replicas, and improves the data reliability. In order to solve the problem of data inconsistency caused by multi-user concurrency, a replica consistency maintenance strategy based on Fast Paxos algorithm (RC-FP) is proposed. RC-FP strategy considers the comprehensive performance of nodes and selects the node with better performance as the leader according to the comprehensive performance of nodes, and reduces the time of consistency maintenance. Experimental results show that the proposed strategies perform well in terms of the consistency maintenance time and the node throughput. At the same time, the proposed strategies also reduce the overall overhead.
Keywords: Edge-cloud; Consistency maintenance; Latency-sensitive application

Pronaya Bhattacharya, Arpit Shukla, Sudeep Tanwar, Neeraj Kumar, Ravi Sharma,
6Blocks: 6G-enabled trust management scheme for decentralized autonomous vehicles,
Computer Communications,
Volume 191,
2022,
Pages 53-68,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.04.024.
(https://www.sciencedirect.com/science/article/pii/S0140366422001360)
Abstract: In this paper, we propose a 6G-envisioned blockchain (BC)-based scheme for cellular vehicle-to-everything (C-V2X) ecosystems (termed as 6Blocks), which optimizes the edge-resource provisioning of 5G C-V2X nodes via network function virtualization (NFV)-enabled infrastructure. 6Blocks operates in three phases. In the first phase, a secure data aggregation through 6G sensors is performed at the data plane. Then, in the second phase, the aggregated data is sent to the 6G-enabled NFV control plane to leverage resource allocation to connected autonomous smart vehicles (CASVs) through edge nodes via service container operations. In the third phase, management plane operations are designed that includes registration and auction smart contracts (SC) for connected autonomous vehicle owner (CAVO) and CASVs over the BC with public/private pairs fetched from interplanetary file systems (IPFS). The proposed scheme is compared against existing NFV schemes like distant, edge, and SDN-based edge control. At 20 CASVs, 6Blocks achieves an edge-service latency of 0.05ms against 0.76ms, 0.27, and 0.15ms for cloud, edge, and SDN-edge schemes, respectively, with high connection density of 107 nodes, at maximum utilized IPFS bandwidth of 0.2 Mbps. Then, we validate the trust of our model through the automated validation of the internet security protocol and applications (AVISPA) tool and modeled the vehicle and sensor nodes. The obtained results show the efficacy of the proposed scheme against existing state-of-the-art approaches with respect to parameters such as network, security, transaction costs, and auction.
Keywords: 6G; Connected Autonomous Smart Vehicles; Vehicle-to-anything; Blockchain; Software-defined networking; Smart contracts

Zhongjin Li, Victor Chang, Haiyang Hu, Dongjin Yu, Jidong Ge, Binbin Huang,
Profit maximization for security-aware task offloading in edge-cloud environment,
Journal of Parallel and Distributed Computing,
Volume 157,
2021,
Pages 43-55,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.05.016.
(https://www.sciencedirect.com/science/article/pii/S0743731521001283)
Abstract: Mobile devices (MDs) and applications are receiving extensive popularity and attracting significant attention. Mobile applications, especially for artificial intelligence (AI) applications, require powerful computation-intensive resources. Hence, running all the AI applications on a single MD introduces high energy consumption and application delay, as it has limited battery capacity and computation resources. Fortunately, the emerging edge-cloud computing (ECC) architecture pushes the computation resource to both the network edge and remote cloud to cope with challenging AI applications. Although the advantage of ECC greatly benefits various mobile applications, data security remains an important open issue in this scenario, which has not been well studied. This paper focuses on the profit maximization (PM) problem for security-aware task offloading in an ECC environment, i.e., considering the tasks from MDs with different service demands, edge nodes should decide them to be processed on the edge node or the remote cloud with a security guarantee. Specifically, we first construct the security model to measure the time overhead for each task under various scenarios. We then formulate the PM problem by jointly considering the security demand and deadline constraints of tasks. Finally, we propose a genetic algorithm-based PM (GA-PM) algorithm, the coding strategy of which considers the task execution location and execution order. Moreover, the crossover and mutation operations are implemented based on the coding strategy. Extensive simulation experiments with various parameters varying demonstrate that our GA-PM can achieve better performance than all the comparison algorithms.
Keywords: Edge-cloud computing (ECC); Task offloading; Security constraint; Profit maximization (PM); Genetic algorithm (GA)

Hui Guo, Rui-chang Shi, Ping-li Gu, Jia-lu Li, Shu-long Wang,
Allocating edge service resources to the up-offloaded vehicle tasks in ICV environment,
Computer Networks,
Volume 227,
2023,
109715,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109715.
(https://www.sciencedirect.com/science/article/pii/S1389128623001603)
Abstract: Inspired by the rapid proliferation of intelligent transportation and smart city, ICV (Intelligent and Connected Vehicle) has drawn much attention, which not only brings great opportunities but also poses new challenges to the dynamic vehicle task serving mode. In this paper, we focus on the edge resource allocation process of the overloaded vehicle tasks that offloaded to edge servers in ICV environment. First of all, we introduce MEC and SDN technology into the traditional IoV (Internet of Vehicle) architecture to construct an SDN-assisted ICV network model. Next, a BiGRU+Attention edge service resource demands predicting model is designed, and based on the above model, we are able to further assess the future edge service resources availability and perceive the future vehicle mobility. Then, we build a task serving delay minimization problem for the edge service resource allocation process, where the impact of resource allocation decision-makings on the global network performance is also considered in the form of soft constraints. Furthermore, we put forward a timeslot-based edge service resource allocation algorithm with three phases. Finally, we simulate our scheme and another three schemes on NS3 platform, the results show that our algorithm outperforms in terms of average task serving delay, success ratio and load distribution.
Keywords: Intelligent and Connected Vehicle; Edge service resource allocation; Availability assessment

Rajni Gupta, Juhi Gupta,
Future generation communications with game strategies: A comprehensive survey,
Computer Communications,
Volume 192,
2022,
Pages 1-32,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.05.024.
(https://www.sciencedirect.com/science/article/pii/S0140366422001761)
Abstract: For an effective, low latency, and ultra reliable pervasive connectivity among next-generation wireless networks such as Internet of Things (IoT) devices, machine-to-machine (M2M) communication, and wireless sensor network (WSN), the users play intelligent strategies which enable them to take crucial decisions in order to obtain the optimal solutions. Game theory, a mathematical tool helps in solving various problems of wireless communication related to security, resource allocation, power management, energy harvesting, spectrum usage, coverage, connectivity, capacity, reliability, efficiency, optimum bandwidth, rewards and punishments of wireless nodes, and balancing of various trade-offs. This paper presents a comprehensive review, potential benefits of applying game theory (GT) in wireless communication (WC). For this purpose, a detailed overview of GT including cooperative and non-cooperative games, Q-learning, and reinforcement learning for different applications like cellular communication, multiple-input-multiple-output (MIMO), unmanned aerial vehicle (UAV), vehicle-to-vehicle (V2V) communication, cognitive radio (CR), device-to-device (D2D), wireless sensor networks (WSN) and many other applications that are pertinent to wireless networks is presented. In addition, various important design and optimization challenges are addressed. Two GT-based case studies related to physical layer security and resource allocation are also presented. In a nutshell, GT models enhanced by various learning algorithms has the potential to optimize the configuration parameters of any wireless network. Finally, we reflect the future directions and the challenges based on GT to improvise the performance of the wireless systems in the 5G technology and beyond.
Keywords: Energy harvesting; Game theory; Resource allocation; Security; Wireless communication

Zahra Makki Nayeri, Toktam Ghafarian, Bahman Javadi,
Application placement in Fog computing with AI approach: Taxonomy and a state of the art survey,
Journal of Network and Computer Applications,
Volume 185,
2021,
103078,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103078.
(https://www.sciencedirect.com/science/article/pii/S1084804521000989)
Abstract: With the increasing use of the Internet of Things (IoT) in various fields and the need to process and store huge volumes of generated data, Fog computing was introduced to complement Cloud computing services. Fog computing offers basic services at the network for supporting IoT applications with low response time requirements. However, Fogs are distributed, heterogeneous, and their resources are limited, therefore efficient distribution of IoT applications tasks in Fog nodes, in order to meet quality of service (QoS) and quality of experience (QoE) constraints is challenging. In this survey, at first, we have an overview of basic concepts of Fog computing, and then review the application placement problem in Fog computing with focus on Artificial intelligence (AI) techniques. We target three main objectives with considering a characteristics of AI-based methods in Fog application placement problem: (i) categorizing evolutionary algorithms, (ii) categorizing machine learning algorithms, and (iii) categorizing combinatorial algorithms into subcategories includes a combination of machine learning and heuristic, a combination of evolutionary and heuristic, and a combinations of evolutionary and machine learning. Then the security considerations of application placement have been reviewed. Finally, we provide a number of open questions and issues as future works.
Keywords: Fog computing; Edge computing; Artificial intelligence; Application placement; Service placement; Task scheduling; Resource management

Chenjing Tian, Haotong Cao, Yinjin Fu, Sahil Garg, Georges Kaddoum, Mohammad Mehedi Hassan,
Online and reliable SFC protection scheme of distributed cloud network for future IoT application,
Computer Communications,
Volume 208,
2023,
Pages 179-189,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.06.007.
(https://www.sciencedirect.com/science/article/pii/S0140366423002049)
Abstract: The implementation of intelligent device-free sensing (IDFS) necessitates a substantial number of edge IoT devices to extend the service coverage. Prior to deploying these IoT applications on a large scale, a reliable service provision scheme is imperative. In recent years, network function virtualization (NFV) technologies enable fast and convenient service provision by instantiating service function chain (SFC) in the distributed cloud network. However, ensuring resilient service provision is a challenge in NFV environment due to the vulnerability of software-defined network functions. Furthermore, the sequential arrangement of virtual network functions (VNFs) in SFC exacerbates the situation, as the malfunction of any VNFs within the SFC could result in the collapse of the associated service. In this paper, we firstly built a mathematical model for SFC online reliability protection and resource consumption optimization. We then propose a multi-mode VNF backup scheme to guarantee the different reliability requirements for various services. Furthermore, a novel SFC Online Reliability Protection (SORP) scheme is introduced to handle the dynamic SFC requests in distributed cloud network. The simulation results evince the efficacy of the SORP scheme in mitigating service failures and achieving a superior request acceptance ratio, while concurrently reducing average bandwidth consumption, which is particularly noteworthy in high latency scenarios.
Keywords: Distributed cloud network; Network function virtualization; Service function chain; VNF backup; Online reliability protection

Redowan Mahmud, Satish Narayana Srirama, Kotagiri Ramamohanarao, Rajkumar Buyya,
Quality of Experience (QoE)-aware placement of applications in Fog computing environments,
Journal of Parallel and Distributed Computing,
Volume 132,
2019,
Pages 190-203,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.03.004.
(https://www.sciencedirect.com/science/article/pii/S0743731518301771)
Abstract: Fog computing aims at offering Cloud like services at the network edge for supporting Internet of Things (IoT) applications with low latency response requirements. Hierarchical, distributed and heterogeneous nature of computational instances make application placement in Fog a challenging task. Diversified user expectations and different features of IoT devices also intensify the application placement problem. Placement of applications to compatible Fog instances based on user expectations can enhance Quality of Experience (QoE) regarding the system services. In this paper, we propose a QoE-aware application placement policy that prioritizes different application placement requests according to user expectations and calculates the capabilities of Fog instances considering their current status. In Fog computing environment, it also facilitates placement of applications to suitable Fog instances so that user QoE is maximized in respect of utility access, resource consumption and service delivery. The proposed policy is evaluated by simulating a Fog environment using iFogSim. Experimental results indicate that the policy significantly improves data processing time, network congestion, resource affordability and service quality.
Keywords: Fog computing; Application placement; Quality of experience; Fuzzy logic; User expectation

Anichur Rahman, Kamrul Hasan, Dipanjali Kundu, Md. Jahidul Islam, Tanoy Debnath, Shahab S. Band, Neeraj Kumar,
On the ICN-IoT with federated learning integration of communication: Concepts, security-privacy issues, applications, and future perspectives,
Future Generation Computer Systems,
Volume 138,
2023,
Pages 61-88,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.08.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002667)
Abstract: The individual and integration use of the Internet of Things (IoT), Information-Centric Networking (ICN), and Federated Learning (FL) have recently been used in several network-related scenarios and have consequently experienced a growing interest in the research community. Federated learning addresses the privacy and security issues of the IoT data in a decentralized manner. Also, it can be capable of training the multiple learning algorithms through local content except for exchanging data through intelligent Artificial Intelligence (AI)-based algorithms. Moreover, in ICN, the content is retrieved and stored based on the content name rather than the content location address. On the other hand, it is challenging to support the massive IoT devices by the fifth generation (5G) mobile-cellular networks. Therefore, the cellular 6G networks are expected to increase the connection capabilities by 10–100 times over 5G, which necessitates a convergence of Communication, Computing, and Caching (3C). At the same time, the in-network caching capabilities of ICN can be attractive features for IoT networks. IoT aspires to link anybody and/or everything at any time and location. However, integrating IoT with different areas is a new academic topic and is still in its infancy. As a result, this research highlights the potential of ICN for IoTs by conducting an exhaustive literature review. This work provides a comprehensive survey regarding these three recent research trends (i.e., FL, IoT, and ICN) and reviews the related state-of-the-art literature. We first describe the main features of each technology and discuss their most common and used variants. Furthermore, we envision the integration of such technologies to take advantage efficiently. Indeed, we consider their group-wise (FL-ICN-IoT) utilization based on the need for more robust security and privacy. Additionally, we cover the application fields of these technologies both individually and combinedly. Finally, we discuss the open issues of the reviewed research and describe potential directions for future avenues regarding integrating IoT, ICN, and FL technologies.
Keywords: Internet of Things; Information-Centric Networking; Federated Learning; Communication; Security; Computing; Machine Learning; Privacy; Artificial Intelligence; Confidentiality

Himanshu Sharma, Gitika Sharma, Neeraj Kumar,
AI-assisted secure data transmission techniques for next-generation HetNets: A review,
Computer Communications,
Volume 215,
2024,
Pages 74-90,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.12.015.
(https://www.sciencedirect.com/science/article/pii/S0140366423004504)
Abstract: Heterogeneous Networks (HetNets) play an imperative role in enhancing the quality-of-service (QoS) for end-users by increasing the spectral efficiency (SE) of the network and reducing the power consumption of user equipment (UE). However, the dynamic and distributed nature of HetNets makes them susceptible to several types of attacks (e.g., jamming, eavesdropping). Also, new technologies of 5G like massive MIMO, mmWave, and NOMA bring unique security concerns to 5G HetNets, which were not present in pre-5G HetNets. Implementing traditional security techniques such as access control, encryption, and network security seems to be insufficient for 5G HetNets and their inherent vulnerabilities. Physical layer security (PLS) has evolved as a novel approach and robust substitute to cryptography methods for ensuring secure transmission of confidential data. Also, AI methods are critical in transforming the HetNets’ security from just allowing safe communications between UEs to security-enabled intelligence systems. Motivated by the benefits of AI and PLS in improving wireless security, this paper seeks to provide a comprehensive survey of artificial intelligence (AI) enabled PLS techniques for secure data transmission in 5G HetNets. We have introduced physical layer threats and significant security issues in 5G HetNets. Then, we provide an outline of the conventional PLS techniques and challenges associated with them in the design of secure transmission techniques for 5G HetNets, such as security-oriented beamforming, cooperative jamming, etc. Then we provide an in-depth analysis of the role of AI in optimizing and designing the intelligent PLS techniques, which can be used to enhance the security of data transmission in 5G HetNets. We have also performed a strengths, weaknesses, opportunities, and threats (SWOT) analysis of the existing PLS techniques to further assist the readers in designing secure transmission techniques. Finally, we discuss various issues and future research directions associated with designing AI-enabled secure data transmission techniques.
Keywords: Heterogeneous networks; Physical layer security; Artificial intelligence; 5G

Maryam Songhorabadi, Morteza Rahimi, AmirMehdi MoghadamFarid, Mostafa Haghi Kashani,
Fog computing approaches in IoT-enabled smart cities,
Journal of Network and Computer Applications,
Volume 211,
2023,
103557,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103557.
(https://www.sciencedirect.com/science/article/pii/S1084804522001989)
Abstract: These days, the development of smart cities, specifically in location-aware, latency-sensitive, and security-crucial applications (such as emergency fire events, patient health monitoring, or real-time manufacturing), heavily depends on more advanced computing paradigms to address these requirements. In this regard, fog computing, a robust cloud computing complement, plays a preponderant role by virtue of locating closer to the end-devices. Nonetheless, utilized approaches in smart cities are frequently cloud-based, which causes not only the security and time-sensitive services to suffer but also its flexibility and reliability to be restricted. In order to obviate the limitations of cloud and other related computing paradigms such as edge computing, this paper proposes a study for the state-of-the-art fog-based approaches in smart cities. Furthermore, according to the reviewed research content, a classification is proposed that falls into three classes: service-based, resource-based, and application-based. This study also investigates the evaluation factors, used tools, evaluation methods, merits, and demerits of each class. Types of proposed algorithms in each class are mentioned as well. Above all else, by taking various perspectives into account, comprehensive and distinctive open issues and challenges are provided via classifying future trends and issues into practical sub-classes.
Keywords: Fog computing; Internet of things (IoT); Smart cities; Urban

Mohammad Samadi, Sara Royuela, Luis Miguel Pinho, Tiago Carvalho, Eduardo Quiñones,
Time-predictable task-to-thread mapping in multi-core processors,
Journal of Systems Architecture,
Volume 148,
2024,
103068,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103068.
(https://www.sciencedirect.com/science/article/pii/S1383762124000055)
Abstract: The performance of time-predictable systems can be improved in multi-core processors using parallel programming models (e.g., OpenMP). However, schedulability analysis of parallel applications is a big challenge due to their sophisticated structure. The common drawbacks of current task-to-thread mapping approaches in OpenMP are that they (i) utilize a global queue in the mapping process, which may increase contention, (ii) do not apply heuristic techniques, which may reduce the predictability and performance of the system, and (iii) use basic analytical techniques, which may cause notable pessimism in the temporal conditions. Accordingly, this paper proposes a task-to-thread mapping method in multi-core processors based on the OpenMP framework. The mapping process is carried out through two phases: allocation and dispatching. Each thread has an allocation queue in order to minimize contention, and the allocation and dispatching processes are performed using several heuristic algorithms to enhance predictability. In the allocation phase, each task-part from the OpenMP DAG is allocated to one of the allocation queues, which includes both sibling and child task-parts. A suitable thread (i.e., allocation queue) is selected using one of the suggested heuristic allocation algorithms. In the dispatching phase, when a thread is idle, a task-part is selected from its allocation queue using one of the suggested heuristic dispatching algorithms and then dispatched to and executed by the thread. The performance of the proposed method is evaluated under different conditions (e.g., varying the number of tasks and the number of threads) in terms of application response time and overhead of the mapping process. The simulation results show that the proposed method surpasses the other methods, especially in the scenario that includes overhead of the mapping. In addition, a prototype implementation of the main heuristics is evaluated using two kernels from real-world applications, showing that the methods work better than LLVM's default scheduler in most of the configurations.
Keywords: Real-time systems; Multi-core processors; OpenMP; Task-to-thread mapping; Response time; Running time

Praveen Kumar Reddy Maddikunta, Quoc-Viet Pham, Dinh C. Nguyen, Thien Huynh-The, Ons Aouedi, Gokul Yenduri, Sweta Bhattacharya, Thippa Reddy Gadekallu,
Incentive techniques for the Internet of Things: A survey,
Journal of Network and Computer Applications,
Volume 206,
2022,
103464,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103464.
(https://www.sciencedirect.com/science/article/pii/S1084804522001138)
Abstract: The Internet of Things (IoT) has remarkably evolved over the last few years to realize a wide range of newly emerging services and applications empowered by the unprecedented proliferation of smart devices. The quality of IoT networks heavily relies on the involvement of devices for undertaking functions from data sensing, computation to communication and IoT intelligence. Stimulating IoT devices to actively participate and contribute to the network is a practical challenge, where incentive techniques such as blockchain, game theory, and Artificial Intelligence (AI) are highly desirable to build a sustainable IoT ecosystem. In this article, we present a systematic literature review of the incentive techniques for IoT, aiming to provide general readers with an overview of incentive-enabled IoT from background, motivations, and enabling techniques. Particularly, we first present the fundamentals of IoT data network infrastructure, and several key incentive techniques for IoT are described in details, including blockchain, game theory, and AI. We next provide an extensive review on the use of these incentive techniques in a number of key IoT services, such as IoT data sharing, IoT data offloading and caching, IoT mobile crowdsensing, and IoT security and privacy. Subsequently, we explore the potential of incentives in important IoT applications, ranging from smart healthcare, smart transportation to smart city and smart industry. The research challenges of incentive techniques in IoT networks are highlighted, and the potential directions are also pointed out for future research of this important area.
Keywords: Incentives; Internet of Things; Game theory; Blockchain; Artificial intelligence; Smart applications

Qiang Tang, Lu Chang, Kun Yang, Kezhi Wang, Jin Wang, Pradip Kumar Sharma,
Task number maximization offloading strategy seamlessly adapted to UAV scenario,
Computer Communications,
Volume 151,
2020,
Pages 19-30,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.12.018.
(https://www.sciencedirect.com/science/article/pii/S0140366419313878)
Abstract: Mobile edge computing (MEC) has been proposed in recent years to process resource-intensive and delay-sensitive applications at the edge of mobile networks, which can break the hardware limitations and resource constraints at user equipment (UE). In order to fully use the MEC server resource, how to maximize the number of offloaded tasks is meaningful especially for crowded place or disaster area. In this paper, an optimal partial offloading scheme POSMU (Partial Offloading Strategy Maximizing the User task number) is proposed to obtain the optimal offloading ratio, local computing frequency, transmission power and MEC server computing frequency for each UE. The problem is formulated as a mixed integer nonlinear programming problem (MINLP), which is NP-hard and challenging to solve. As such, we convert the problem into multiple nonlinear programming problems (NLPs) and propose an efficient algorithm to solve them by applying the block coordinate descent (BCD) as well as convex optimization techniques. Besides, we can seamlessly apply POSMU to UAV (Unmanned Aerial Vehicle) enabled MEC system by analyzing the 3D communication model. The optimality of POSMU is illustrated in numerical results, and POSMU can approximately maximize the number of offloaded tasks compared to other schemes.
Keywords: Offloaded task number maximization; Partial offloading; Unmanned aerial vehicle; Mobile edge computing

Jose Moura, David Hutchison,
Fog computing systems: State of the art, research issues and future trends, with a focus on resilience,
Journal of Network and Computer Applications,
Volume 169,
2020,
102784,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102784.
(https://www.sciencedirect.com/science/article/pii/S1084804520302587)
Abstract: Many future innovative computing services will use Fog Computing Systems (FCS), integrated with Internet of Things (IoT) resources. These new services, built on the convergence of several distinct technologies, need to fulfil time-sensitive functions, provide variable levels of integration with their environment, and incorporate data storage, computation, communications, sensing, and control. There are, however, significant problems to be solved before such systems can be considered fit for purpose. The high heterogeneity, complexity, and dynamics of these resource-constrained systems bring new challenges to their robust and reliable operation, which implies the need for integral resilience management strategies. This paper surveys the state of the art in the relevant fields, and discusses the research issues and future trends that are emerging. We envisage future applications that have very stringent requirements, notably high-precision latency and synchronization between a large set of flows, where FCSs are key to supporting them. Thus, we hope to provide new insights into the design and management of resilient FCSs that are formed by IoT devices, edge computer servers and wireless sensor networks; these systems can be modelled using Game Theory, and flexibly programmed with the latest software and virtualization platforms.
Keywords: Fog computing; Internet of things; IoT; Edge computing; Cyber-physical systems; CPS; Software defined networks; SDN; Challenges; Game theory; Network function virtualization; NFV; Cyber-attacks; Resilient systems; Self-awareness; Network slicing

Wai-xi Liu, Jun Cai, Qing Chun Chen, Yu Wang,
DRL-R: Deep reinforcement learning approach for intelligent routing in software-defined data-center networks,
Journal of Network and Computer Applications,
Volume 177,
2021,
102865,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102865.
(https://www.sciencedirect.com/science/article/pii/S1084804520303313)
Abstract: Data-center networks (DCN) possess multiple new features: coexistence of elephant flow/mice flow/coflow, and coexistence of multiple network resources (bandwidth, cache and computing). The cache should be a factor of effecting routing decision because it can eliminate redundant traffic in DCN. However, the conventional routing schemes cannot learn from their previous experiences regarding network abnormalities (such as, congestion), and their metric are still the single link state (such as, hop, distance, and cost) which does not include the effect of cache. Thus, they cannot enough efficiently allocate these resources to well meet the performance requirements for various flow types. Therefore, this paper proposes deep reinforcement learning-based routing (DRL-R). Firstly, we propose a method that recombines multiple network resources with different metrics, where we recombine cache and bandwidth by quantifying their contribution score in reducing the delay. Secondly, we propose a routing scheme with resource-recombined state. By optimally allocating network resources for traffic, a DRL agent deployed on a software-defined networking (SDN) controller continually interacts with the network to adaptively perform reasonable routing according to the network state. We employ deep Q-network (DQN) and deep deterministic policy gradient (DDPG) to build the DRL-R. Finally, we demonstrate the effectiveness of DRL-R through extensive simulations. Benefitting from continuous learning with a global view, DRL-R has lower flow completion time, higher throughput and better load balance as well as better robustness, compared to OSPF. In addition, because it efficiently utilizes the network resources, DRL-R can also outperform another DRL-based routing scheme (namely TIDE). Compared to OSPF and TIDE, respectively, DRL-R can improve throughput by up to 40% and 18.5%; DRL-R can reduce flow completion time by up to 47% and 39%; DRL-R can improve the link load balance by up to 18.8% and 9.3%. Additionally, we observed that DDPG has better performance than DQN.

Weiwei Jiang, Haoyu Han, Miao He, Weixi Gu,
When game theory meets satellite communication networks: A survey,
Computer Communications,
Volume 217,
2024,
Pages 208-229,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.02.005.
(https://www.sciencedirect.com/science/article/pii/S0140366424000446)
Abstract: Satellite communication networks have been considered an integral part of B5G and 6G networks to achieve global coverage and enhanced Internet services. However, the integration of satellite and terrestrial networks also brings many challenges, including the explosion of management complexity, the limited resource in satellite nodes, and the strategic behavior among network participants. To solve these challenges, game theory has emerged as a potential solution for rapidly evolving satellite communication networks. While there are some surveys discussing game theory in various networking scenarios, there is a lack of surveys targeting game theory-based solutions in satellite communication networks. To fill in this research gap, the objective and research motivation of this study are to summarize and present a comprehensive and up-to-date literature review of recent studies applying game theory to various applications in satellite networks. Both cooperative and non-cooperative games are covered, with a total number of fourteen different game models. Based on the review of existing studies, research challenges and opportunities are further proposed to inspire future research directions. To the best of our knowledge, this paper is the first comprehensive survey focusing on the application of game theory to satellite communication networks.
Keywords: Game theory; Satellite communication network; Cooperative game; Non-cooperative game

Jie Sun, Yi Zhang, Feng Liu, Huandong Wang, Xiaojian Xu, Yong Li,
A survey on the placement of virtual network functions,
Journal of Network and Computer Applications,
Volume 202,
2022,
103361,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103361.
(https://www.sciencedirect.com/science/article/pii/S1084804522000285)
Abstract: The dependence of traditional network functions (NFs) on special hardware results in high capital expenditures (CAPEX) and operating expenditures (OPEX). Network Function Virtualization (NFV) is considered a promising technology to reduce the specificity of network equipment as well as the CAPEX and OPEX. With the decoupling of software and hardware, the flexible placement of virtual network functions (VNFs) resolves a wide range of network issues ranging from delay to reliability, from cost to performance, and so on. In this paper, we conduct a comprehensive survey of the VNF placement problem (VNFPP). Firstly, we provide a generic definition of the VNFPP by proposing and combining the basic models of chaining, placing, embedding, and routing. Secondly, we construct a fine-grained taxonomy of works in the VNF placement field based on the solving steps, i.e., objectives, constraints, models, and algorithms. In addition to the traditional cloud network scenario, we investigate the research progress of VNF placement problems in emerging placing scenarios, e.g., backbone networks, mobile networks, and the Internet of Things. Finally, we point out the open challenges and research directions in the field of VNF placement. Compared with the existing surveys, our survey provides a more granular taxonomy and a more comprehensive analysis of the state-of-the-art works in the VNFPP field.
Keywords: Virtual network function placement; Network function virtualization; Solving-based taxonomy; Strategy optimization

Muhammad Morshed Alam, Muhammad Yeasir Arafat, Sangman Moh, Jian Shen,
Topology control algorithms in multi-unmanned aerial vehicle networks: An extensive survey,
Journal of Network and Computer Applications,
Volume 207,
2022,
103495,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103495.
(https://www.sciencedirect.com/science/article/pii/S1084804522001370)
Abstract: In recent years, unmanned aerial vehicles (UAVs) have attracted increased attention from academic and industrial research communities, owing to their wide range of potential applications in military and civilian domains. A collaborative group of UAVs operating in an ad hoc manner known as a flying ad hoc network (FANET) can accomplish complex tasks more efficiently. However, owing to the high mobility of UAVs, such applications remain limited by a few key challenges, including dynamic time-varying topologies, energy constraints, frequent link breakages, inter-UAV collisions, and external obstacle avoidance. A proper topology control algorithm (TCA) for UAV swarms with reasonable overhead helps to optimize both mission and communication performance in FANET. Thus, TCA provides wider coverage ensuring the quality of service in aerial connectivity. Additionally, it supports the efficient energy management, better target exploration, improved formation stability while ensuring inter-UAV collision avoidance, external obstacle avoidance, lower interference, and the enhanced autonomy of UAV swarms. In this article, we present a comprehensive survey of available TCAs for FANET, and provide a novel taxonomy of TCAs based on the FANET topology architectures and underlying mathematical models. Through an in-depth assessment of recent innovative research articles and their comparative studies, we aim to provide novel insights into the latest technologies for autonomous cooperative coordination. The key open research issues and their respective solutions are addressed as future research directions.
Keywords: —Clustering; Collision avoidance; Connectivity control; Coverage control; Flying ad hoc network; Flocking control; Swarm intelligence; Topology control algorithm; Unmanned aerial vehicle network

Bowen Liu, Xiaolong Xu, Lianyong Qi, Qiang Ni, Wanchun Dou,
Task scheduling with precedence and placement constraints for resource utilization improvement in multi-user MEC environment,
Journal of Systems Architecture,
Volume 114,
2021,
101970,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101970.
(https://www.sciencedirect.com/science/article/pii/S1383762120302174)
Abstract: Efficient task scheduling improves offloading performance in mobile edge computing (MEC) environment. The jobs offloaded by different users would have different dependent tasks with diverse resource demands at different times. Meanwhile, due to the heterogeneity of edge servers configurations in MEC, offloaded jobs may frequently have placement constraints, restricting them to run on a particular class of edge servers meeting specific software running settings. This spatio-temporal information gives the opportunity to improve the resource utilization of the computing system. In this paper, we study the scheduling method for the jobs consisting of dependent tasks offloaded by different users in MEC. A new task offloading scheduler, Horae, is proposed to not only improve the resource utilization of MEC environment but also guarantees to select the edge server which could satisfy placement constraints for each offloaded task. Concretely, considering the fact that each job would experience slack time as a result of competing for limited resource with other jobs in MEC, Horae minimizes the sum of all slack time values of all the jobs while guaranteeing placement constraints, and therefore improve the resource utilization of the system. Horae was validated for its feasibility and efficiency by means of extensive experiments, which are presented in this paper.
Keywords: Mobile edge computing; Offloading; Precedence constraints; Resource utilization

Enas Bagies, Ahmed Barnawi, Saoucene Mahfoudh, Neeraj Kumar,
Content delivery network for IoT-based Fog Computing environment,
Computer Networks,
Volume 205,
2022,
108688,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108688.
(https://www.sciencedirect.com/science/article/pii/S1389128621005557)
Abstract: With the continuous improvement and evolution of network technologies like IoT which demands the innovation and improvement of legacy networks, as well as, the requirements of end users’ Quality of Service (QoS) and Quality of Experience (QoE), new network paradigms have been invented, such as, Content Delivery Network (CDN), IoT and Fog Computing. In addition, the management and controlling of such a network may create an overhead to the network administrators. Therefore, Software-defined Network (SDN) has been introduced as a framework for managing and controlling the network devices where the network abstraction facilitates the introduction innovative solutions and improve the overall performance. In this paper, an SDN based architecture to implement fog-based CDN network was proposed aiming at further improvement on the routing functionalities essential in Fog-based CDN deployment. We have tested the proposed architecture and the results show significant gains. Moreover, we have identified some factors to be considered in such architectures for further performance improvement.
Keywords: SDN; Fog Computing; CDN; IoT

Felippe Vieira Zacarias, Vinicius Petrucci, Rajiv Nishtala, Paul Carpenter, Daniel Mossé,
Intelligent colocation of HPC workloads,
Journal of Parallel and Distributed Computing,
Volume 151,
2021,
Pages 125-137,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.02.010.
(https://www.sciencedirect.com/science/article/pii/S0743731521000319)
Abstract: Many HPC applications suffer from a bottleneck in the shared caches, instruction execution units, I/O or memory bandwidth, even though the remaining resources may be underutilized. It is hard for developers and runtime systems to ensure that all critical resources are fully exploited by a single application, so an attractive technique for increasing HPC system utilization is to colocate multiple applications on the same server. When applications share critical resources, however, contention on shared resources may lead to reduced application performance. In this paper, we show that server efficiency can be improved by first modeling the expected performance degradation of colocated applications based on measured hardware performance counters, and then exploiting the model to determine an optimized mix of colocated applications. This paper presents a new intelligent resource manager and makes the following contributions: (1) a new machine learning model to predict the performance degradation of colocated applications based on hardware counters and (2) an intelligent scheduling scheme deployed on an existing resource manager to enable application co-scheduling with minimum performance degradation. Our results show that our approach achieves performance improvements of 7% (avg) and 12% (max) compared to the standard policy commonly used by existing job managers.
Keywords: Resource manager; HPC systems; Machine learning; Colocation; Performance Characterization; Performance counters

Andrea Agiollo, Paolo Bellavista, Matteo Mendula, Andrea Omicini,
EneA-FL: Energy-aware orchestration for serverless federated learning,
Future Generation Computer Systems,
Volume 154,
2024,
Pages 219-234,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000074)
Abstract: Federated Learning (FL) represents the de-facto standard paradigm for enabling distributed learning over multiple clients in real-world scenarios. Despite the great strides reached in terms of accuracy and privacy awareness, the real adoption of FL in real-world scenarios, in particular in industrial deployment environments, is still an open thread. This is mainly due to privacy constraints and to the additional complexity stemming from the set of hyperparameters to tune when employing AI techniques on bandwidth-, computing-, and energy-constrained nodes. Motivated by these issues, we focus on scenarios where participating clients are characterised by highly heterogeneous computing capabilities and energy budgets proposing EneA-FL, an innovative scheme for serverless smart energy management. This novel approach dynamically adapts to optimise the training process while fostering seamless interaction between Internet of Things (IoT) devices and edge nodes. In particular, the proposed middleware provides a containerised software module that efficiently manages the interaction of each worker node with the central aggregator. By monitoring local energy budget, computational capabilities, and target accuracy, EneA-FL intelligently takes informed decisions about the inclusion of specific nodes in the subsequent training rounds, effectively balancing the tripartite trade-off between energy consumption, training time, and final accuracy. Finally, in a series of extensive experiments across diverse scenarios, our solution demonstrates impressive results, achieving between 30% and 60% lower energy consumption against popular client selection approaches available in the literature while being up to 3.5 times more efficient than standard FL solutions.
Keywords: Serverless; Federated learning; Energy management; Internet of things; Resource-constrained learning

Ece Gelal Soyak, Ozgur Ercetin,
Effective networking: Enabling effective communications towards 6G,
Computer Communications,
Volume 215,
2024,
Pages 1-8,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.12.002.
(https://www.sciencedirect.com/science/article/pii/S0140366423004358)
Abstract: The realization of envisioned 6G use cases involving holographic and multi-sensory communications demand terabits per second data rates and latencies in the range of microseconds for an immersive experience. Concurrently, 6G’s hyper-intelligent IoT use cases require extremely low-latency and reliable communications at the network edge. To address these requirements, communications should be tailored to end-user goals. To this end, we study communication effectiveness, where a sender and receiver harness their computing capabilities and artificial intelligence, to maximize the impact of transmitted messages while sending fewer bits. On our model, the messages can get shorter as locally accumulated knowledge increases the targeted effect of the message. Hence, we describe a framework in which the accumulated knowledge can be aggregated and shared in a distributed manner. On a real-life use case, we showcase the potential reduction in the number of bits transmitted owing to the transferred accumulated knowledge. Finally, we explore future research directions in effective communications, considering technical, economical, and privacy considerations.
Keywords: Effective communications; 6G; Semantic communications; Task-oriented communications; Edge intelligence; Future network architectures; Knowledge accumulation

Yifan Chen, Zhiyong Li, Bo Yang, Ke Nai, Keqin Li,
A Stackelberg game approach to multiple resources allocation and pricing in mobile edge computing,
Future Generation Computer Systems,
Volume 108,
2020,
Pages 273-287,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.02.045.
(https://www.sciencedirect.com/science/article/pii/S0167739X19311653)
Abstract: Mobile edge computing is a new paradigm that can enhance the computation capability of end devices and alleviate communication traffic loads during transmission. Mobile edge computing is highly useful for emerging resource-hungry mobile applications. However, a key challenge for mobile edge computing systems is multiple resources allocation between Mobile Edge Clouds (MECs) and End Users (EUs), especially for multiple heterogeneous MECs and EUs. To address this problem, we propose a Stackelberg game-based framework in which EUs and MECs act as followers and leaders, respectively. The proposed framework aims to compute a Stackelberg equilibrium solution in which each MEC achieves the maximum revenue while each EU obtains utility-maximized resources under budget constraints. We decompose the multiple resources allocation and pricing problem into a set of subproblems in which each subproblem only considers a single resource type. The Stackelberg game framework is constructed for each subproblem wherein each player (i.e., an EU) can selfishly maximize its utility by selecting an appropriate strategy in the strategy space. We prove the existence of the subgame Stackelberg equilibrium and develop algorithms to determine the Stackelberg equilibrium for each resource type, including an optimal demand computation algorithm, to determine the best resource demand strategy for an EU and an iterative algorithm to find an equilibrium price. The equilibrium solutions of all subgames constitute the equilibrium solution of the original problem. We also conduct simulation experiments of our game, such as numerical data for the Stackelberg equilibrium, numerical data for the convergence of the Stackelberg equilibrium, and numerical data as the system size increases. Finally, we demonstrate that an EU with idle resources can play the role of an MEC.
Keywords: Game theory; Mobile edge computing; Multiple resources allocation; Resource pricing

Giovanni Nardini, Giovanni Stea,
Enabling simulation services for digital twins of 5G/B5G mobile networks,
Computer Communications,
Volume 213,
2024,
Pages 33-48,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.10.017.
(https://www.sciencedirect.com/science/article/pii/S0140366423003821)
Abstract: Digital Twins (DTs) have been proposed as digital replicas of physical entities (e.g., manufacturing equipment), which one can observe in real-time and interact with. Digital Twins of Networks (DTNs) are increasingly being discussed in the literature, as an enabler for efficient data-driven network management and performance-driven network optimization (e.g., to support dynamic reconfiguration, or anticipate the effects of faults). A DTN includes service mapping models, i.e. models that can be fed with acquired data to produce insight on the network itself - e.g., to run what-if scenarios, based on multiple underlying technologies, from Machine Learning to analytical models, e.g. Markov Chains. In this paper we examine the case of DTNs of mobile networks, DTMNs, tailored to 5G and beyond, where issues of dynamic reconfiguration and fault anticipation are critical. We argue that simulation services should be offered by the DTMN in order to allow performance-driven network optimization, and that discrete-event network simulators are ideal instruments to be employed for this purpose. We discuss the challenges that need be addressed to make this happen, e.g., centralized vs. distributed implementation, gathering input from the physical network, security issues and hosting, and we review the possibilities offered by network simulation in terms of what-if analysis, defining the concepts of lockstep and branching analysis. We present a framework to endow a DTMN with simulation services and we exemplify it using Simu5G, a popular 5G/B5G simulation library for OMNeT++, as a reference case study.
Keywords: Digital twin network; Simu5G; Emulation; Simulation; Mobile networks

Hongwei Zhang, Wei Fan, Jinsong Wang,
Bidirectional utilization of blockchain and privacy computing: Issues, progress, and challenges,
Journal of Network and Computer Applications,
Volume 222,
2024,
103795,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103795.
(https://www.sciencedirect.com/science/article/pii/S108480452300214X)
Abstract: With the rapid development of information technology and the increasing popularity of personalized services, the flow of users’ personal information in the Internet is inevitable between different platforms and applications, which greatly affects the trust relationship in the digital society. In recent years, an increasing number of researchers have been dedicated to exploring the applications of blockchain and privacy computing technologies in the process of data value release. Scholars have discovered that the combination of the two technologies can alleviate many pain points of each other. Specifically, privacy computing is a computational theory and method for protecting privacy of information throughout its lifecycle. Privacy computing enables the processing, analysis, and computation of data while preserving the confidentiality of the original information. Nevertheless, challenges related to data ownership and benefit distribution impede the establishment of a fair and effective collaborative environment among multiple parties, thereby restricting its further practical development. On the other hand, blockchain is a technology for establishing a trusted platform among mutually untrusted parties, characterized by decentralization, immutability, traceability, and incentive mechanisms. However, blockchain also faces security issues such as transparency of on-chain data and lack of protection in smart contract execution environments. Therefore, this paper aims to comprehensively analyze and explore the integration of blockchain and privacy computing from both perspectives, focusing on bidirectional utilization. We systematically categorize and summarize relevant literature in this field, employing problem-oriented approaches. Finally, we identify the existing challenges and propose potential improvement methods in this field.
Keywords: Blockchain; Privacy computing; Bidirectional utilization; Secure multi-party computation; Trusted execution environment; Federated learning

S. ArunKumar, B. Vinoth Kumar, M. Pandi,
Artificial bee colony optimization based energy-efficient wireless network interface selection for industrial mobile devices,
Computer Communications,
Volume 154,
2020,
Pages 1-10,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.067.
(https://www.sciencedirect.com/science/article/pii/S0140366419308795)
Abstract: Management of energy is a crucial challenge for mobile devices, having the network activity frequently taking up a considerable part of the energy in the overall system. The important goal of this work is to offer energy efficiency for mobile handhelds. The industrial mobile devices in the recent times are fitted with several wireless network interfaces, these devices function with less battery power. In mobile handhelds, each user is able to run multiple applications with the purpose of have particular Quality of Service (QoS) requirements. Hence, it would be beneficial for a multi-interface terminal to simultaneously use two or more interfaces to gain in performance. So satisfying the QoS becomes very difficult task. An energy-effective Adaptive Wireless Network Interface Selection (AWNIS) with Artificial Bee Colony (ABC) is proposed in this paper. AWNIS-ABC algorithm chooses the best of the wireless network interface in regard to the energy consumed by taking the QoS of the link into consideration as well as following a hi-speed network interface-selection interval based on the scenario in the network. Then Hidden Semi Markov Model (HSMM) is introduced for searching the detouring paths if the energy state of a given relay node is very less to utilize the required interface, which ensures timely delivery. Also, it provides the diversification of the route paths in the network with the purpose of extending the lifetime of the network. An alternate path is proposed for reliable data delivery beforehand the route path encounters a breakage owing to energy exhaustion is found. A number of wireless network interfaces in mobile devices are utilized in improving the energy efficiency, satisfaction of QoS occurring during data transfer. The simulation of the network is implemented via the Network Simulator 2(NS2).
Keywords: Energy efficiency; Industrial mobile devices; Smart phones; Wireless network interface selection; Routing; Multiple channel; Hidden Semi Markov Model (HSMM); Quality of Service (qoS); Artificial Bee Colony (ABC)

Wei Wei, Qi Wang, Weidong Yang, Yashuang Mu,
Efficient stochastic scheduling for highly complex resource placement in edge clouds,
Journal of Network and Computer Applications,
Volume 202,
2022,
103365,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103365.
(https://www.sciencedirect.com/science/article/pii/S1084804522000315)
Abstract: For the edge cloud-based large-scale distributed systems in wide areas, it is important to quickly adjust the deployed resource in multiple edge clouds to maximize the resource revenue and meet the quality of service requirements. The mean values of demands are usually used in the scheduling algorithms for simplicity, but in the real-world scenarios the resource demands may fluctuate greatly, which cannot be effectively modeled in the mean value-based demand model, resulting in the under-utilization of resources. To address the problem, we investigate a general stochastic scheduling problem in the edge clouds, whose objective is to place the given amount of resources into the edge areas, and to maximize the scheduling revenue like the weighted sum of satisfied demands. We then propose an efficient algorithm by identifying the optimal conditions of nested subproblems. Experiments show that in the scenarios with general settings, the algorithm can achieve at least 97% average revenue of the traditional optimal algorithm with much lower time complexity, which can be further reduced through parallelization. The algorithm has the potential to be an effective supplement to the existing algorithms under the time-tense scheduling scenarios with a large number of resources.
Keywords: Edge cloud; Resource placement; Stochastic demand; Nested problem; Cloud computing

Chi Xu, Zhengzhe Li, Guo Qing Huai, Jia Zhao, Yifei Zhu, Xiaoqiang Ma, Haiyang Wang,
Enabling lightweight immersive user interaction in smart buildings through learning-based mobile panorama streaming,
Computer Communications,
Volume 222,
2024,
Pages 68-76,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.04.002.
(https://www.sciencedirect.com/science/article/pii/S0140366424001245)
Abstract: Smart buildings integrate users’ digitized wearables with their physical surroundings, creating a seamless and interactive user experience. This is achieved through the utilization of multiple sensors, video streaming, artificial intelligence, and edge computing. These technologies gather extensive data and provide users with a wide range of applications, such as 3D audio/video in AR/VR, localization, virtual tours, and vigilant monitoring. Nevertheless, the current AR/VR devices face limitations due to the bulkiness and discomfort of the hardware used for on-body sensing, such as headsets and specialized glasses. These components often become uncomfortable during prolonged usage, posing a challenge for creating an immersive system that combines lightweight interaction with high-quality presentation. This paper presents a comprehensive system designed to enable immersive interaction in smart buildings with a focus on lightweight solutions. The system consists of the following components: (1). A lightweight panoramic imaging framework to address the challenges related to hardware size and functionality. (2). A learning-based video transcoding cost prediction framework for efficient load balancing. (3). A layered networking architecture designed to facilitate high-quality mobile panorama live streaming. Collectively, these components offer lightweight interaction paired with enhanced presentation quality. Our experimental results demonstrate the effectiveness of the system design, showcasing its seamless operation across different times, geographical locations, and heterogeneous wireless networks.
Keywords: Artificial intelligence; Streaming media; Mobile video; Smart devices; Interactive systems

Andrea Sabbioni, Lorenzo Rosa, Armir Bujari, Luca Foschini, Antonio Corradi,
DIFFUSE: A DIstributed and decentralized platForm enabling Function composition in Serverless Environments,
Computer Networks,
Volume 210,
2022,
108993,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108993.
(https://www.sciencedirect.com/science/article/pii/S138912862200161X)
Abstract: Serverless computing is an emerging proposition in the cloud offering landscape that promotes a higher level of abstraction, further decoupling software operations from the underlying hardware. Often recognized as an economically driven computational approach, the serverless model relies on the execution of short-lived stateless functions, enabling a fine-grained accounting and control of resources. In this context, function composition represents an appealing feature, allowing the composition of two or more functions to create tailored processing pipelines, incentivizing modularity and reusability of functions, while paving the way to application-specific run-time optimizations. This work presents DIFFUSE: a DIstributed and decentralized platForm enabling Function composition in Serverless Environments. DIFFUSE embodies an innovative infrastructural support, enabling the efficient and transparent composition of functions by relying on pluggable middleware support, serving as a conveyor of messages among the platform components. Broadening the deployment spectrum of our proposal, we assess different middleware solutions, each presenting distinct delivery profiles, evidencing the tradeoffs that emerge.
Keywords: Serverless computing; Function composition; Shared memory; Middleware; Latency; Distributed system

Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, Viktor Prasanna,
Accurate, efficient and scalable training of Graph Neural Networks,
Journal of Parallel and Distributed Computing,
Volume 147,
2021,
Pages 166-183,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.08.011.
(https://www.sciencedirect.com/science/article/pii/S0743731520303579)
Abstract: Graph Neural Networks (GNNs) are powerful deep learning models to generate node embeddings on graphs. When applying deep GNNs on large graphs, it is still challenging to perform training in an efficient and scalable way. We propose a novel parallel training framework. Through sampling small subgraphs as minibatches, we reduce training workload by orders of magnitude compared with state-of-the-art minibatch methods. We then parallelize the key computation steps on tightly-coupled shared memory systems. For graph sampling, we exploit parallelism within and across sampler instances, and propose an efficient data structure supporting concurrent accesses from samplers. The parallel sampler theoretically achieves near-linear speedup with respect to number of processing units. For feature propagation within subgraphs, we improve cache utilization and reduce DRAM traffic by data partitioning. Our partitioning is a 2-approximation strategy for minimizing the communication cost compared to the optimal. We further develop a runtime scheduler to reorder the training operations and adjust the minibatch subgraphs to improve parallel performance. Finally, we generalize the above parallelization strategies to support multiple types of GNN models and graph samplers. The proposed training outperforms the state-of-the-art in scalability, efficiency and accuracy simultaneously. On a 40-core Xeon platform, we achieve 60× speedup (with AVX) in the sampling step and 20× speedup in the feature propagation step, compared to the serial implementation. Our algorithm enables fast training of deeper GNNs, as demonstrated by orders of magnitude speedup compared to the Tensorflow implementation. We open-source our code at https://github.com/GraphSAINT/GraphSAINT
Keywords: Graph representation learning; Graph Neural Networks; Graph sampling; Graph partitioning; Memory optimization

Drew Penney, Bin Li, Jaroslaw J. Sydir, Lizhong Chen, Charlie Tai, Stefan Lee, Eoin Walsh, Thomas Long,
PROMPT: Learning dynamic resource allocation policies for network applications,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 164-175,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.03.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000936)
Abstract: A growing number of service providers are exploring methods to improve server utilization and reduce power consumption by co-scheduling high-priority latency-critical workloads with best-effort workloads. This practice requires strict resource allocation between workloads to reduce contention and maintain Quality-of-Service (QoS) guarantees. Prior work demonstrated promising opportunities to dynamically allocate resources based on workload demand, but may fail to meet QoS objectives in more stringent operating environments due to the presence of resource allocation cliffs, transient fluctuations in workload performance, and rapidly changing resource demand. We therefore propose PROMPT, a novel resource allocation framework using proactive QoS prediction to guide a reinforcement learning controller. PROMPT enables more precise resource optimization, more consistent handling of transient behaviors, and more robust generalization when co-scheduling new best-effort workloads not encountered during policy training. Evaluation shows that the proposed method incurs 4.2x fewer QoS violations, reduces severity of QoS violations by 12.7x, improves best-effort workload performance, and improves overall power efficiency over prior work.
Keywords: Workload co-scheduling; Dynamic resource allocation; Latency-critical workloads; Network edge; Performance prediction; Reinforcement learning

Fabien Bouquillon, Smail Niar, Giuseppe Lipari,
Reducing the fault vulnerability of hard real-time systems,
Journal of Systems Architecture,
Volume 133,
2022,
102758,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102758.
(https://www.sciencedirect.com/science/article/pii/S1383762122002430)
Abstract: With the progress of the technology, the presence of transient faults (e.g. bit-flipping errors) in cache memories becomes a challenge, especially in embedded real-time systems. These are mission critical systems that are often subject to both fault-tolerant and real-time constraints. To reduce the impact of transient faults, hardware protection mechanisms are usually proposed. However, these mechanisms introduce too much pessimism in the computation of the worst-case execution time of a task, decreasing the overall system performance. In this paper, we propose a methodology to evaluate and reduce the vulnerability of hard real-time applications to soft errors in IL1 cache memories. We use static analysis tools to analyze a binary program and compute the overall vulnerability of its instructions. Then, we propose to reduce this vulnerability by invalidating some cache blocks at specific instants during the execution, thus forcing vulnerable instruction blocks to be reloaded from higher layers of memory. Since adding invalidation points will likely increase the WCETs of the tasks, we perform a static analysis to guarantee that the application deadlines are respected Finally, we analyze how our methodology can be combined with hardware protection mechanisms as ECC memories, and we evaluate the performance on a set of benchmarks.
Keywords: Real-time systems; Reliability; Cache memory; Transient faults; Vulnerability; Static code analysis; WCET analysis

Shashank Shekhar, Ajay Chhokra, Hongyang Sun, Aniruddha Gokhale, Abhishek Dubey, Xenofon Koutsoukos, Gabor Karsai,
URMILA: Dynamically trading-off fog and edge resources for performance and mobility-aware IoT services,
Journal of Systems Architecture,
Volume 107,
2020,
101710,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101710.
(https://www.sciencedirect.com/science/article/pii/S1383762120300047)
Abstract: The fog/edge computing paradigm is increasingly being adopted to support a range of latency-sensitive IoT services due to its ability to assure the latency requirements of these services while supporting the elastic properties of cloud computing. IoT services that cater to user mobility, however, face a number of challenges in this context. First, since user mobility can incur wireless connectivity issues, executing these services entirely on edge resources, such as smartphones, will result in a rapid drain in the battery charge. In contrast, executing these services entirely on fog resources, such as cloudlets or micro data centers, will incur higher communication costs and increased latencies in the face of fluctuating wireless connectivity and signal strength. Second, a high degree of multi-tenancy on fog resources involving different IoT services can lead to performance interference issues due to resource contention. In order to address these challenges, this paper describes URMILA, which makes dynamic resource management decisions to achieve effective trade-offs between using the fog and edge resources yet ensuring that the latency requirements of the IoT services are met. We evaluate URMILA’s capabilities in the context of a real-world use case on an emulated but realistic IoT testbed.
Keywords: Fog computing; Edge computing; Cloud computing; User mobility; Latency-sensitive; IoT; Resource management; Performance interference; Latency; Offloading

B Premalatha, P Prakasam,
Optimal Energy-efficient Resource Allocation and Fault Tolerance scheme for task offloading in IoT-FoG Computing Networks,
Computer Networks,
Volume 238,
2024,
110080,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110080.
(https://www.sciencedirect.com/science/article/pii/S138912862300525X)
Abstract: Due to technology development in the recent past, it is observed that an exponential growth in the usage of high speed intelligent devices which includes smart objects, smart home, smart vehicles etc. Therefore, for the efficient transmission between different smart objects/devices situated across different regions, an effective communication is required and the corresponding technology is called as ‘Internet of Things (IoT)’. This leads to an issues in various aspects like, increase in complexity, low data rate & latency and security may reduce the performance of the existing technologies. Therefore, there is a huge requirement of advanced technology which may support ultra-low latency transmission for the task received from the devices. Fog computing (FC) is one of an emerging technology that can improve network performance and provide resource-constrained applications for the IoT devices. In this paper, the Optimal Energy-efficient Resource Allocation (OEeRA) algorithm is proposed based on Minimal Cost Resource Allocation (MCRA) and Fault Identification and Rectification (FIR) algorithms for effective task offloading of IoT-FoG computing networks. The MCRA algorithm is proposed to assign at least one FN and Resource Block (RB) for each device, and also it ensures that each FN is connected with one or more RBs and devices. The leftover RBs are collected and stored in the buffer to replace the faulty RBs, as proposed in the FIR algorithm, which achieves better processing and response time with higher fault detection accuracy. The energy efficiency (EE) of the proposed OEeRA algorithm is computed through MCRA and FIR algorithms by varying FN, RB, and IoT devices. The performance analysis shows that the proposed algorithm achieved the maximum EE of 6.12 × 109 bit/J, 5.69 × 1010 bit/J, and 3.019 × 1010 (bit/J) for varying RBs, IoTs, and FNs, respectively.
Keywords: Fog Computing; Internet of Things; Task offloading; Fault Tolerance; Energy Efficiency

Ning Chen, Sheng Zhang, Jie Wu, Zhuzhong Qian, Sanglu Lu,
Learning scheduling bursty requests in Mobile Edge Computing using DeepLoad,
Computer Networks,
Volume 184,
2021,
107655,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107655.
(https://www.sciencedirect.com/science/article/pii/S1389128620312706)
Abstract: The emergence of Mobile Edge Computing (MEC) alleviates the large transmission latency resulting from the traditional cloud computing. For the compute-intensive requests such as video analysis, mobile users prefer to obtain a desired quality of experience (QoE) with neglected latency and reduced energy consumption. The popularity of smart devices allows users to release a run of compute-intensive as well as latency-sensitive requests anywhere, which may lead to bursty requests. A single resource-constrained edge server nearby is capable of handling a small amount of requests quickly, yet it seems helpless when encountering bursty compute-intensive requests. Despite the abundance of recently proposed schemes, the majority focus on efficiently scheduling pending requests in a single edge server, and ignored the potential role of edge collaboration to schedule bursty requests. Besides, while some recent studies proposed to finish a task using multiple devices, they focused on collaboration between mobile devices rather than between edge servers. Hence, we propose DeepLoad, a S2S system that schedules the bursty requests with a collaborative method using reinforcement learning (RL). DeepLoad decouples the scheduling decision into AP selection for setting the access point and workload redistribution for collaborative servers. DeepLoad trains a neural network model that picks decisions for each request based on observations collected by mobile devices. DeepLoad learns to make scheduling decisions solely through the resulting performance of historical decisions rather than rely on pre-programmed models or specific assumptions for the environment. Naturally, DeepLoad automatically learns the scheduling algorithm for each request and obtains a gratifying QoE. We aim to maximize the fraction of requests finished before their attached deadlines. Based on the Shanghai taxi trajectory data set, we design a simulator to obtain abundant samples, and leverage two GeForce GTX TITAN Xp GPUs to train the Actor–Critic network. Compared to the state-of-the-art bandwidth-based and server resources-based methods, DeepLoad can achieve a significant improvement in average fraction.
Keywords: Bursty requests; Edge collaboration; Deep reinforcement learning

Hanan Elazhary,
Internet of Things (IoT), mobile cloud, cloudlet, mobile IoT, IoT cloud, fog, mobile edge, and edge emerging computing paradigms: Disambiguation and research directions,
Journal of Network and Computer Applications,
Volume 128,
2019,
Pages 105-140,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2018.10.021.
(https://www.sciencedirect.com/science/article/pii/S1084804518303497)
Abstract: Currently, we are experiencing a technological shift, which is expected to change the way we program and interact with the world. Cloud computing and mobile computing are two prominent research areas that have already had such an impact. The Internet of Things (IoT), which is concerned with building a network of Internet-enabled devices to promote a smart environment, is another promising area of research. Numerous emerging computing paradigms related to those areas of research and/or their intersections have come into play. These include Mobile Cloud Computing (MCC), cloudlet computing, mobile clouds, mobile IoT computing, IoT cloud computing, fog computing, Mobile Edge Computing (MEC), edge computing, the Web of Things (WoT), the Semantic WoT (SWoT), the Wisdom WoT (W2T), opportunistic sensing, participatory sensing, mobile crowdsensing, and mobile crowdsourcing. Unfortunately, those paradigms suffer from the lack of standard definitions, and so we frequently encounter a single term referring to various paradigms or several terms referring to a single paradigm. Accordingly, this paper attempts to disambiguate those paradigms and explain how and where they fit in the above three areas of research and/or their intersections before it becomes a serious problem. They are tracked back to their inception as much as possible. This is in addition to discussing research directions in each area. The paper also introduces technologies related to the IoT such as ubiquitous and pervasive computing, the Internet of Nano Things (IoNT), and the Internet of Underwater Things (IoUT).
Keywords: Cloudlet computing; Crowdsensing; Crowdsourcing; Edge computing; Fog computing; Internet of things; Mobile cloud; Mobile cloud computing; Mobile edge computing; Opportunistic sensing; Participatory sensing; Semantic web of things; Web of things; Wisdom web of things

Aditya Kashi, Pratik Nayak, Dhruva Kulkarni, Aaron Scheinberg, Paul Lin, Hartwig Anzt,
Integrating batched sparse iterative solvers for the collision operator in fusion plasma simulations on GPUs,
Journal of Parallel and Distributed Computing,
Volume 178,
2023,
Pages 69-81,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.03.012.
(https://www.sciencedirect.com/science/article/pii/S0743731523000540)
Abstract: Batched linear solvers, which solve many small related but independent problems, are increasingly important for highly parallel processors such as graphics processing units (GPUs). GPUs need a substantial amount of work to keep them operating efficiently and it is not an option to solve smaller problems one-by-one. Because of the small size of each problem, the task of implementing a parallel partitioning scheme and mapping the problem to hardware is not trivial. In recent history, significant attention has been given to batched dense linear algebra. However, there is also an interest in utilizing sparse iterative solvers in a batched form. An example use case is found in a gyrokinetic Particle-In-Cell (PIC) code used for modeling magnetically confined fusion plasma devices. The collision operator has been identified as a bottleneck, and a proxy app has been created for facilitating optimizations and porting to GPUs. The current collision kernel linear solver does not run on the GPU—a major bottleneck. As these matrices are sparse and well-conditioned, batched iterative sparse solvers are an attractive option. A batched sparse iterative solver capability has recently been developed in the Ginkgo library. In this paper, we describe how Ginkgo's batched solver technology can integrate into the XGC collision kernel and accelerate the simulation process. Comparisons for the solve times on NVIDIA V100 and A100 GPUs and AMD MI100 GPUs with one dual-socket Intel Xeon Skylake CPU node with 40 cores are presented for matrices from the collision kernel of XGC. Further, the speedups observed for the overall collision kernel are presented in comparison to different modern CPUs on multiple supercomputer systems. The results suggest that Ginkgo's batched sparse iterative solvers are well suited for efficient utilization of the GPU for this problem, and the performance portability of Ginkgo in conjunction with Kokkos (used within XGC as the heterogeneous programming model) allows seamless execution on exascale-oriented heterogeneous architectures.
Keywords: Sparse linear systems; Batched solvers; Plasma simulation; GPU; Performance portability

Omar Nassef, Wenting Sun, Hakimeh Purmehdi, Mallik Tatipamula, Toktam Mahmoodi,
A survey: Distributed Machine Learning for 5G and beyond,
Computer Networks,
Volume 207,
2022,
108820,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108820.
(https://www.sciencedirect.com/science/article/pii/S1389128622000421)
Abstract: 5G is the fifth generation of cellular networks. It enables billions of connected devices to gather and share information in real time; a key facilitator in Industrial Internet of Things (IoT) applications. It has more capabilities in terms of bandwidth, latency/delay, processing powers and flexibility to utilize either edge or cloud resources. Furthermore, 6G is expected to be equipped with the new capability to converge ubiquitous communication, computation, sensing and controlling for a variety of sectors, which heightens the complexity in a more heterogeneous environment This increased complexity, combined with energy efficiency and Service Level Agreement (SLA) requirements makes application of Machine Learning (ML) and distributed ML necessary. A decentralized approach stemming from distributed learning is a very attractive option compared with a centralized architecture for model learning and inference. Distributed ML exploits recent Artificial Intelligence (AI) technology advancements to allow collaborated ML, whilst safeguarding private data, minimizing both communication and computation overhead along with addressing ultra-low latency requirements. In this paper, we review a number of distributed ML architectures and designs, that focus on optimizing communication, computation and resource distribution. Privacy, information security and compute frameworks, are also analyzed and compared with respect to different distributed ML approaches. We summarize the major contributions and trends in this area and highlight the potential of distributed ML to help researchers and practitioners make informed decisions on selecting the right ML approach for 5G and Beyond related AI applications. To enable distributed ML for 5G and Beyond, communication, security, and computing platform often counter balance each other, thus, consideration and optimization of these aspects at an overall system level is crucial to realize the full potential of AI for 5G and Beyond. These different aspects do not only pertain to 5G, but will also enable careful design of distributed machine learning architectures to circumvent the same hurdles that will inevitably burden 5G and Beyond network generations. This is the first survey paper that brings together all these aspects for distributed ML.
Keywords: Machine Learning; Distributed machine learning; Distributed inference; Latency; 5G networks

Sri Harsha Mekala, Zubair Baig, Adnan Anwar, Sherali Zeadally,
Cybersecurity for Industrial IoT (IIoT): Threats, countermeasures, challenges and future directions,
Computer Communications,
Volume 208,
2023,
Pages 294-320,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.06.020.
(https://www.sciencedirect.com/science/article/pii/S0140366423002189)
Abstract: IIoT is evolving as the future of system and process integration across Operations Technology (OT) and Information Technology (IT). The dynamic nature of OT and IT across diverse industrial sectors has led to the emergence of evolving threats which are challenging to address. We present a holistic analysis of the IIoT paradigm, identifying specific domains of IIoT adoption, assessing threats and vulnerabilities, and providing a detailed analysis of existing countermeasures. We also identify future research directions in IIoT cybersecurity to address future threats and vulnerabilities.
Keywords: Industrial Internet of Things (IIoT); Information Technology (IT); Operational Technology (OT); Threats; Vulnerabilities; Edge computing; Cloud computing; Information Control Systems (CPSs); Supervisory Control and Data Acquisition (SCADA); Programmable Logic Controller (PLC); Forensics

Lin Li, Carlo Sau, Tiziana Fanni, Jingui Li, Timo Viitanen, François Christophe, Francesca Palumbo, Luigi Raffo, Heikki Huttunen, Jarmo Takala, Shuvra S. Bhattacharyya,
An integrated hardware/software design methodology for signal processing systems,
Journal of Systems Architecture,
Volume 93,
2019,
Pages 1-19,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2018.12.010.
(https://www.sciencedirect.com/science/article/pii/S1383762118301735)
Abstract: This paper presents a new methodology for design and implementation of signal processing systems on system-on-chip (SoC) platforms. The methodology is centered on the use of lightweight application programming interfaces for applying principles of dataflow design at different layers of abstraction. The development processes integrated in our approach are software implementation, hardware implementation, hardware-software co-design, and optimized application mapping. The proposed methodology facilitates development and integration of signal processing hardware and software modules that involve heterogeneous programming languages and platforms. As a demonstration of the proposed design framework, we present a dataflow-based deep neural network (DNN) implementation for vehicle classification that is streamlined for real-time operation on embedded SoC devices. Using the proposed methodology, we apply and integrate a variety of dataflow graph optimizations that are important for efficient mapping of the DNN system into a resource constrained implementation that involves cooperating multicore CPUs and field-programmable gate array subsystems. Through experiments, we demonstrate the flexibility and effectiveness with which different design transformations can be applied and integrated across multiple scales of the targeted computing system.
Keywords: Dataflow; Model-based design; Hardware/software co-design; Low power techniques; Deep learning; Signal processing systems

Gengbiao Shen, Qing Li, Yong Jiang, Yu Wu, Jianhui Lv,
A four-stage adaptive scheduling scheme for service function chain in NFV,
Computer Networks,
Volume 175,
2020,
107259,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107259.
(https://www.sciencedirect.com/science/article/pii/S1389128619314276)
Abstract: Network Function Virtualization (NFV) enables the flexible software implementation of Network Functions (NFs) which is called Virtualized Network Function (VNF) and placed along the routing path of the network flow. A sequence of VNFs constitutes a Service Function Chain (SFC) to satisfy the processing requirements of flows. Since the SFC scheduling depends on the current network state and the dynamics of flows, it brings a great challenge to make an optimal SFC scheduling decision efficiently. In this paper, we present a Four-stage Adaptive Scheduling Scheme (FSASM) to make a trade-off between different scheduling goals and effects on network performance and management overhead. We design the specific mechanism for each stage when the network is in different workloads. Then, we prove the NP hardness of the optimization models in FSASM and propose a Minimum wEight Path Selection Algorithm (MEPS) with polynomial time complexity to realize a practical SFC scheduling. Moreover, we perform comprehensive experiments under different real-world topologies and network states. The results demonstrate that FSASM can achieve high network throughput and resource utilization as well as decrease the scaling frequency in highly dynamic network scenarios.
Keywords: Service function chain; Network function virtualization; Software-defined networking; Flow scheduling

Venkatarami Reddy Chintapalli, Madhura Adeppady, Bheemarjuna Reddy Tamma, Antony Franklin A.,
RESTRAIN: A dynamic and cost-efficient resource management scheme for addressing performance interference in NFV-based systems,
Journal of Network and Computer Applications,
Volume 201,
2022,
103312,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103312.
(https://www.sciencedirect.com/science/article/pii/S1084804521003003)
Abstract: Network Functions Virtualization (NFV) replaces the conventional middleboxes by their software counterparts known as Virtual Network Functions (VNFs) which run on general-purpose hardware platforms and promise several benefits like reduced cost, ease of deployment, flexibility, etc. However, NFV faces some critical challenges as VNFs running on the same physical hardware still have to compete for shared resources such as Last Level Cache (LLC) and different levels of Memory Bandwidth (MB) (between L2 cache & LLC and LLC & main memory), which might result in unpredictable and variable performance interference to the co-located VNFs deployed. Some recent works have explored mechanisms for allocating LLC dynamically using Cache Allocation Technology (CAT) but they did not look into MB contentions among the co-located VNFs. Dynamic allocation of both LLC and MB to the co-located VNFs remains unexplored. To address this, in this work, by leveraging Intel’s CAT and Memory Bandwidth Allocation (MBA) technologies, we profile different VNFs to determine their minimum LLC and MB resource requirements to achieve performance isolation for different input traffic rates. We then propose a dynamic, joint resource allocation scheme, RESTRAIN, that takes each VNF’s input traffic rate as an input and dynamically adjusts LLC ways and MB resources allocated among them to avoid performance interference and thereby improves the overall resource utilization of the underlying hardware system and the number of VNFs meeting their QoS guarantees. Experimental results on a prototype system show that the proposed RESTRAIN scheme guarantees performance isolation. Further, it improves performance by 30% over a static allocation mechanism and 17% over ResQ, a state-of-the-art scheme.
Keywords: Network Functions Virtualization (NFV); Virtual Network Function (VNF); Performance interference; Last level cache partitioning; Memory bandwidth partitioning; Performance isolation

Surbhi Gupta, Sangeeta R, Ravi Shankar Mishra, Gaurav Singal, Tapas Badal, Deepak Garg,
Corridor segmentation for automatic robot navigation in indoor environment using edge devices,
Computer Networks,
Volume 178,
2020,
107374,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107374.
(https://www.sciencedirect.com/science/article/pii/S1389128620306782)
Abstract: Corridor segmentation is an essential component of the Indoor Navigation System (INS) for robotic activity. It is required for navigation from one place to another in the indoor environment for different applications such as autonomous corridor cleaning, robot-based customer service in hotels, malls, and hospitals. Global Positioning System (GPS) does not work in indoor as it is applicable only in the outdoor environment for navigation and existing beacon-based systems require infrastructure support for mapping the floor plan. The robots with edge devices are resource-constrained. Hence there is a need for memory, computationally, and communicationally efficient solutions. The applications of a robot for these specific task encourages us to solve the navigation problem by visual perception. Image processing based solution cuts the communication cost however it is challenging to stack the right set of operations in proper order for the segmentation. To address such a challenge, this paper builds a right recipe that detects and segments the corridor by real-time image processing and tackles the side obstacles using sensor input. The proposed region-based method using Gray conversion, Canny edge detection, Morphological operations, can process frames at the speed of 6 fps. Its effectiveness is verified by testing on varied real-world illumination scene videos and reported significant precision. With this hardware cum software prototype, an average precision score of 0.82 has been achieved.
Keywords: Path; Robot; Indoor navigation; Computer vision; Lane detection; Freespace detection

Quang-Huy Nguyen, Falko Dressler,
A smartphone perspective on computation offloading—A survey,
Computer Communications,
Volume 159,
2020,
Pages 133-154,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.05.001.
(https://www.sciencedirect.com/science/article/pii/S0140366419319401)
Abstract: Computation offloading has emerged as one of the promising approaches to address the issues of restricted resources, leading to poor user experiences on smartphones in terms of battery life and performance when executing CPU intensive tasks. Meanwhile, an entire research domain has been initiated around this topic and several concepts have been studied touching both the smartphone and the cloud side. In this paper, we develop a categorization of fundamental aspects regarding computation offloading in heterogeneous cloud computing from the perspective of smartphone applications. We refer to heterogeneity in terms of the multitude of smartphone applications, the various uplink channels, and the variety of cloud solutions. We also survey state-of-the-art solutions for the identified categories. Finally, we conclude with a summary of the most important research challenges in making computation offloading reality.
Keywords: Computation offloading; Mobile edge computing; Smartphones; Energy consumption

Meng-yuan Zhu, Zhuo Chen, Ke-fan Chen, Na Lv, Yun Zhong,
Attention-based federated incremental learning for traffic classification in the Internet of Things,
Computer Communications,
Volume 185,
2022,
Pages 168-175,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.01.006.
(https://www.sciencedirect.com/science/article/pii/S0140366422000123)
Abstract: The Internet of Things (IoT) traffic follows non-independent and identical distribution (non-IID). Traditional machine learning classification methods will cause low classification accuracy, high communication costs, and privacy leakage issues. Federated Learning enables several clients to train a deep learning model collaboratively without requiring any of the clients to share their local data with a centralized server. In this paper, we propose a novel attention-based federated incremental learning algorithm: Fed-SOINN. We introduce the attention mechanism to improve the weight of parameters uploaded by clients which are beneficial to the global model, where instead of the full gradient, only a small subset of important gradients is communicated. Meanwhile, we improve the sparsity of the model by upgrading the online optimization function in Fed-SOINN, it also brings faster convergence speed and higher accuracy in the changeable network environment. Results reveal that Fed-SOINN has improved detection accuracy by 3.1% compared with benchmark methods and can reduce the number of communications rounds up to 73%. When facing new traffic categories, the incremental learning mechanism in Fed-SOINN also effectively identify unknown traffic categories.
Keywords: Federated learning; Traffic classification; Attention mechanism; Internet of Things; Incremental learning

Boubakr Nour, Kashif Sharif, Fan Li, Sujit Biswas, Hassine Moungla, Mohsen Guizani, Yu Wang,
A survey of Internet of Things communication using ICN: A use case perspective,
Computer Communications,
Volumes 142–143,
2019,
Pages 95-123,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.05.010.
(https://www.sciencedirect.com/science/article/pii/S0140366418309228)
Abstract: Internet of Things (IoT) has gained extensive attention from industry and academia alike in past decade. The connectivity of each and every piece of technology in the environment with Internet, has opened many avenues of research and development. Applications, algorithms, trust models, devices, all have evolved to accommodate the demands of user needs in the most optimal way possible. However, one thing still remains constant: host-centric communication. It is the most predominant way of communication in Internet today. With evolution of everything else, host based communication has been stretched to limits, and exploration of new models have been underway for sometime. Information Centric Networking (ICN) is a major contender for the future Internet architecture, where content is the basic element regardless of its location (host). It intends to offer in-network caching, inherent mobility, multicast support, and content based security as part of design and not add-on functionality. In recent years, numerous efforts have been made to integrate IoT with ICN as the communication model. In this paper, we provide a detailed and systematic review of IoT–ICN research. We investigate ICN as communication enabler for IoT domain specific use cases, and the use of ICN features for the benefit of IoT networks. These include IoT device & content naming, discovery, and caching. We also survey synchronization, interoperability, publish/subscribe communication, quality of service, security, and mobility of IoT devices with ICN perspectives. The paper also presents challenges and possible research directions for the benefit of community.
Keywords: Information-centric networking (ICN); Named-data networking (NDN); Content-centric networking (CCN); Internet of things (IoT)

Saptarshi Debroy, Priyanka Samanta, Amina Bashir, Mainak Chatterjee,
SpEED-IoT: Spectrum aware energy efficient routing for device-to-device IoT communication,
Future Generation Computer Systems,
Volume 93,
2019,
Pages 833-848,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.01.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X17307148)
Abstract: In order to meet the growing demands for high-throughput, cost-effective, and energy efficient solution for the emerging device-to-device (D2D) based Internet of Things (IoT) communication, Dynamic Spectrum Access (DSA) and sharing based protocols have been proposed. However, due to the temporal and spatial transience of spectrum utilization by licensed incumbents, optimal spectrum resource management becomes critical for: (a) effective D2D communication without disrupting the licensed incumbents, and (b) sustained operation in a multi-hop mesh environment due to the inherent energy constraint of IoT devices. In this paper, we propose SpEED-IoT: Spectrum aware Energy- Efficient multi-hop multi-channel routing scheme for D2D communication in IoT mesh network. We assume the knowledge of a radio environment map (REM) obtained through dedicated spectrum sensors that capture the spatio-temporal spectrum usage. We exploit such REMs to propose a multi-hop routing scheme that finds the: (a) best route, (b) best available channels at each hop along the route, and (c) optimal transmission power for each hop. SpEED-IoT also employs an evolutionary game theoretic route allocation model to sustain parallel D2D communication. SpEED-IoT ensures: (i) licensed incumbent protection, (ii) IoT device energy preservation, (iii) effective end-to-end data rate optimization, and (iv) fast convergence and fair route assignment among interfering D2D communications. Through simulation-driven GENI-based IoT testbed, we evaluate SpEED-IoT’s performance in terms of: (a) ensuring connectivity and reachability among the IoT devices under varying spectrum usage conditions, (b) data rate optimization of the assigned routes and the overall IoT network, (c) effectiveness in licensed incumbent protection, and (d) degree of fairness while assigning routes to multiple interfering devices.

Abhishek Singh, Ashish Payal, Sourabh Bharti,
A walkthrough of the emerging IoT paradigm: Visualizing inside functionalities, key features, and open issues,
Journal of Network and Computer Applications,
Volume 143,
2019,
Pages 111-151,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.06.013.
(https://www.sciencedirect.com/science/article/pii/S1084804519302188)
Abstract: Internet of Things (IoT) is often envisioned as a paradigm shift from the traditional Internet to a scenario where all the “things” will be connected with the Internet. IoT forced the development of novel IoT standards to bring heterogeneous devices and protocols under one umbrella for the seamless interaction between them. To this end, a plethora of IoT standards and definitions have been proposed over the last decade. However, a common consensus over these standards and definitions is yet to be achieved. Another contemporary issue faced by IoT research community is the absence of a clear functional view of different IoT components working together in a cohesive manner to achieve the envisioned “connect everything” phenomena that serve the bigger objective of gathering the information and processing it for better decision making. Information gathering and processing bring the technologies such as “Cloud Computing” and “Big-Data” into the picture with IoT. In the line of energy efficient data processing, “Fog Computing” was introduced a few years back that is one of the key enablers of the contemporary industrial revolution often termed as “Industry 4.0”. However, there is still a need for a detailed study about how “Cloud computing”, “Big data”, and “Fog computing” fit together and act as building blocks of the much envisioned IoT. In this paper, we study the aforementioned issues in the following manner. First, we cover almost every IoT standard and definition available in the literature and propose an unbiased definition of IoT that takes into consideration the key objectives of various research groups. We also discuss the key components of IoT architecture and propose a functional view of IoT based on Weiser's model and information value loop that helps to specify a clear functionality of various IoT enablers. Following this, a detail discussion on information gathering and processing in IoT is supported by state-of-the-art and contemporary technologies such as Cloud computing and Fog computing respectively. We also propose a new perspective of IoT as social processes and a decentralized multi-agent system in order to address not so obvious future challenges that may arise as the research in IoT proceeds and new application domains are discovered. All in all, this survey provides a birds-eye view for the domain as well as introduce intricacies about the IoT functionality, enabling technologies, challenges, future research trends and directions. In the end, this paper discusses the state-of-the-art and contemporary simulations tools that can be used to implement the IoT based projects. All in all, this paper gives a much wider view of almost every technology related to IoT and its bigger objective of gathering more information and processing it according to the application requirement for better decision making.
Keywords: Cyber physical system (CPS); Cloud computing; Internet connected systems (ICT); Internet of things (IoT); Fog computing; Situation/context awareness; Cross-layer architectures; Wireless sensor networks (WSN); Wireless sensor & actuation networks (WSAN); Multi-agent systems (MAS); Trust management; Industry 4.0

Hyunseok Chang, Fang Hao, Murali Kodialam, T.V. Lakshman, Sarit Mukherjee, Matteo Varvello,
Towards network-assisted publish–subscribe over wide area networks,
Computer Networks,
Volume 231,
2023,
109702,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109702.
(https://www.sciencedirect.com/science/article/pii/S1389128623001470)
Abstract: The Internet is poised to support new classes of industrial applications driven by large numbers of sensors and actuators generating and consuming sensor data streams. While the publish–subscribe (PubSub) architecture is well-suited for such large-scale data dissemination, existing over-the-top PubSub systems are not only vulnerable to network-induced performance degradation, but also unable to orchestrate the underlying network for better usage. This paper proposes a network-assisted PubSub architecture called SNAPS which enables seamless control of distributed PubSub brokers at the edge, as well as the networks that interconnect them. SNAPS leverages SRv6-based network programmability to build and maintain network-efficient, shareable data distribution trees across one or more administrative domains. The architecture is independent of broker implementation, making it suitable for “bringing your own brokers” at available edge locations. We describe how the networks interconnecting the brokers can be programmed for efficient, shareable data distribution via SRv6. Through simulation and prototype evaluation, we show the efficacy of SNAPS in providing better service for PubSub data delivery and better usage of the network resources.
Keywords: Publish–subscribe; Network architecture; Network management; Programmable networks; SRv6; IoT

Omar Abdel Wahab, Nadjia Kara, Claes Edstrom, Yves Lemieux,
MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions,
Journal of Network and Computer Applications,
Volume 142,
2019,
Pages 37-50,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.06.003.
(https://www.sciencedirect.com/science/article/pii/S1084804519301924)
Abstract: As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches.
Keywords: Network function virtualization; Cloud computing; Machine learning; Data-driven optimization; Semi-supervised learning

Abu Jahid, Mohammed H. Alsharif, Trevor J. Hall,
The convergence of blockchain, IoT and 6G: Potential, opportunities, challenges and research roadmap,
Journal of Network and Computer Applications,
Volume 217,
2023,
103677,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103677.
(https://www.sciencedirect.com/science/article/pii/S1084804523000966)
Abstract: 6G networks envisioned being the game changer in next generation wireless communication systems that will address the challenges of limited information speed escalated with the augmentation of billions of data applications encountered by the current fifth generation (5G) networks. Some key radical technologies in 6G together with existing 5G candidate schemes will guarantee the expected quality of experience (QoE) to attain ubiquitous wireless connectivity for the Internet of Everything (IoE) ranging from the telecom industry to digital smart industries. Blockchain technology (BCT) has gained significant attention due to undertake the decentralization, transparency, spectrum resource scarcity, inherent privacy and security, interoperability, confidentiality, and emerging smart applications domains including Industrial IoT (IIoT) and Industry 5.0 applications. The mismatch between the requirements of many data intensive disruptive IoT applications and 5G network capabilities steered the demand of decentralized BCT based 6G architecture. Inspired by these facts, this paper studies an extensive survey to draw a new direction of blockchain integration into 6G mobile networks, IoT technologies, and smart industries in terms of infrastructure sharing, computational loads, latency, bandwidth overhead, business model, sustainability goals, and edge intelligence focusing the potential merits and challenges. We highlighted the convergence of IoT in blockchain to enable intelligent distribution in future IIoT and the technical model of 6G networks to realize the successful deployment of BCT schemes. This paper pointed out the current intriguing challenges, canvassed the mitigation techniques, and plausible future research opportunities that may privilege the pursuit of this vision.
Keywords: Blockchain; 6G; Internet of things; Industry 4.0; Industrial IoT; 6G business model; Privacy and security

Zhutian Lin, Xi Xiao, Guangwu Hu, Qing Li, Bin Zhang, Xiapu Luo,
Tracking phishing on Ethereum: Transaction network embedding approach for accounts representation learning,
Computers & Security,
Volume 135,
2023,
103479,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103479.
(https://www.sciencedirect.com/science/article/pii/S0167404823003899)
Abstract: The transaction volume of Ethereum has been witnessing a year-on-year increase, which has unfortunately been accompanied by significant losses due to phishing scams. To enhance the ability of downstream classifiers to distinguish phishing accounts more effectively, we produce dense representations of Ethereum accounts in latent space, leveraging the transaction network topology and associated statistical features. However, the task of learning representations from sparse yet voluminous transaction records presents a significant challenge. To address this, we introduce the Temporal-based Sequences Generator (TSG) and the Heterogeneous-based Sequences Generator (HSG). These generators create sequences from the transaction network, optimizing the use of transaction temporal constraints, diverse account types, and transaction amounts. Our method aims to capture latent higher-order information and generate dense vectors using a network embedding technique. Furthermore, we propose a novel Statistics-Based Sampling (SBS) method to mitigate label leakage. We validate our approach through experiments with various classic downstream classifiers, demonstrating that Phish2vec surpasses other comparative methods in performance and exhibits robustness and stability.
Keywords: Ethereum; Phishing scams; Representation learning; Blockchain security; Transaction network

Thomas Parnell, Celestine Dünner, Kubilay Atasu, Manolis Sifalakis, Haralampos Pozidis,
Tera-scale coordinate descent on GPUs,
Future Generation Computer Systems,
Volume 108,
2020,
Pages 1173-1191,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.04.072.
(https://www.sciencedirect.com/science/article/pii/S0167739X17318204)
Abstract: In this work we propose an asynchronous, GPU-based implementation of the widely-used stochastic coordinate descent algorithm for convex optimization. We define the class of problems that can be solved using this method, and show that it includes many popular machine learning applications. For three such applications, we demonstrate at least a 10× training speed-up relative to a state-of-the-art implementation that uses all available resources of a modern CPU. In order to train on very large datasets that do not fit inside the memory of a single GPU, we then consider techniques for distributed learning. We show that while such techniques do not necessarily allow one to achieve further speed-up, they do allow one to train on datasets that would otherwise not fit into memory. We thus propose a distributed learning system that uses the synchronous CoCoA framework to distribute the global optimization across GPUs, and our novel asynchronous algorithm to solve the corresponding local optimizations within each GPU. We benchmark such a system using a 200 GB dataset that consists of 1 billion training examples. We show by scaling out across 16 GPUs, we can train an SVM model to a high degree of accuracy in around 1 min: a 15× speed-up in training time compared to a state-of-the-art CPU-based implementation that uses 640 threads distributed across 8 CPUs.
Keywords: Optimization; Machine learning; GPU; Distributed computing; Big data

Dalila Ressi, Riccardo Romanello, Carla Piazza, Sabina Rossi,
AI-enhanced blockchain technology: A review of advancements and opportunities,
Journal of Network and Computer Applications,
Volume 225,
2024,
103858,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103858.
(https://www.sciencedirect.com/science/article/pii/S1084804524000353)
Abstract: Blockchain technology has rapidly gained popularity, permeating various fields due to its inherent features of security, transparency, and decentralization. Blockchain-based applications, spanning from financial transactions to supply chain management, have revolutionized numerous industries. Concurrently, Artificial Intelligence (AI) techniques have emerged as a powerful tool for efficiently solving complex problems. The integration of AI into blockchain applications has shown promise in addressing key challenges such as security, consensus, scalability, and interoperability. While existing literature offers several surveys on the intersection of AI and blockchain, our work takes a distinct perspective by focusing on how AI solutions can enhance and optimize blockchain technology and its applications. Our goal is to provide a comprehensive literature overview of the methods that have been employed to improve blockchain technology through AI, encompassing machine learning, deep learning, natural language processing and reinforcement learning. Our contribution highlights AI’s potential to enhance blockchain, improving efficiency, security, and reliability of blockchain-based applications. By exploring AI’s role in consensus, smart contracts, and data privacy, it advances theory and practical applications, fostering innovation across sectors for a more secure and efficient digital future.
Keywords: Blockchain; Artificial intelligence; Machine learning

Emilio Calvanese Strinati, Sergio Barbarossa,
6G networks: Beyond Shannon towards semantic and goal-oriented communications,
Computer Networks,
Volume 190,
2021,
107930,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107930.
(https://www.sciencedirect.com/science/article/pii/S1389128621000773)
Abstract: The goal of this paper is to promote the idea that including semantic and goal-oriented aspects in future 6G networks can produce a significant leap forward in terms of system effectiveness and sustainability. Semantic communication goes beyond the common Shannon paradigm of guaranteeing the correct reception of each single transmitted bit, irrespective of the meaning conveyed by the transmitted bits. The idea is that, whenever communication occurs to convey meaning or to accomplish a goal, what really matters is the impact that the received bits have on the interpretation of the meaning intended by the transmitter or on the accomplishment of a common goal. Focusing on semantic and goal-oriented aspects, and possibly combining them, helps to identify the relevant information, i.e. the information strictly necessary to recover the meaning intended by the transmitter or to accomplish a goal. Combining knowledge representation and reasoning tools with machine learning algorithms paves the way to build semantic learning strategies enabling current machine learning algorithms to achieve better interpretation capabilities and contrast adversarial attacks. 6G semantic networks can bring semantic learning mechanisms at the edge of the network and, at the same time, semantic learning can help 6G networks to improve their efficiency and sustainability.
Keywords: 6G; Beyond 5G; MEC; Semantic communications; Semantic learning; Goal oriented communications; Sustainability; Green communications

Clément Mommessin, Renyu Yang, Natalia V. Shakhlevich, Xiaoyang Sun, Satish Kumar, Junqing Xiao, Jie Xu,
Affinity-aware resource provisioning for long-running applications in shared clusters,
Journal of Parallel and Distributed Computing,
Volume 177,
2023,
Pages 1-16,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.02.011.
(https://www.sciencedirect.com/science/article/pii/S0743731523000321)
Abstract: Resource provisioning plays a pivotal role in determining the right amount of infrastructure resource to run applications and reduce the monetary cost. A significant portion of production clusters is now dedicated to long-running applications (LRAs), which are typically in the form of microservices and executed in the order of hours or even months. It is therefore practically important to plan ahead the placement of LRAs in a shared cluster for the minimized number of compute nodes required by them. Existing works on LRA scheduling are often application-agnostic, without particularly addressing the constraining requirements imposed by LRAs, such as co-location affinity constraints and time-varying resource requirements. In this paper, we present an affinity-aware resource provisioning approach for deploying large-scale LRAs in a shared cluster subject to multiple constraints, with the objective of minimizing the number of compute nodes in use. We investigate a broad range of solution algorithms which fall into three main categories: Application-Centric, Node-Centric, and Multi-Node approaches, and tune them for typical large-scale real-world scenarios. Experimental studies driven by the Alibaba Tianchi dataset show that our algorithms can achieve competitive scheduling effectiveness and running time, as compared with the heuristics used by the latest work including Medea and LraSched. Best results are obtained by the Application-Centric algorithms, if the algorithm's running time is of primary concern, and by Multi-Node algorithms, if the solution quality is of primary concern.
Keywords: Resource scheduling; Long-running applications; Vector bin packing

Laurens Versluis, Alexandru Iosup,
A survey of domains in workflow scheduling in computing infrastructures: Community and keyword analysis, emerging trends, and taxonomies,
Future Generation Computer Systems,
Volume 123,
2021,
Pages 156-177,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.04.009.
(https://www.sciencedirect.com/science/article/pii/S0167739X21001308)
Abstract: Workflows are prevalent in today’s computing infrastructures as they support many domains. Different Quality of Service (QoS) requirements of both users and providers makes workflow scheduling challenging. Meeting the challenge requires an overview of state-of-art in workflow scheduling. Sifting through literature to find the state-of-art can be daunting, for both newcomers and experienced researchers. Surveys are an excellent way to address questions regarding the different techniques, policies, emerging areas, and opportunities present, yet they rarely take a systematic approach and publish their tools and data on which they are based. Moreover, the communities behind these articles are rarely studied. We attempt to address these shortcomings in this work. We introduce and open-source an instrument used to combine and store article meta-data. Using this meta-data, we characterize and taxonomize the workflow scheduling community and four areas within workflow scheduling: (1) the workflow formalism, (2) workflow allocation, (3) resource provisioning, and (4) applications and services. In each characterization, we obtain important keywords overall and per year, identify keywords growing in importance, get insight into the structure and relations within each community, and perform a systematic literature survey per part to validate and complement our taxonomies
Keywords: Cloud; Cluster; Grid; Workflow; Scheduling; Survey; Taxonomy; Formalism; Allocation; Provisioning; Applications; Services; Policies; Community; Meta-analysis

Sumanth Umesh, Sparsh Mittal,
A survey of techniques for intermittent computing,
Journal of Systems Architecture,
Volume 112,
2021,
101859,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101859.
(https://www.sciencedirect.com/science/article/pii/S1383762120301430)
Abstract: Intermittent computing (ImC) refers to the scenario where periods of program execution are separated by reboots. ImC systems are generally powered by energy-harvesting (EH) devices: they start executing a program when the accumulated energy reaches a threshold and stop when the energy buffer is exhausted. Since ImC does not depend on a fixed supply of power, it can be used in a wide range of scenarios/devices such as medical implants, wearables, IoT sensors, extraterrestrial systems and so on. Although attractive, ImC also brings challenges such as avoiding data-loss and data inconsistency, and striking the right balance between performance, energy and quality of the result. In this paper, we present a survey of techniques and systems for ImC. We organize the works on key metrics to expose their similarities and differences. This paper will equip researchers with the knowledge of recent developments in ImC and also motivate them to address the remaining challenges for reaping the full potential of ImC.
Keywords: Review; Incidental computing; Energy harvesting; DVFS; Checkpointing; Debugging; Approximate computing; IoT

Weisu Wang, Christopher Meyers, Robert Roy, Sarah Diesburg, An-I Andy Wang,
ADAPT: An auxiliary storage data path toolkit,
Journal of Systems Architecture,
Volume 113,
2021,
101902,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101902.
(https://www.sciencedirect.com/science/article/pii/S1383762120301727)
Abstract: The legacy storage data path is largely structured in black-box layers and has four major limitations: (1) functional redundancies across layers, (2) poor cross-layer coordination and data tracking, (3) presupposition of high-latency storage devices, and (4) poor support for new storage data models. While addressing all these limitations is a daunting challenge, we introduce ADAPT, an auxiliary storage data path toolkit that complements the legacy storage data path to help mitigate these limitations. This toolkit enables all storage layers to coordinate and track data using shared data structures constructed through the ADAPT API. Our case studies have shown that we can directly support applications such as a key-value store without going through the file system. We also built an ADAPT-based file system and prioritized caching to demonstrate the usability, extensibility, and robustness of ADAPT. In addition, we built per-file secure deletion via our ADAPT-based file system to demonstrate data-path-wide coordination and data tracking.
Keywords: Storage data path and file systems

Hourieh Khalajzadeh, Dong Yuan, Bing Bing Zhou, John Grundy, Yun Yang,
Cost effective dynamic data placement for efficient access of social networks,
Journal of Parallel and Distributed Computing,
Volume 141,
2020,
Pages 82-98,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.03.013.
(https://www.sciencedirect.com/science/article/pii/S0743731518301710)
Abstract: Social networks boast a huge number of worldwide users who join, connect, and publish various content, often very large, e.g. videos, images etc. For such very large-scale data storage, data replication using geo-distributed cloud services with virtually unlimited capabilities are suitable to fulfill the users’ expectations, such as low latency when accessing their and their friends’ data. However, service providers ideally want to spend as little as possible on replicating users’ data. Moreover, social networks have a dynamic nature and thus replicas need to be adaptable according to the environment, users’ behaviors, social network topology, and workload at runtime. Hence, it is not only crucial to have an optimized data placement and request distribution – meeting individual users’ acceptable latency requirements while incurring minimum cost for service providers – but the data placement must be adapted based on changes in the social network to keep it efficient and effective over time. In this paper, we model data placement as a dynamic set cover problem and propose a novel approach to solve this problem. We have run several experiments using two large-scale, open Facebook and Gowala datasets and real latencies derived from Amazon cloud datacenters to demonstrate our novel strategy’s efficiency and effectiveness.
Keywords: Access latency; Cost optimization; Data placement; Data replication; Social networks

Mir Salim Ul Islam, Ashok Kumar, Yu-Chen Hu,
Context-aware scheduling in Fog computing: A survey, taxonomy, challenges and future directions,
Journal of Network and Computer Applications,
Volume 180,
2021,
103008,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103008.
(https://www.sciencedirect.com/science/article/pii/S1084804521000357)
Abstract: Fog computing extends Cloud-based facilities and stays in the vicinity of the end-users to provide an attractive solution to a diverse range of latency-sensitive applications. The applications are becoming more sophisticated, context-aware, and computation-intensive due to varying situational and environmental conditions in order to meet the ever-increasing users’ demands. Further, resource heterogeneity, dynamic nature, resource limitations, and unpredictability of the Fog environment make scheduling of application tasks while satisfying Quality of Service (QoS) requirements a challenging job. To overcome these issues various scheduling strategies have been proposed considering contextual information of different entities involved in Fog computing. This survey represents a comprehensive literature analysis pertaining to context-aware scheduling in Fog computing. It provides detailed comparison of existing scheduling approaches based on important factors such as context-aware parameters, case studies, performance metrics, and evaluation tools along with advantages and limitations. It also presents detailed taxonomy, performance metrics, and context-aware parameter analysis. Further, it list several issues and challenges. This study will aid the research community in exploring future research directions and essential aspects of scheduling approaches using different types of contextual information.
Keywords: Fog computing; Context-awareness; Scheduling; Resource management; Resource estimation; Resource provisioning; Contextual information

Firat Karatas, Ibrahim Korpeoglu,
Fog-Based Data Distribution Service (F-DAD) for Internet of Things (IoT) applications,
Future Generation Computer Systems,
Volume 93,
2019,
Pages 156-169,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.10.039.
(https://www.sciencedirect.com/science/article/pii/S0167739X18315310)
Abstract: With advances in technology, devices, machines, and appliances get smarter, more capable and connected to each other. This defines a new era called Internet of Things (IoT), consisting of a huge number of connected devices producing and consuming large amounts of data that may be needed by multiple IoT applications. At the same time, cloud computing and its extension to the network edge, fog computing, become an important way of storing and processing large amounts of data. Then, an important issue is how to transport, place, store, and process this huge amount of IoT data in an efficient and effective manner. In this paper, we propose a geographically distributed hierarchical cloud and fog computing based IoT architecture, and propose techniques for placing IoT data into the components, i.e., cloud and fog data centers, of the proposed architecture. Data is considered in different types and each type of data may be needed by multiple applications. Considering this fact, we model the data placement problem as an optimization problem and propose algorithms for efficient and effective placement of data generated and consumed by geographically distributed IoT nodes. Data used by multiple applications is stored only once in a location that is efficiently accessed by applications needing that type of data. We perform extensive simulation experiments to evaluate our proposal and the results show that our architecture and placement techniques can place and store data efficiently while providing good performance for applications and network in terms of access latency and bandwidth consumed.
Keywords: Internet of Things; Fog computing; Data placement; Cloud computing; Network topology; Data management

Wai-Xi Liu, Jun Cai, Ying-Hao Zhu, Jun-Ming Luo, Jin Li,
Load balancing inside programmable data planes based on network modeling prediction using a GNN with network behaviors,
Computer Networks,
Volume 227,
2023,
109695,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109695.
(https://www.sciencedirect.com/science/article/pii/S1389128623001408)
Abstract: In data center networks, existing control plane- and end host-based load-balancing methods are encumbered by excessively large decision delays during rapid reactions to microbursts. However, existing programmable data plane-based load balancing methods require large overheads involved in active probing. Accurate network modeling can optimize load balancing. However, existing modeling methods suffer from low generalization and high overhead. In this study, we propose a network modeling method based on a graph neural network (GNN) with basic network behavior (hereinafter called GNN-Behavior). This is derived from two inherent correlations observed: the correlation between global network behavior and basic network behavior and the correlation among basic network behaviors. We employed the GNN with an improved message-passing neural network to learn such two inherent correlations. Particularly, we considered modeling end-to-end delay as a use case to validate GNN-Behavior. Furthermore, we propose a packet-level load-balancing scheme inside programmable data planes (PDPs) based on the accurate prediction of end-to-end delay from the GNN-Behavior model(LBPP). LBPP is a control plane-PDP collaborative method that integrates a global view from a controller and quick response from switches. Experimental results demonstrate the feasibility and effectiveness of the GNN-Behavior and LBPP. Compared with queuing theory (QT), RouteNet, and GNN-based scheme, GNN-Behavior increases goodness of fit (R2) by 73.1%, 11.1%, and 3.74%, respectively. Under an unknown traffic control strategy, the generalization ability of GNN-Behavior is considerably better than that of QT and RouteNet. Compared with flow-level ECMP, flowlet-level LetFlow, and packet-level DRILL, LBPP can reduce average flow completion time by up to 43.9%, 37.4%, and 17.2%, respectively.
Keywords: Network modeling; Graph neural networks; Network behavior; Load balancing; Programmable data plane

Jiwei Huang, Han Gao, Shaohua Wan, Ying Chen,
AoI-aware energy control and computation offloading for industrial IoT,
Future Generation Computer Systems,
Volume 139,
2023,
Pages 29-37,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.09.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002916)
Abstract: In Industrial Internet of Things (IIoT), a large volume of data is collected periodically by IoT devices, and timely data routing and processing are important requirements. Age of Information (AoI), which is a metric to evaluate the freshness of status information in data processing, has become one of the most important objectives in IIoT. In this paper, considering limited communication, computation and energy resources on IoT devices, we jointly study the optimal AoI-aware energy control and computation offloading problem within a dynamic IIoT scenario with multiple IoT devices and multiple edge servers. Based on extensive analysis of real-life IoT dataset, Markovian queueing models are constructed to capture the dynamics of IoT devices and edge servers, and their corresponding analyses are provided. With the quantitative analytical results, we formulate a dynamic Markov decision problem with the objective of minimizing the long-term energy consumption while satisfying AoI constraints for real-time data processing. To solve the problem, we apply Deep Reinforcement Learning (DRL) techniques for adapting to large-scale dynamic IIoT environments, and design an intelligent Energy Control and Computation Offloading (ECCO) algorithm. Extensive simulation experiments are conducted based on real-world dataset, and the comparison results illustrate the superiority of our ECCO algorithm over both existing DRL and non-DRL algorithms.
Keywords: Age of Information (AoI); Computation offloading; Industrial Internet of Things (IIoT); Deep Reinforcement Learning (DRL)

Zhe Zhao, Lu Gao, Weitao Pan, Zhiliang Qiu, Xu Guo, Ya Gao, Ling Zheng,
Access mechanism for period flows of non-deterministic end systems for time-sensitive networks,
Computer Networks,
Volume 231,
2023,
109805,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109805.
(https://www.sciencedirect.com/science/article/pii/S1389128623002505)
Abstract: The IEEE 802.1Qbv standard schedules time-triggered (TT) flows (i.e., period flows) in a fixed TT Window each period. However,period flows generated by non-deterministic end systems exhibit significant jitter, whichleads to a mismatch between the generation times and the scheduled TT Windows. In the worst case scenario, this mismatch can cause an additional send delay of approximately one period in the flow. In this study,we model the send delay issue due to the maximum send delay requirement for all frames in a period flow not being satisfied simultaneously, and measure the jitter of period flows in a typical non-deterministic end system. Moreover, we propose an access mechanism for jittered period flows to schedule multiple conflict-free TT Windows in each period of flows at the source end system and control the send delay within the required delay tolerance. This mechanism enables deterministic access to jittered period flows, providing a prerequisite for reliable end-to-end transmission in the network. Moreover, this mechanism adopts a multi-objective integer linear programming (MILP) solver to optimize the TT Windows schedule. Furthermore, we establish the constraints and objective functions for the MILP solver and evaluate the mechanism with actual sampled frames in a real non-deterministic end system. Compared with the conventional fixed single TT Window and worst-case delay analysis mechanisms, the proposed mechanism satisfies the send delay requirements and considerably reduces the buffer usage at the source end system.
Keywords: Time-sensitive network; Jittered period flows; Access mechanism; Send delay control; Integer linear programming-based scheduling

Divya Pandey, Vandana Kushwaha,
An exploratory study of congestion control techniques in Wireless Sensor Networks,
Computer Communications,
Volume 157,
2020,
Pages 257-283,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.04.032.
(https://www.sciencedirect.com/science/article/pii/S0140366419320055)
Abstract: Congestion is one of the pervasive issues for Wireless Sensor Networks(WSNs) because of its bounded resources with respect to data processing, storage, transmission capacity and most importantly energy supply. A multitude of survey operations have been carried out over the previous decade to investigate and address multiple congestion-oriented problems and issues that are still unresolved. Various review articles published in the literature concentrated mainly on classical techniques for congestion control such as traffic-based, resource-based, and hybrid techniques, etc. In this survey article, an attempt has been made to present a systematic review of recent efforts assisted at refining the congestion control methodologies in WSNs by considering classical approaches as well as soft computing based approaches. It is a practicable idea to take a holistic view and study both approaches together. Finally, this article discusses design difficulties, various optimization models and future directions for the mechanism of congestion control in the domain of wireless sensor networks.
Keywords: Computational intelligence; Congestion control; Energy efficiency; Optimization; Soft computing

Daniel Happ, Suzan Bayhan, Vlado Handziski,
JOI: Joint placement of IoT analytics operators and pub/sub message brokers in fog-centric IoT platforms,
Future Generation Computer Systems,
Volume 119,
2021,
Pages 7-19,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.01.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X21000364)
Abstract: Internet of Things (IoT) systems are expected to generate a massive amount of data that needs to be processed. Given the large scale and geo-distributed nature of such systems, fog computing along with publish/subscribe (pub/sub) messaging has been proposed as possible solutions for coping with processing at scale. However, it is still unclear how practitioners can leverage the benefits of fog computing, e.g., how to optimally place data processing operators and pub/sub brokers. Moreover, current IoT systems typically rely on pub/sub brokers at the cloud, which might diminish the benefits offered by edge or fog processing as the communication between IoT operators has to be mediated by the brokers located in the cloud. To address this shortcoming, we propose to place the IoT application operators and the pub/sub brokers jointly on a network of nodes spanning from edge to the cloud considering various factors such as network topology or the locations of the IoT sensors and the consumers of the IoT applications. Different than the prior works, we specifically consider pub/sub brokers and their unique characteristics in the placement decision. First, we formulate the placement of operators and brokers jointly across edge, fog, and the cloud as a cost minimization problem. Next, we design two low-complexity heuristics. Our simulation results corroborate the argument that a placement in the cloud is usually a good option for IoT use cases, but also reveal the gap to the optimal solution in scenarios with heavier clustering of producers and consumers of sensor data. Studying the optimality gap shows that in such a setting heuristic solutions usually stay under a stretch factor of 2, with a worst case factor of 2.5 for a tabu-based solution and 2.85 for a greedy and a fixed placement in the cloud.
Keywords: Publish/subscribe; Operator placement; IoT; Cloud; Fog; Brokerage

Anatolij Zubow, Ahmad Rostami, Suzan Bayhan,
On practical cooperative multi point transmission for 5G networks,
Computer Networks,
Volume 171,
2020,
107105,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107105.
(https://www.sciencedirect.com/science/article/pii/S138912861830803X)
Abstract: 5G networks are expected to drastically enhance the existing broadband wireless connectivity in many ways including higher bandwidth, lower delay, and wider coverage. To provide the required spectral efficiency, a key technology that could support the 5G vision is a transmission scheme based on Cooperative Multi Point Transmission (CoMP) combined with Orthogonal Frequency-Division Multiplexing (OFDM). This, however, requires an optimum resource allocation in a CoMP-OFDM system, which is shown to be a highly complex problem. Our proposal, referred to as scalable CoMP (SCoMP), tackles this complexity by dividing the optimal resource allocation problem into four subproblems such that CoMP becomes scalable and applicable in a practical 5G network without a huge overhead in terms of protocol signaling and backhaul requirements. Namely, these four subproblems are as follows: base station (BS) group formation (clustering), assignment of mobile stations (MS) to BS groups, MS group formation and OFDM frame construction, which are solved independently and therefore at lower complexity. Our approach improves its scalability even further by applying a static but overlapping BS grouping scheme which is MS-unaware. We decrease the potential performance loss due to MS-unawareness that is needed for improved scalability by two mechanisms. First, we propose to use overlapping BS clusters supporting variable cluster sizes which mitigates inter-cluster interference. Second, we apply dynamic grouping of MSs to make sure, only spatially-compatible MSs are being served simultaneously in the same group. Finally, our approach is able to deal with bursty network traffic. The realisation of SCoMP is enabled through the deployment of the software defined networking approaches in the backhaul network.
Keywords: Cooperative multi point transmission; OFDM; Wireless networks; SDN

Jin Fang, Gongming Zhao, Hongli Xu, Huaqing Tu, Haibo Wang,
Reveal: Robustness-aware VNF placement and request scheduling in edge clouds,
Computer Networks,
Volume 233,
2023,
109882,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109882.
(https://www.sciencedirect.com/science/article/pii/S1389128623003274)
Abstract: In the edge cloud network, service providers place virtual network functions (VNFs) in edge clouds to serve users’ requests. Thus, it is essential to consider VNF placement and request scheduling in edge clouds. Existing works often focus on minimizing request completion time or maximizing network throughput to utilize network resources and ensure users’ QoS efficiently. However, they ignore two practical factors: malicious users and failed VNFs, leading to poor network robustness. To this end, this paper studies robustness-aware VNF placement and request scheduling, named Reveal. Specifically, we limit the number of VNFs each user can access and the number of users each VNF can serve to control the influence scope of malicious users and VNF failures. Since placing VNFs is time-consuming and requests arrive dynamically, we solve this problem through two phases: robust VNF placement and assignment, and online request scheduling. For the first phase, we design an efficient knapsack-based rounding algorithm with bounded approximation factors. For online request scheduling, we propose a primal–dual based algorithm with a competitive ratio of 1−ϵ,O(log1/ϵ) where ϵ∈(0,1). Experiment and simulation results show that Reveal can achieve better performance and robustness than other alternatives.
Keywords: Edge cloud; VNF placement; Request scheduling; Robustness

Fauzia Irram, Mudassar Ali, Muhammad Naeem, Shahid Mumtaz,
Physical layer security for beyond 5G/6G networks: Emerging technologies and future directions,
Journal of Network and Computer Applications,
Volume 206,
2022,
103431,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103431.
(https://www.sciencedirect.com/science/article/pii/S108480452200087X)
Abstract: Physical layer security (PLS) has proven to be a potential solution for enhancing the security performance of future 5G networks, which promises to fulfill the demands of increasing user traffic. Preventing eavesdroppers from overhearing and stealing useful information in such high traffic environments is as challenging as eliminating them from the network. The goal of this survey is to present a comprehensive study of the latest PLS works proposed to enhance the security performance in different 5G technologies. The survey starts by first giving a detailed introduction and overview of existing surveys that explicitly or partially discuss PLS in 5G and its emerging technologies. Many researchers have presented a number of PLS schemes, using either a separate technology such as Multiple-input–multiple-output (MIMO), Millimeter Wave (mmWave), Radio frequency (RF), Non-orthogonal multiple access (NOMA), Visible light communication (VLC), etc., or a combination of two or more technologies, for securing each field of future 5G networks such as Heterogeneous networks (HetNets), Device-to-Device (D2D), Internet-of-Things (IoT), Cognitive radio network (CRN), Unmanned Aerial Network (UAV), etc. After summarizing the existing surveys, we present a detailed overview on the PLS research works performed till now in HetNets, with respect to its different underlaying technologies, as well as in other emerging 5G technologies. Then, optimization ontology is presented that discusses different security metrics used for measuring PLS performance. Different from rest of the surveys, our survey includes a comprehensive discussion regarding the proposed PLS techniques based on artificial intelligence and machine learning techniques, especially highlighting the works performed using reinforcement learning and deep learning algorithms, allowing us to understand how artificial intelligence can help to achieve better PLS. Towards the end, we discuss numerous challenges being encountered in practical implementation of PLS techniques, and propose different interesting areas that can be opted as future research direction.
Keywords: 5G; Physical layer security; PLS; HetNets; OFDM/OFDMA; D2D; NOMA; SWIPT; Smart grid; CRN; Cooperative networks; Artificial intelligence; Machine learning

Jeffrey Regier, Keno Fischer, Kiran Pamnany, Andreas Noack, Jarrett Revels, Maximilian Lam, Steve Howard, Ryan Giordano, David Schlegel, Jon McAuliffe, Rollin Thomas,  Prabhat,
Cataloging the visible universe through Bayesian inference in Julia at petascale,
Journal of Parallel and Distributed Computing,
Volume 127,
2019,
Pages 89-104,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.12.008.
(https://www.sciencedirect.com/science/article/pii/S0743731518304672)
Abstract: A key task in astronomy is to locate astronomical objects in images and to characterize them according to physical parameters such as brightness, color, and morphology. This task, known as cataloging, is challenging for several reasons: many astronomical objects are much dimmer than the sky background, labeled data is generally unavailable, overlapping astronomical objects must be resolved collectively, and the datasets are enormous – terabytes now, petabytes soon. In this work, we infer an astronomical catalog from 55 TB of imaging data using Celeste, a Bayesian variational inference code written entirely in the high-productivity programming language Julia. Using over 1.3 million threads on 650,000 Intel Xeon Phi cores of the Cori Phase II supercomputer, Celeste achieves a peak rate of 1.54 DP PFLOP/s. Celeste is able to jointly optimize parameters for 188 M stars and galaxies, loading and processing 178 TB across 8192 nodes in 14.6 min. To achieve this, Celeste exploits parallelism at multiple levels (cluster, node, and thread) and accelerates I/O through Cori’s burst buffer. Julia’s native performance enables Celeste to employ high-level constructs without resorting to hand-written or generated low-level code (C/C++/Fortran) while still achieving petascale performance.
Keywords: Astronomy; Bayesian; Distributed optimization; Variational inference; Julia; High-performance computing

Yalan Wu, Jigang Wu, Long Chen, Jiaquan Yan, Yuchong Luo,
Efficient task scheduling for servers with dynamic states in vehicular edge computing,
Computer Communications,
Volume 150,
2020,
Pages 245-253,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.11.019.
(https://www.sciencedirect.com/science/article/pii/S0140366419307273)
Abstract: Vehicular edge computing has become an appealing paradigm to provide the delay-sensitive and multimedia-rich services by densely deploying the roadside units (RSUs) placed with edge servers. However, due to the geographical difference and energy efficiency, the computation loads of RSUs are serious unbalance, and the state of RSUs may switch to sleep in some cases for saving energy. This paper proposes a novel model of task scheduling where the state of the RSUs may dynamically switch between sleep and work in vehicular edge computing. The task requests of vehicles are modeled as an independent Poisson stream, and each edge server in RSU is modeled as a simple M/M/1 queuing system. The problem of minimizing the total delay of tasks on the proposed model is formulated and the NP-hardness of the problem is proved in this paper. A greedy algorithm is proposed for solving the problem by carefully selecting the RSUs. Meanwhile, a tabu search algorithm is customized to refine the solution generated by the proposed greedy algorithm. Moreover, a deep Q-network based algorithm is proposed utilizing the deep reinforcement learning approach, to learn the optimal scheduling policy without the prior knowledge of dynamic statistics. Simulation results show that, the deep Q-network based algorithm is the best among the proposed algorithms in terms of the total response time of tasks. All the proposed algorithms perform better than the random algorithm also presented in this paper. For example, the total response time of tasks for the deep Q-network based algorithm decreases by 24.13%, 28.73% and 35.95%, compared with the customized tabu search algorithm, the greedy algorithm and the random algorithm, respectively, for the case of that the maximum tolerant response time of each task is 14s.
Keywords: Vehicular edge computing; Task scheduling; Roadside units switching ON/OFF; Stochastic requests; M/M/1 queuing system

César Piñeiro, Juan C. Pichel,
A unified framework to improve the interoperability between HPC and Big Data languages and programming models,
Future Generation Computer Systems,
Volume 134,
2022,
Pages 123-139,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.04.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X2200125X)
Abstract: One of the most important issues in the path to the convergence of HPC and Big Data is caused by the differences in their software stacks. Despite some research efforts, the interoperability between their programming models and languages is still limited. To deal with this problem we introduce a new computing framework called IgnisHPC, whose main objective is to unify the execution of Big Data and HPC workloads in the same framework. IgnisHPC has native support for multi-language applications using JVM and non-JVM-based languages. Since MPI was used as its backbone technology, IgnisHPC takes advantage of many communication models and network architectures. Moreover, MPI applications can be directly executed in an efficient way in the framework. The main consequence is that users could combine in the same multi-language code HPC tasks (using MPI) with Big Data tasks (using MapReduce operations). The experimental evaluation demonstrates the benefits of our proposal in terms of performance and productivity with respect to other frameworks. IgnisHPC is publicly available for the Big Data and HPC research community.
Keywords: Big Data; HPC; MPI; Multi-language; Programming models

Jing Chen, Xiaoqiang Di, Rui Xu, Hui Qi, Ligang Cong, Kehan Zhang, Ziyang Xing, Xiongwen He, Wenping Lei, Shiwei Zhang,
A remote sensing data transmission strategy based on the combination of satellite-ground link and GEO relay under dynamic topology,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 337-353,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.02.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000572)
Abstract: The low earth orbit (LEO) remote sensing satellite has a short communication time with the earth station (ES), and a large amount of remote sensing data cannot be transmitted back to the ES in time using the LEO-ES link during the communication period. Using relay satellites can indirectly increase the amount of data transmitted back from LEO. In this paper, we combine LEO- ES link and relay satellite offloading to study the problem of maximizing the amount of data transmitted back from LEO remote sensing satellites. Most of the existing methods do not consider the effect of topology change on policy. In this paper, we consider a three-layer satellite network architecture of geostationary earth orbit (GEO), LEO remote sensing satellite, and ES. We studied the problem of maximizing the amount of LEO transmitted back data under dynamic topology between layers, and proposed a transmission strategy based on a combination of LEO-ES link and GEO offload under dynamic topology. First, in order to reduce the number of link interruptions in each time slot, a Non-Uniform Time Slot Division Method (NUTSDM) based on visible relationships between layers is proposed based on discrete-time points, which helps to accurately determine the number and identity of LEOs competing under each time slot. Second, the relationship among GEOs, LEOs, and network administrators is modeled as a Stackelberg game model, and a Two-way Bargaining Game Scheme under Dynamic Topology (TWBGS-DT) is proposed to maximize the amount of data transmitted back from space. Compared with the existing methods, the experimental results confirm the effectiveness of the proposed scheme in terms of algorithm convergence speed, terms of pricing, GEO cache space allocation, and increase the data volume of LEO transmissions back by 11.5% and 8.2 times relative to the ISL-Aided strategy and GAA-FARR strategy, respectively.
Keywords: Satellite networks; Dynamic topology; Data offloading; Stackelberg game; Resource allocation; Remote sensing data

Azad Jalalian, Saleh Yousefi, Thomas Kunz,
Network slicing in virtualized 5G Core with VNF sharing,
Journal of Network and Computer Applications,
Volume 215,
2023,
103631,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103631.
(https://www.sciencedirect.com/science/article/pii/S1084804523000504)
Abstract: Abstract
Through multiplexing separate virtual networks on the same network infrastructure, network slicing will lead to customization, scalability, flexibility, and isolation of services in different domains of the network. It also results in lower CAPEX/OPEX via increased utilization of network resources. In this paper, we have formulated an Integer Linear Programming (ILP) model to create network slices in the 5G virtualized core by considering the sharing of virtual network functions (VNFs) between different network slices to minimize cost. The ILP model aims to minimize the number of VNF instances and resource consumption while meeting the network slice requirements. The main motivation for sharing the 5G core VNFs among different network slices is to reduce the number of VNF instances, which results in a lower management overhead of VNFs. As solving the optimization problems is time-consuming and does not scale for larger deployments, we also developed a Heuristic Backtracking Algorithm for Network Slicing (HBA-NS). Furthermore, in order to account for the dynamic nature of traffic, our proposed network slicing framework also includes a CNN-LSTM deep learning model to predict UEs’ requests in each network slice. Based on the predicted requests, the HBA-NS algorithm scales allocated resources of different network slices to meet the dynamic SLA requirements of users. Our evaluation results show that the solution achieved by the HBA-NS algorithm is on average 6% worse than the optimal solution in the worst case but derived at a significantly lower run time. We also evaluated the proposed network slicing framework in terms of different criteria including its isolation property. Our simulation results show that network slicing can provide better performance compared to competitive approaches.
Keywords: 5G Core; Network slicing; Placement; CNN; LSTM; Machine learning; AMF; SMF; UPF

Binlei Cai, Bin Wang, Meihong Yang, Qin Guo,
AutoMan: Resource-efficient provisioning with tail latency guarantees for microservices,
Future Generation Computer Systems,
Volume 143,
2023,
Pages 61-75,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.01.014.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000213)
Abstract: Modern user-facing services are progressively evolving from large monolithic applications to complex graphs of loosely-coupled microservices. While microservice architecture greatly improves the efficiency of development and operation, it also complicates resource allocation and performance guarantee due to complex dependencies across different microservices. To prevent resource wastage and ensure user satisfaction, we present AutoMan, a learning-driven resource manager for microservices that enables much more efficient resource provisioning while guaranteeing the end-to-end tail latency Service Level Objective (SLO). AutoMan leverages a multi-agent deep deterministic policy gradient (MADDPG)-based method to capture the dependencies across different microservices and to allocate a proper amount of resources to each microservice subject to the target end-to-end tail latency SLO. During runtime, it further proactively identifies the critical microservices responsible for performance anomaly by deriving partial SLOs mathematically, and performs dynamic reprovisioning to mitigate the potential SLO violations. Testbed experiments show that AutoMan can save CPU and memory resources by up to 49.6% and 29.1% on average, while guaranteeing the same end-to-end tail latency objective.
Keywords: Cloud computing; Microservices; Resource management; Reinforcement learning; Quality of service; Tail latency

Inder Monga, Chin Guok, John MacAuley, Alex Sim, Harvey Newman, Justas Balcas, Phil DeMar, Linda Winkler, Tom Lehman, Xi Yang,
Software-Defined Network for End-to-end Networked Science at the Exascale,
Future Generation Computer Systems,
Volume 110,
2020,
Pages 181-201,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.04.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X19305618)
Abstract: Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.
Keywords: Intent based networking; End-to-end orchestration; Intelligent network services; Distributed infrastructure; Resource modeling; Software defined networking; Real-time; Interactive

Yongzhe Zhang, Ariful Azad, Aydın Buluç,
Parallel algorithms for finding connected components using linear algebra,
Journal of Parallel and Distributed Computing,
Volume 144,
2020,
Pages 14-27,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.04.009.
(https://www.sciencedirect.com/science/article/pii/S0743731520302689)
Abstract: Finding connected components is one of the most widely used operations on a graph. Optimal serial algorithms for the problem have been known for half a century, and many competing parallel algorithms have been proposed over the last several decades under various different models of parallel computation. This paper presents a class of parallel connected-component algorithms designed using linear-algebraic primitives. These algorithms are based on a PRAM algorithm by Shiloach and Vishkin and can be designed using standard GraphBLAS operations. We demonstrate two algorithms of this class, one named LACC for Linear Algebraic Connected Components, and the other named FastSV which can be regarded as LACC’s simplification. With the support of the highly-scalable Combinatorial BLAS library, LACC and FastSV outperform the previous state-of-the-art algorithm by a factor of up to 12x for small to medium scale graphs. For large graphs with more than 50B edges, LACC and FastSV scale to 4K nodes (262K cores) of a Cray XC40 supercomputer and outperform previous algorithms by a significant margin. This remarkable performance is accomplished by (1) exploiting sparsity that was not present in the original PRAM algorithm formulation, (2) using high-performance primitives of Combinatorial BLAS, and (3) identifying hot spots and optimizing them away by exploiting algorithmic insights.
Keywords: Connected Component; Distributed memory; GraphBLAS

Wei-Che Chien, Hung-Yen Weng, Chin-Feng Lai, Zhang Fan, Han-Chieh Chao, Ying Hu,
A SFC-based access point switching mechanism for Software-Defined Wireless Network in IoV,
Future Generation Computer Systems,
Volume 98,
2019,
Pages 577-585,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.01.030.
(https://www.sciencedirect.com/science/article/pii/S0167739X1832082X)
Abstract: The development of Internet of Vehicles (IoVs) need to be supported by high-speed network and a large number of Internet of Things (IoT) technologies. In order to meet the requirements of different IoV applications, effective transmission quality is one of the necessary conditions for the development of IoV. Therefore, this study applies the concept of Fog computing and cloud computing to IoV to effectively transmit a large number of multimedia data and sensing data. Based on the architecture of Software-Defined Wireless Network (SDWN), the optimization algorithm is proposed to reduce the transmission latency through optimizing access point selection and the load balancing of the Service Function (SF). The simulation results shows that the proposed mechanisms can distribute the traffic evenly and find the optimal access point.
Keywords: Software Defined Wireless Networks (SDWN); Load balancing; Network planning; Internet of Vehicles

Partha Pratim Ray, Neeraj Kumar,
SDN/NFV architectures for edge-cloud oriented IoT: A systematic review,
Computer Communications,
Volume 169,
2021,
Pages 129-153,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.01.018.
(https://www.sciencedirect.com/science/article/pii/S0140366421000396)
Abstract: Software-defined network (SDN) and network function virtualization (NFV) have entirely changed the way internetwork backhaul should be utilized and behaved for virtualized service provisioning. Several benefits have been observed in multiple domains of applications that has used SDN and NFV in integrated way. Thus, SDN/NFV paradigm has been investigated to seek whether network services could be efficiently delivered, managed, and disseminated to the end users. Internet of Things (IoT) is justifiably associated with the SDN/NFV augmentation to make this task enriched. However, factors related to edge-cloud communication and network services have not been effectively mitigated until now. In this paper, we present an in-depth, qualitative, and comprehensive systematic review to find the answers of following research questions, such as, (i) how does state-of-the-art SDN/NFV architecture look like, (ii) how to solve next generation cellular services via architecture involvement, (iii) what type of application/test-bed need to be studied, and (iv) security framework should be catered. We further, elaborate various key issues and challenges in the existing architecture mitigation for SDN/NFV integration to the IoT-based edge-cloud oriented network service provisioning. Future directions are also prescribed to support fellow researchers to improve existing virtualized service scenario. Lessons learned after performing comparative study with other survey articles dictates that our work presents timely contribution in terms of novel knowledge toward understanding of formulating SDN/NFV virtualization services under the aegis of IoT-centric edge-cloud scenario.
Keywords: SDN; NFV; IoT; Edge; Cloud; Architecture

Haizhou Du, Yijian Chen, Xiaojie Feng, Qiao Xiang, Haoyu Liu,
An efficient federated learning framework for multi-channeled mobile edge network with layered gradient compression,
Computer Networks,
Volume 221,
2023,
109517,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109517.
(https://www.sciencedirect.com/science/article/pii/S1389128622005515)
Abstract: A fundamental issue for federated learning (FL) is how to achieve efficient training performance under complex dynamic communication environments. This issue can be alleviated by the fact that modern edge devices usually can connect to the edge server via multiple communication channels (e.g., 4G, LTE, and 5G) because multi-channel communication can increase the communication bandwidth and has lower communication costs and energy consumption than a single high-speed communication channel. However, if the communication data cannot be properly allocated to multiple channels in a complex dynamic communication network, multi-channel communication will still waste resources (e.g., bandwidth, battery life, and monetary cost). In this paper, we propose an efficient FL framework called, which consists of two parts, the layered gradient compression (LGC), and a learning-driven control algorithm. Specifically, with LGC, local gradients from a device are coded into several layers, and each layer is sent to the server along a different channel. The FL server aggregates the received layers of local gradients from devices to update the global model and sends the result back to the devices. Furthermore, we prove the convergence of LGC and formally define the problem of resource-efficient with LGC. We then propose a learning-driven algorithm for each device to dynamically adjust its local computation (i.e., the number of local stochastic descent) and communication decisions (i.e., the compression level of different layers and the layer-to-channel mapping) in each iteration. Results from extensive experiments show that significantly reduces the training time and improves the resource utilization (energy consumption and money cost) while achieving a similar test accuracy compared with well-known FL baselines.
Keywords: Federated learning; Mobile edge network; Layered gradient compression; Multi-channel communication; Deep reinforcement learning

Mohammed Redha Bouzidi, Mourad Daoudi, Benameur Ziani, Kamel Boukhalfa, Chaker Abdelaziz Kerrache, Nasreddine Lagraa,
FAMOBACH: A fast and survivable workflow scheduling approach based MOHEFT using backtacking and checkpointing,
Computer Communications,
Volume 171,
2021,
Pages 16-27,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.02.005.
(https://www.sciencedirect.com/science/article/pii/S0140366421000670)
Abstract: Workflows and workflow scheduling are increasingly used in many applications with many tasks and dependency constraints between these tasks. On the other hand, cloud computing offers huge opportunities to process large-scale workflows and data-intensive applications. The processing performance of a workflow is highly depending on how the different tasks are scheduled on the cloud’s resources. Workflow scheduling is still a challenging issue that falls into NP-hard problems category, for which finding an exact solution is intractable. Much research effort has gone into solving this problem, including multi-objective heterogeneous earliest-finish-time (MOHEFT) method, which turns to be a reference for the workflow scheduling problem. While demonstrating very high efficiency at the provided results quality, MOHEFT suffers from its high time complexity when it comes to large-scale workflows. To address this shortcoming, we first investigate MOHEFT complexity. Then, we propose FAst workflow scheduling approach based MOHEFT using BAcktraking and CHeckpointing (FAMOBACH). The latter is an improved version that eliminates redundant calculations in MOHEFT using checkpointing and backtacking. FAMOBACH performance evaluation depicts it runs up to 9 times faster than the MOHEFT.
Keywords: MOHEFT; FAMOBACH; Workflow scheduling; Cloud Computing

Tiago C.S. Xavier, Igor L. Santos, Flavia C. Delicato, Paulo F. Pires, Marcelo P. Alves, Tiago S. Calmon, Ana C. Oliveira, Claudio L. Amorim,
Collaborative resource allocation for Cloud of Things systems,
Journal of Network and Computer Applications,
Volume 159,
2020,
102592,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102592.
(https://www.sciencedirect.com/science/article/pii/S1084804520300667)
Abstract: The conceptual approach known as Fog/Edge Computing has recently emerged, aiming to move part of the computing and storage resources from the cloud to the edge of the network. The combination of IoT devices, edge nodes, and the Cloud gives rise to a three-tier Cloud of Things (CoT) architecture. In the complex and dynamic CoT ecosystems, a key issue is how to efficiently and effectively allocate resources to meet the demands of applications. Similar to traditional clouds, the goal of resource allocation in the CoT is to maximize the number of applications served by the infrastructure while ensuring a target operational cost. We propose a resource allocation algorithm for CoT systems that (i) supports heterogeneity of devices and applications, (ii) leverages the distributed nature of edge nodes to promote collaboration during the allocation process and (iii) provides an efficient usage of the system resources while meeting latency requirements and considering different priorities of IoT applications. Our algorithm follows a heuristic-based approach inspired on an economic model for solving the resource allocation problem in CoT. A set of simulations were performed, with promising results, showing that our collaborative resource allocation algorithm is more scalable, reduces the response time for applications and the energy consumption of end devices, in comparison to a two-tier, Cloud-based approach. Moreover, the network traffic between edge nodes, and between the Edge and Cloud tiers, is considerably smaller when using our collaborative solution, in comparison to other evaluated approaches.
Keywords: Collaborative work; Edge computing; Internet of things; Resource allocation; Data freshness; Hierarchical

Amin Shahraki, Amir Taherkordi, Øystein Haugen, Frank Eliassen,
Clustering objectives in wireless sensor networks: A survey and research direction analysis,
Computer Networks,
Volume 180,
2020,
107376,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107376.
(https://www.sciencedirect.com/science/article/pii/S1389128620303121)
Abstract: Wireless Sensor Networks (WSNs) typically include thousands of resource-constrained sensors to monitor their surroundings, collect data, and transfer it to remote servers for further processing. Although WSNs are considered highly flexible ad-hoc networks, network management has been a fundamental challenge in these types of networks given the deployment size and the associated quality concerns such as resource management, scalability, and reliability. Topology management is considered a viable technique to address these concerns. Clustering is the most well-known topology management method in WSNs, grouping nodes to manage them and/or executing various tasks in a distributed manner, such as resource management. Although clustering techniques are mainly known to improve energy consumption, there are various quality-driven objectives that can be realized through clustering. In this paper, we review comprehensively existing WSN clustering techniques, their objectives and the network properties supported by those techniques. After refining more than 500 clustering techniques, we extract about 215 of them as the most important ones, which we further review, catergorize and classify based on clustering objectives and also the network properties such as mobility and heterogeneity. In addition, statistics are provided based on the chosen metrics, providing highly useful insights into the design of clustering techniques in WSNs.
Keywords: Wireless sensor networks; Clustering; Survey; Routing; Cluster head selection; Topology management

Angel Cañete, Mercedes Amor, Lidia Fuentes,
HADES: An NFV solution for energy-efficient placement and resource allocation in heterogeneous infrastructures,
Journal of Network and Computer Applications,
Volume 221,
2024,
103764,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103764.
(https://www.sciencedirect.com/science/article/pii/S1084804523001832)
Abstract: Network Function Virtualization (NFV) aims to replace traditional network functions running in proprietary hardware with software instances (i.e., Virtual Network Functions, VNFs) embedded in general-purpose virtualization solutions. Aware that the transition to a fully virtualized network infrastructure will pay a high energy cost, especially in IoT systems composed of a myriad of devices, energy efficiency is one of the key innovative targets of future networks. Edge computing should be considered in IoT environments to save time and energy by processing data near the producer devices. However, applying NFV in the context of IoT/Edge/Cloud environments complicates the placement of VNFs, due to the inherent heterogeneous nature of such environments and the variety of resource demands. This paper proposes an energy-aware placement of service function chains of VNFs and a resource-allocation solution for heterogeneous edge infrastructures that considers the computation and communication delays according to the VNFs’ location in the infrastructure. The solution has been integrated with the ETSI-sponsored project Open Source Management and Orchestration (OSM) as an extension called HADES, which allows the configuration of VNFs and their subsequent resource allocation and deployment at the edge, minimizing energy consumption and ensuring a quality of service. We have applied the deployment of augmented reality services in real and simulated scenarios. The results show up to a 59% reduction in power consumption and QoS compliance in all scenarios considered compared to default OSM placement and four other allocation policies. We prove that our solution has negligible power overhead, and validate the scalability and applicability of HADES.
Keywords: Energy efficiency; B5G; VNF placement; Edge computing; Open source MANO; IoT; Feature Models

Parimala Boobalan, Swarna Priya Ramu, Quoc-Viet Pham, Kapal Dev, Sharnil Pandya, Praveen Kumar Reddy Maddikunta, Thippa Reddy Gadekallu, Thien Huynh-The,
Fusion of Federated Learning and Industrial Internet of Things: A survey,
Computer Networks,
Volume 212,
2022,
109048,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109048.
(https://www.sciencedirect.com/science/article/pii/S1389128622001955)
Abstract: Industrial Internet of Things (IIoT) lays a new paradigm for the concept of Industry 4.0 and paves an insight for new industrial era. Nowadays smart machines and smart factories use machine learning/deep learning based models for incurring intelligence. However, storing and communicating the data to the cloud and end device leads to issues in preserving privacy. In order to address this issue, Federated Learning (FL) technology is implemented in IIoT by the researchers nowadays to provide safe, accurate, robust and unbiased models. Integrating FL in IIoT ensures that no local sensitive data is exchanged, as the distribution of learning models over the edge devices has become more common with FL. Therefore, only the encrypted notifications and parameters are communicated to the central server. In this paper, we provide a thorough overview on integrating FL with IIoT in terms of privacy, resource and data management. The survey starts by articulating IIoT characteristics and fundamentals of distributed machine learning and FL. The motivation behind integrating IIoT and FL for achieving data privacy preservation and on-device learning are summarized. Then we discuss the potential of using machine learning (ML), deep learning (DL) and blockchain techniques for FL in secure IIoT. Further we analyze and summarize several ways to handle the heterogeneous and huge data. Comprehensive background on data and resource management are then presented, followed by applications of IIoT with FL in automotive, robotics, agriculture, energy, and healthcare industries. Finally, we shed light on challenges, some possible solutions and potential directions for future research.
Keywords: Data storage; IIoT; Federated Learning; Data privacy; Data sharing; Resource management

Chunlin Li, YaPing Wang, Yi Chen, Youlong Luo,
Energy-efficient fault-tolerant replica management policy with deadline and budget constraints in edge-cloud environment,
Journal of Network and Computer Applications,
Volume 143,
2019,
Pages 152-166,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.04.018.
(https://www.sciencedirect.com/science/article/pii/S1084804519301420)
Abstract: With the development of large-scale distributed systems such as grids and clouds, data replication management has become a hot research topic. Although replica management can improve the cluster system performance, it also brings a series of management and overhead issues. Therefore, the energy-efficient fault-tolerant replica management policy with the deadline and budget constraints in the edge-cloud environment is proposed. The experiments show that the proposed dynamic replica placement algorithm can effectively reduce the mean job time, reduce the use of network bandwidth and improve the utilization of storage space. Considering the issue of energy efficiency, the energy-aware cluster scaling strategy is proposed to reduce system energy consumption and achieve energy efficiency by sleeping and waking up the data nodes according to the load state of the system. Besides, in order to avoid access failures and data loss caused by node failures, the node failure recovery method based on availability metrics is used to deal with node failures. The experiments show that the performance of the proposed algorithm is better than the other algorithms in terms of energy efficiency and fault tolerance.
Keywords: Replica management; Energy-efficient; Fault-tolerant; Edge-cloud environment

Bo Peng, Jianguo Yao, Xin Xu, Haibing Guan,
Proactive coordination for low-congestion multi-path datacenter networks,
Journal of Systems Architecture,
Volume 101,
2019,
101656,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.101656.
(https://www.sciencedirect.com/science/article/pii/S1383762119304631)
Abstract: Modern datacenters involve a rich mix of workloads, each of which puts forward different service-level objective, including high throughput, low latency, etc. Currently, most datacenters introduce statistical multiplexing technology and oversubscription to the network design to lower the total cost, which can easily lead to the occurrence of network congestion, especially when the network is highly occupied by throughput-intensive workloads. This paper describes ProCAM, a proactive congestion avoidance mechanism for datacenter networks. As throughput-intensive flows are the chief culprit of network congestion, ProCAM adapts the multi-path routing to control transmission bandwidth and utilizes the predictability of throughput-intensive flows to prearrange optimal coordinate scheme (desynchronize the sending time of concurrent long-flows) beforehand as a proactive manner, by solving the low-congestion transmission model which minimizes network-wide host-to-host transmission latency from a global perspective. In this way, queue length in buffers can be kept at a low level and the performance of latency-sensitive flows can be guaranteed. In the evaluation experiments based on simulation with Mininet and SDN controller Ryu, the extensive simulations show that the proposed ProCAM can achieve high throughput with nearly zero packet loss and low latency when the network is highly occupied.
Keywords: Datacenter networks; Low congestion; High throughput; Proactive

Georgios Amponis, Thomas Lagkas, Panagiotis Sarigiannidis, Vasileios Vitsas, Panagiotis Fouliras, Shaohua Wan,
A survey on FANET routing from a cross-layer design perspective,
Journal of Systems Architecture,
Volume 120,
2021,
102281,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102281.
(https://www.sciencedirect.com/science/article/pii/S1383762121001934)
Abstract: With the introduction of UAVs to networking, ad hoc communications have evolved past confinement to the terrestrial grid and have moved towards aerial meshes. Until now, Flying Ad-hoc Networks (FANETs) have been relying on strictly layered communication protocols for their function and routing, a tradition set by conventional networks. With layers of said protocols functioning as ”black boxes”, any form of interaction between non-adjacent layers constitutes a direct violation of the protocols’ architecture. The work presented in this survey intends to examine existing protocols of both legacy and cross-layer architectures in terms of their potential in accommodating routing in FANET deployments. Special attention is given to multi-altitude (3D) deployments, where a substantially greater amount of processing and packet route complexity is observed, and a greater amount of node location precision is required. The potential of cross-layer designs is expressed as a function of power budgeting, mobility (and awareness thereof), security, and resource allocation, given their importance for efficient control of flying ad hoc networks.
Keywords: 3D FANETs; Cross layer designs; Drone swarms; Multi-altitude UAV deployments

Jannatun Noor, Saiful Islam Salim, A.B.M. Alim Al Islam,
Strategizing secured image storing and efficient image retrieval through a new cloud framework,
Journal of Network and Computer Applications,
Volume 192,
2021,
103167,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103167.
(https://www.sciencedirect.com/science/article/pii/S1084804521001788)
Abstract: Nowadays, a major problem in faster page loading through optimizing websites is the absence of having images in their intended sizes. Accordingly, while loading a webpage, most of the time overhead pertains to image-related tasks such as image loading and resizing, which can be optimized substantially through the pre-availability of smaller-size images. Therefore, in this study, we propose a strategy to enable faster and efficient image retrieval from a cloud via the necessary pre-processing of images beyond conventional online processing. We also extend the abilities of cloud file sharing from the conventional “only storing images” to pre-processing along with security reinforcing. Here, we perform the first task of resizing images to several dimensions for covering diversified remote user devices available nowadays. Then, we perform encoding and encryption of images using the P-Fibonacci transform of Discrete Cosine Coefficients (PFCC) algorithm. Afterward, we resize images using the Bicubic interpolation method to both JPEG and Progressive-JPEG (PJPEG) formats by adding a new middleware named iBuck for faster and smooth retrieval. We leverage necessary algorithms and image types after careful evaluation of their performances in our intended framework. Our evaluation covers both objective and subjective evaluations including QoE metrics such as MSE, SSIM, etc. Furthermore, we conduct detailed experimentation over real setups to evaluate the performance of our implemented middleware in a cloud file sharing environment with both custom and standard data sets. Our evaluation reveals substantial performance improvement using our proposed framework compared to that using conventional alternatives.
Keywords: Image cloud; OpenStack Swift; PJPEG; Middleware; Image resizing; Encryption

Amr Tolba, Zafer Al-Makhadmeh,
A recursive learning technique for improving information processing through message classification in IoT–cloud storage,
Computer Communications,
Volume 150,
2020,
Pages 719-728,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.12.001.
(https://www.sciencedirect.com/science/article/pii/S0140366419313052)
Abstract: Processing and accessing distributed information is a prominent requirement for Internet of Things (IoT) in supporting business and consumer applications to improve accessibility. As the volume of information is being stored and processed is hefty, message classification is challenging in a mobile environment. This also results in prolonged processing delays and backlogs. In order to bridge the gap between message classification and request processing, this paper proposes a classification technique that operates on the basis of request-prioritized recursive learning for ease of message identification and service mapping. This learning technique predicts the type of information and its attributes through intensive learning and services them based on priority to minimize retrieval time. The priority of message servicing relies on the error obtained during the learning process. Despite an increasing number of user requests, attributes associated with each are bundled independently to provide an instant response. Prioritization accounts for the minimum number of error states while learning a message with different attributes to curtail prolonged response time. Error in the learning process is evaluated through a numerical analysis for different learning scenarios of the classified messages. The proposed learning-based access and retrieval technique were analyzed using the metrics of request backlogs, response time, caching delay, and the rate of utilization. The results of experiments verified the effectiveness of the proposed technique in terms of minimizing response time, backlogs, and caching delays, and improving utilization.
Keywords: Attribute classification; Cloud storage access; Information processing; Recurrent learning; Priority-based transmission

Israat Haque, Dipon Saha,
SoftIoT: A resource-aware SDN/NFV-based IoT network,
Journal of Network and Computer Applications,
Volume 193,
2021,
103208,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103208.
(https://www.sciencedirect.com/science/article/pii/S1084804521002113)
Abstract: The proliferation of the Internet of things (IoT) devices and the corresponding high-volume sensing data pave the path for data-driven IoT services such as smart cities and smart farming. The first step towards achieving such services is to efficiently collect the data at collection centers, which requires an agile data collection strategy coping with dynamic environment changes. Network programmability (software-defined networking (SDN) or network function virtualization (NFV)) fits this need by enabling re-configurable IoT networks. However, existing SDN/NFV-based solutions in the IoT context fall back due to the lack of resource and overhead awareness and the compatibility with standard protocols. SoftIoT fills that gap by designing SDN/NFV-enabled IoT nodes and network architecture that consider resource and energy constraints a top priority. It optimizes the number of activated NFV nodes and assigns traffic sources to that VNFs over optimal routes of enough energy and link quality. We extensively evaluate the performance of SoftIoT in the Cooja simulator over the Contiki OS using real sensor traffic, which shows that the proposed solution consumes 40% and 60% less energy than existing works. SoftIoT also outperforms its counterparts over the packet delivery ratio, network lifetime, and overhead reduction while being compatible with standard protocols.
Keywords: SDN; NFV; IoT; ILP; Energy-aware routing

Zorka Jovanovic, Zhe Hou, Kamanashis Biswas, Vallipuram Muthukkumarasamy,
Robust integration of blockchain and explainable federated learning for automated credit scoring,
Computer Networks,
Volume 243,
2024,
110303,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110303.
(https://www.sciencedirect.com/science/article/pii/S138912862400135X)
Abstract: This article examines the integration of blockchain, eXplainable Artificial Intelligence (XAI), especially in the context of federated learning, for credit scoring in financial sectors to improve the credit assessment process. Research shows that integration of these cutting-edge technologies is in its infancy, specifically in the areas of embracing broader data, model verification, behavioural reliability and model explainability for intelligent credit assessment. The conventional credit risk assessment process utilises historical application data. However, reliable and dynamic transactional customer data are necessary for robust credit risk evaluation in practice. Therefore, this research proposes a framework for integrating blockchain and XAI to enable automated credit decisions. The main focus is on effectively integrating multi-party, privacy-preserving decentralised learning models with blockchain technology to provide reliability, transparency, and explainability. The proposed framework can be a foundation for integrating technological solutions while ensuring model verification, behavioural reliability, and model explainability for intelligent credit assessment.
Keywords: Automated credit scoring; Blockchain; Explainable artificial intelligence; Decentralised federated learning

Asad Ali, Inaam Ilahi, Adnan Qayyum, Ihab Mohammed, Ala Al-Fuqaha, Junaid Qadir,
A systematic review of federated learning incentive mechanisms and associated security challenges,
Computer Science Review,
Volume 50,
2023,
100593,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100593.
(https://www.sciencedirect.com/science/article/pii/S1574013723000606)
Abstract: In response to various privacy risks, researchers and practitioners have been exploring different paradigms that can leverage the increased computational capabilities of consumer devices to train machine learning (ML) models in a distributed fashion without requiring the uploading of the training data from individual devices to central facilities. For this purpose, federated learning (FL) was proposed as a technique that can learn a global machine model at a central master node by the aggregation of models trained locally using private data. However, organizations may be reluctant to train models locally and to share these local ML models due to the required computational resources for model training at their end and due to privacy risks that may result from adversaries inverting these models to infer information about the private training data. Incentive mechanisms have been proposed to motivate end users to participate in collaborative training of ML models (using their local data) in return for certain rewards. However, the design of an optimal incentive mechanism for FL is challenging due to its distributed nature and the fact that the central server has no access to clients’ hyperparameters information and the amount/quality data used for training, which makes the task of determining the reward based on the contribution of individual clients in FL environment difficult. Even though several incentive mechanisms have been proposed for FL, a thorough up-to-date systematic review is missing and this paper fills this gap. To the best of our knowledge, this paper is the first systematic review that comprehensively enlists the design principles required for implementing these incentive mechanisms and then categorizes various incentive mechanisms according to their design principles. In addition, we also provide a comprehensive overview of security challenges associated with incentive-driven FL. Finally, we highlight the limitations and pitfalls of these incentive schemes and elaborate upon open-research issues that require further research attention.
Keywords: Federated learning; Incentive schemes; Game theory; Mechanism design; Blockchain

Mohammad Mainul Islam, Fahimeh Ramezani, Hai Yan Lu, Mohsen Naderpour,
Optimal placement of applications in the fog environment: A systematic literature review,
Journal of Parallel and Distributed Computing,
Volume 174,
2023,
Pages 46-69,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.12.001.
(https://www.sciencedirect.com/science/article/pii/S0743731522002465)
Abstract: The fog-computing paradigm complements cloud computing to support the deployment and execution of latency-sensitive applications at the network edge by offering enhanced computational power. Optimal placement of such applications over a fog network comprising geographically distributed, heterogeneous, and resource-constrained fog nodes is a core challenge in fog-computing paradigm research. This study systematically reviews existing research on optimal fog application placement over the cloud-to-thing continuum. Surveyed articles are analyzed in four aspects: i) layers of the cloud-to-thing continuum considered for placing an application; ii) application characteristics that are considered in making placement decisions; iii) application placement mechanism; iv) tools and technology for placing an application. This review also categorizes the research problems associated with fog application placement. Finally, based on this review, we suggest directions for future adaptive fog-application placement research.
Keywords: Fog computing; Application placement; Service placement; Resource management

Xing Gao, An He, Guangwei Wu, Jinhuan Zhang,
APAP: An adaptive packet-reproduction and active packet-loss data collection protocol for WSNs,
Computer Communications,
Volume 210,
2023,
Pages 294-311,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.08.015.
(https://www.sciencedirect.com/science/article/pii/S0140366423003006)
Abstract: In lossy, large-scale wireless sensor networks (WSNs), data collection protocols face the challenge of balancing expected data transmission reliability and limited resources of sensors. Packet redundancy strategies have been widely employed to guarantee transmission reliability. However, these schemes often result in energy waste, especially in sink-neared hotspot districts. We propose the APAP to balance the two competing interests in energy-constrained WSNs. APAP mainly consists of two parts: adaptive packet-reproduction routing and active packet-loss mechanism. In the first part, we design a compensation function to compensate for packet losses and employ it in the packet-reproduction routing. Through available network parameters, such a function offers an optimized number of redundant packets. In the second part, we devise a novel distributed packet-loss mechanism to detect and intercept redundant packets in the hotspot area. Hotspot nodes record the simple traffic information in their vicinity on a local micro-table, these nodes transform their mode based on the records to reach a basic consensus with one-hop neighbors, they then collaboratively intercept excess packets from outside, thus the injection traffic can be mitigated. Such an interception area has the potential to be expanded and prolonged as the redundancy becomes more severe. Our mechanism imposes low-complexity requirements on general devices. Moreover, thorough mathematical analyses are offered for the performance of APAP. Simulation results indicate that with high reliability, the maximum node energy consumption is reduced by 44.7% to 66.3% compared to conventional protocols in one round. Besides, the network lifetime is prolonged by 91.1% to 200%.
Keywords: Large-scale WSNs; Data collection protocol; Hotspot issue; Multi-path routing; Active packet-loss mechanism

Peizhuang Cong, Yuchao Zhang, Zheli Liu, Thar Baker, Hissam Tawfik, Wendong Wang, Ke Xu, Ruidong Li, Fuliang Li,
A deep reinforcement learning-based multi-optimality routing scheme for dynamic IoT networks,
Computer Networks,
Volume 192,
2021,
108057,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108057.
(https://www.sciencedirect.com/science/article/pii/S1389128621001535)
Abstract: With the development of Internet of Things (IoT) and 5G technologies, more and more applications, such as autonomous vehicles and tele-medicine, become more sensitive to network latency and accuracy, which require routing schemes to be more flexible and efficient. In order to meet such urgent need, learning-based routing strategies are emerging as strong candidate solutions, with the advantages of high flexibility and accuracy. These strategies can be divided into two categories, centralized and distributed, enjoying the advantages of high precision and high efficiency, respectively. However, routing becomes more complex in dynamic IoT network, where the link connections and access states are time-varying, hence these learning-based routing mechanisms are required to have the capability to adapt to network changes in real time. In this paper, we designed and implemented both centralized and distributed Reinforcement Learning-based Routing schemes combined with Multi-optimality routing criteria (RLR-M). By conducting a series of experiments, we performed a comprehensive analysis of the results and arrived at the conclusion that the centralized is better suited to cope with dynamic networks due to its faster reconvergence (2.2 × over distributed), while the distributed is better positioned to handle with large-scale networks through its high scalability (1.6 × over centralized). Moreover, the multi-optimality routing scheme is implemented through model fusion, which is more flexible than traditional strategies and as such is better placed to meet the needs of IoT.
Keywords: Routing; Multi-optimality criteria; Deep reinforcement learning; Deep Q network

Chung-Ming Huang, Han-I Wang,
Fuzzy-rule-decided small cell offloading for rate-adaptive SVC-DASH video streaming over the vehicle environment,
Computer Communications,
Volume 212,
2023,
Pages 1-20,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.09.014.
(https://www.sciencedirect.com/science/article/pii/S0140366423003249)
Abstract: This paper proposed an SVC-DASH video streaming method over the vehicular environment based on the Multi-access Edge Computing (MEC) architecture. For the streaming concern, a control scheme using the Segment-Set-based buffer-aware bitrate adaptation with the backward quality's increment control was proposed; for the mobility concern, a Small Cell Video Streaming Offloading (SC–VSO) control scheme adopting the multiple attribute fuzzy-based decision for deciding whether it can offload the video streaming traffic from a macro cell to a small cell or not was proposed. To have the suitable bitrate adaptation and stable quality variation, three factors that the proposed streaming control scheme considers are (i) estimated bandwidth, (ii) buffer occupancy and (iii) quality variation. Since the vehicle is moving across many types of Base Stations (BSs), the proposed mobility control scheme can decide whether to have SC-VSO or not before vehicle X entering into the ahead small cell's signal coverage based on (i) the network situations of the corresponding macro and small cell and (ii) the reported context from vehicle X using the proposed fuzzy logic mechanism. The performance evaluation results shown that the proposed method has the higher video quality and better quality stability for video streaming over the vehicle environment.
Keywords: Scalable video coding (SVC); SVC streaming; SVC-DASH; Mobile edge computing (MEC); Small cell offloading; Traffic offloading; Fuzzy inference system

 Sakshi Popli, Rakesh Kumar Jha, Sanjeev Jain,
A comprehensive survey on Green ICT with 5G-NB-IoT: Towards sustainable planet,
Computer Networks,
Volume 199,
2021,
108433,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108433.
(https://www.sciencedirect.com/science/article/pii/S1389128621003959)
Abstract: Rapid advancement in ICT is promoting us into an era of unprecedented prosperity & countless possibilities. However, there is one gloomy side of the ICT technology that contributes toward the inflation of carbon footprint. Research from 2020, estimates the ICT sector, carbon emission, to be 1,100 million tons. The future generation networks and IoT will further escalate this figure, as these would overburden the core ICT pillars i.e. Data Centers (DC's), and Mobile networks (NT). This in turn will inflate the ICT power consumption and leads to more carbon emission. Thus researchers and industries are continuously putting efforts to transform ICT into Green ICT. Apart from this, there is one bright side of ICT i.e. “Green BY ICT” that helps other industries to abate their carbon emission using smart IoT applications. However, the smart IoT devices/sensors/actuators used for this are mostly battery-operated. To reduce the battery waste, efforts are also being made to either prolong their battery life or to make them self-powered or battery-free. This survey discusses both aspects of ICT i.e. Green of ICT and Green by ICT. Firstly, the recent approaches for the Greening of ICT include techniques for Green-DC, Green-NT are discussed. Post discussing this, the paper also confers the energy harvesting solutions & energy-efficient techniques for the greening of user device/senor. In continuation of this, 5G green physical layer solution, Narrowband Internet of Things (NB-IoT) that prolongs battery life is also discussed, including its enhancement from release 13 to release 16, recent techniques to further optimize the NB-IoT performance, and future research challenges. Apart from this the recent advancement related to renewable energy solutions for Green ICT is also discussed. Overall this survey concludes that ICT's own environmental impact must be evaded, to utilize the ICT's tremendous potential.
Keywords: Green Information and communication technology (Green ICT); Data center; Access network; Narrow band internet of things (NB-IoT); Green communication; Smart health

R. Borrell, D. Dosimont, M. Garcia-Gasulla, G. Houzeaux, O. Lehmkuhl, V. Mehta, H. Owen, M. Vázquez, G. Oyarzun,
Heterogeneous CPU/GPU co-execution of CFD simulations on the POWER9 architecture: Application to airplane aerodynamics,
Future Generation Computer Systems,
Volume 107,
2020,
Pages 31-48,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.01.045.
(https://www.sciencedirect.com/science/article/pii/S0167739X1930994X)
Abstract: High fidelity Computational Fluid Dynamics simulations are generally associated with large computing requirements, which are progressively acute with each new generation of supercomputers. However, significant research efforts are required to unlock the computing power of leading-edge systems, currently referred to as pre-Exascale systems, based on increasingly complex architectures. In this paper, we present the approach implemented in the computational mechanics code Alya. We describe in detail the parallelization strategy implemented to fully exploit the different levels of parallelism, together with a novel co-execution method for the efficient utilization of heterogeneous CPU/GPU architectures. The latter is based on a multi-code co-execution approach with a dynamic load balancing mechanism. The assessment of the performance of all the proposed strategies has been carried out for airplane simulations on the POWER9 architecture accelerated with NVIDIA Volta V100 GPUs.
Keywords: Heterogeneous computing; POWER9; NVIDIA volta V100; GPU computing; CFD; Load balancing

Monika Gill, Dinesh Singh,
A comprehensive study of simulation frameworks and research directions in fog computing,
Computer Science Review,
Volume 40,
2021,
100391,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100391.
(https://www.sciencedirect.com/science/article/pii/S1574013721000319)
Abstract: Context:
Fog computing paradigm consists of resource constrained devices that support data processing and service provisioning at the edge of the network. Simulation frameworks play a key role in the design, development and validation of novel approaches for fog environment. The existing fog simulators model one or more aspects of fog environment and hence it becomes a tedious task to analyse and choose them as per the research requirements.
Objective:
This paper reviews the literature of simulation tools for fog computing and aims to help the novice researchers to explore and assess fog related proposals.
Method:
The study has employed a systematic search procedure to identify relevant articles published in the duration of 2015-2020.
Results:
The relevant publications are evaluated to highlight their strengths and underline the limitations. A comparative analysis of studies based on eight characteristic and few non technical features is presented. The scope for improvement in fog simulators is reported. Lastly, the prevailing research challenges in fog that can be addressed with reviewed simulation frameworks are detailed out.
Conclusion:
The paper has identified an increased interest in the development of novel and extended fog simulators thus emphasizing their importance. Also, the need to develop more advanced fog simulators that can model a wider range of fog environments is recognized. Directions are given for future work.
Keywords: Fog computing; Edge; Simulation

Kalupahana Liyanage Kushan Sudheera, Maode Ma, Peter Han Joo Chong,
Real-time cooperative data routing and scheduling in software defined vehicular networks,
Computer Communications,
Volume 181,
2022,
Pages 203-214,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.10.003.
(https://www.sciencedirect.com/science/article/pii/S0140366421003753)
Abstract: Links in vehicular networks are highly dynamic and generally exist only for a limited amount of time. Moreover, data packets are generally transmitted over multiple hops and have diverse latency requirements. The above situations together create a dilemma on the order/priority of the packets to be transmitted. However, the existing channel access mechanism of VANET does not critically consider the aforementioned aspects when granting channel access, and rather focuses on avoiding and resolving packet collisions. In order to bridge this gap, we present an improved channel access granting mechanism for data routing and scheduling via software defined vehicular networks (SDVN). We scrutinize the aforementioned parameters via a global perspective and utilize diverse channel modes cooperatively to maximize the delivery of data packets under delay tolerance and link connectivity regions. The primary problem is formulated as an integer linear programming (ILP) problem. The model avoids possible packet transmission conflicts via constraints while scrutinizing the wireless nature of links. Moreover, the proposed model is easily extended for the cooperative data communication under road side unit (RSU) application scenario. Furthermore, we present two computational improvement strategies based on incremental optimization and maximum independent sets (MIS) for the two application scenarios to shift the computational complexity to a realizable level for real-time communication. The effectiveness of the proposed routing and scheduling framework is comparatively evaluated with realistic mobility embedded road networks.
Keywords: Optimization; Routing; Scheduling; Software defined networking; Vehicular ad-hoc networks

Morteza Golkarifard, Carla Fabiana Chiasserini, Francesco Malandrino, Ali Movaghar,
Dynamic VNF placement, resource allocation and traffic routing in 5G,
Computer Networks,
Volume 188,
2021,
107830,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107830.
(https://www.sciencedirect.com/science/article/pii/S1389128621000177)
Abstract: 5G networks are going to support a variety of vertical services, with a diverse set of key performance indicators (KPIs), by using enabling technologies such as software-defined networking and network function virtualization. It is the responsibility of the network operator to efficiently allocate the available resources to the service requests in such a way to honor KPI requirements, while accounting for the limited quantity of available resources and their cost. A critical challenge is that requests may be highly varying over time, requiring a solution that accounts for their dynamic generation and termination. With this motivation, we seek to make joint decisions for request admission, resource activation, VNF placement, resource allocation, and traffic routing. We do so by considering real-world aspects such as the setup times of virtual machines, with the goal of maximizing the mobile network operator profit. To this end, first, we formulate a one-shot optimization problem which can attain the optimum solution for small size problems given the complete knowledge of arrival and departure times of requests over the entire system lifespan. We then propose an efficient and practical heuristic solution that only requires this knowledge for the next time period and works for realistically-sized scenarios. Finally, we evaluate the performance of these solutions using real-world services and large-scale network topologies. Results demonstrate that our heuristic solution performs better than a state-of-the-art online approach and close to the optimum.

Sabarish Krishna Moorthy, Nicholas Mastronarde, Scott Pudlewski, Elizabeth Serena Bentley, Zhangyu Guan,
Swarm UAV networking with collaborative beamforming and automated ESN learning in the presence of unknown blockages,
Computer Networks,
Volume 231,
2023,
109804,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109804.
(https://www.sciencedirect.com/science/article/pii/S1389128623002499)
Abstract: This paper aims at designing high-data-rate swarm UAV networks with distributed beamforming capabilities. The primary challenge is that the beamforming gain in swarm UAV networks is highly affected by the UAVs’ flight altitude, their movements and the resulting intermittent link blockages, as well as the availability of channel state information (CSI) at individual UAVs. To address this challenge, we propose FlyBeam, a learning-based framework for joint flight and beamforming control in swarm UAV networks. We first present a mathematical formulation of the control problem with the objective of maximizing the throughput of swarm UAV networks by jointly controlling the flight and distributed beamforming of UAVs. Then, a distributed solution algorithm is designed based on a combination of Echo State Network (ESN) learning and online reinforcement learning. The former is adopted to approximate the utility function for individual UAVs based on online measurements, by jointly considering the unknown blockage dynamics and other factors that affect the beamforming gain. The latter is used to guide the exploitation and exploration in FlyBeam. We further design a scheme referred to as AutoESN to automate the training of the ESN model. AutoESN can update the configurable ESN parameters automatically using a combination of loss function and step size. The effectiveness of FlyBeam is evaluated through an extensive simulation campaign on UBSim, a Python-based Universal Broadband Simulator for integrated aerial and ground wireless networking. The performance of FlyBeam is compared with two benchmark schemes, one designed based on traditional optimization techniques and the other based on learning with utility function approximation through Long Short-Term Memory (LSTM). In the performance evaluation, we consider both Zero-Forcing (ZF) and Maximum Ratio Transfer (MRT) for beamforming with sequential and simultaneous channel state estimation. It is shown that significant (up to 450%) beamforming gain can be achieved by FlyBeam. We also investigate the effects of blockages and UAV flight altitude on the beamforming gain. It is found that, somewhat surprisingly, higher (rather than lower) beamforming gain can be achieved by FlyBeam with denser blockages in swarm UAV networks.
Keywords: Swarm UAV networks; Distributed beamforming; Echo State Network; Reinforcement learning

Rajendra Patil, Sivaanandh Muneeswaran, Vinay Sachidananda, Mohan Gurusamy,
E-Audit: Distinguishing and investigating suspicious events for APTs attack detection,
Journal of Systems Architecture,
Volume 144,
2023,
102988,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102988.
(https://www.sciencedirect.com/science/article/pii/S1383762123001674)
Abstract: To detect Advanced Persistent Threats (APTs), recent research efforts focus on modeling the common attack kill chain. The provenance graph is one of the proven techniques which maintains a long-term historic correlation of the attack stages to model the kill chain. However, it is overwhelming to model the kill chain by inspecting the high volume of system events. The paper proposes E-Audit, a hybrid (event-correlation and classification) approach to model the APT kill chain based on threat-likely events that are suspicious events on the system. E-Audit first distinguishes the threat-likely events and builds a provenance graph for threat-likely events only. Second, it assesses the APT campaigns, identifies the attack-irrelevant campaigns, and removes the corresponding paths from the graph. The novel sequence-based model assesses the APT campaigns as a sequence to identify the attack-relevant and attack-irrelevant campaigns. We evaluated the effectiveness of E-Audit over the audit logs dataset. We validated it in real-time using audit logs generated over the enterprise network set-up. The experimental results show that E-Audit can distinguish 668 threat-likely events from the 183223 system events in 174 ms on average and effectively detect the APT attacks with 99.71% accuracy and generate the kill chain.
Keywords: Advanced Persistent Threats; Threat-likely events; SmartProvenance graph; Graph-optimization; Conclusion stage; MITRE

Federico Chiariotti,
A survey on 360-degree video: Coding, quality of experience and streaming,
Computer Communications,
Volume 177,
2021,
Pages 133-155,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.06.029.
(https://www.sciencedirect.com/science/article/pii/S014036642100253X)
Abstract: The commercialization of Virtual Reality (VR) headsets has made immersive and 360-degree video streaming the subject of intense interest in the industry and research communities. While the basic principles of video streaming are the same, immersive video presents a set of specific challenges that need to be addressed. In this survey, we present the latest developments in the relevant literature on four of the most important ones: (i) omnidirectional video coding and compression, (ii) subjective and objective Quality of Experience (QoE) and the factors that can affect it, (iii) saliency measurement and viewport prediction, and (iv) the adaptive streaming of immersive 360-degree videos. The final objective of the survey is to provide an overview of the research on all the elements of an immersive video streaming system, giving the reader an understanding of their interplay and performance.
Keywords: Video streaming; Virtual Reality; Quality of Experience

Changjiang Gou, Anne Benoit, Mingsong Chen, Loris Marchal, Tongquan Wei,
Mapping series-parallel streaming applications on hierarchical platforms with reliability and energy constraints,
Journal of Parallel and Distributed Computing,
Volume 163,
2022,
Pages 45-61,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.01.016.
(https://www.sciencedirect.com/science/article/pii/S0743731522000211)
Abstract: Streaming applications come from various application fields such as physics, where data is continuously generated and must be processed on the fly. Typical streaming applications have a series-parallel dependence graph, and they are processed on a hierarchical failure-prone platform, as for instance in miniaturized satellites. The goal is to minimize the energy consumed when processing each data set, while ensuring real-time constraints in terms of processing time. Dynamic voltage and frequency scaling (DVFS) is used to reduce the energy consumption, and we ensure a reliable execution by either executing a task at maximum speed, or by triplicating it, so that the time to execute a data set without failure is bounded. We propose a structure rule to partition the series-parallel applications and map the application onto the platform, and we prove that the optimization problem is NP-complete. We design a dynamic-programming algorithm for the special case of linear chains, which is optimal for a special class of schedules. Furthermore, this algorithm provides an interesting heuristic and a building block for designing heuristics for the general case. The heuristics are compared to a baseline solution, where each task is executed at maximum speed. Simulations on realistic settings demonstrate the good performance of the proposed heuristics; in particular, significant energy savings can be obtained.
Keywords: Series-parallel streaming applications; Task mapping; Hierarchical platforms; Reliability; Energy

Mauro Tortonesi, Marco Govoni, Alessandro Morelli, Giulio Riberto, Cesare Stefanelli, Niranjan Suri,
Taming the IoT data deluge: An innovative information-centric service model for fog computing applications,
Future Generation Computer Systems,
Volume 93,
2019,
Pages 888-902,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.06.009.
(https://www.sciencedirect.com/science/article/pii/S0167739X17306702)
Abstract: Fog Computing is a new computation paradigm, recently emerged from the convergence of IoT, WSN, mobile computing, edge computing, and Cloud Computing, which is particularly well suited for Smart City environments. Fog Computing aims at supporting the development of time-sensitive, location-, social-, and context-aware applications by using computational resources in close proximity of information producers and consumers, such as increasingly common cheap and powerful modern hardware platforms. However, realizing Fog Computing solutions for Smart Cities represents a very challenging task, because of the massive amount of data to process, the strict resource and time constraints, and the significant dynamicity and heterogeneity of computation and network resources. These formidable challenges suggest taking into consideration new information and service model solutions that explore several trade-offs between processing speed and accuracy. Along these guidelines, we designed the SPF Fog-as-a-Service platform, which proposes a new information-centric and utility-based service model and allows the definition of self-adaptive and composition-friendly services, which can execute either on edge devices or in the Cloud. In numerous evaluations, SPF proved to be a very effective platform for running Fog services on heterogeneous devices with significantly different computational capabilities while also demonstrating remarkable ease of development and management characteristics.
Keywords: Fog computing; Smart cities; Information-centric networking; Internet-of-Things; Value of information

Jiachen Zeng, Fangfang Gou, Jia Wu,
Task offloading scheme combining deep reinforcement learning and convolutional neural networks for vehicle trajectory prediction in smart cities,
Computer Communications,
Volume 208,
2023,
Pages 29-43,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.05.021.
(https://www.sciencedirect.com/science/article/pii/S0140366423001871)
Abstract: With the development of Artificial Intelligence, the intelligent vehicle with diverse functions and complex system architectures brings more computing tasks to vehicles. Due to insufficient local computing resources for vehicles, mobile edge computing is seen as a solution to relieve local computing pressure. In the background of Telematics, when the vehicle offloads the computation task to the edge server, the communication time between the vehicle and the base station will become shorter due to the high-speed movement of the vehicle. If the vehicle leaves the current base station before the computation is completed, the vehicle will not be able to obtain the computation results in time. Therefore, a task offloading scheme based on trajectory prediction in the context of Telematics is proposed to solve the problem of short communication time between vehicles and base stations due to high-speed movement of vehicles. The solution combines Long Short Term Memory and convolutional neural networks to predict the base station the vehicle will pass and the time to reach it, which enables the return of calculation results through the communication between the base stations and enables tasks with larger data volumes to be offloaded to the edge server. After simulation experiments, the results can prove that the scheme proposed in this paper is adapted to the intelligent vehicle environment, shows greater stability in the face of large computational tasks and reduces about 25% task latency compared to the traditional task offloading scheme.
Keywords: Convolutional neural networks; Reinforcement learning; Smart vehicle; Task offloading; Trajectory prediction

M. Luglio, M. Quadrini, C. Roseti, F. Zampognaro,
A Flexible Web Traffic Generator for the dimensioning of a 5G backhaul in NPN,
Computer Networks,
Volume 221,
2023,
109531,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109531.
(https://www.sciencedirect.com/science/article/pii/S1389128622005655)
Abstract: Web-based services have nowadays aggregated the majority of applications, ranging from multimedia delivery to instant messaging, embracing requirements from different classes of users (i.e., gamers, streamers, professionals, etc.). The common factor is the need for connectivity everywhere and every time at high performance and quality, while the services traffic profiles and the users’ behaviour can be drastically different. This need is fully satisfied by the recent 5G mobile networks, where the coexistence of heterogeneous applications with different Quality of Service (QoS) levels is possible, defining new vertical markets and cutting-edge applications. In this context, we propose a Flexible Web Traffic Generator (FWTG) tool to model the dynamics of recent applications and user interactions in a realistic way, allowing the generation of real-time HTTP traffic and the possibility to inject it into real networks. FTWG represents a valid in-laboratory solution for the rapid configuration and applications performance evaluation of network slices in the context of 5G Non-Public Networks (NPN), pursuing an effective emulation approach. The advantage of this approach is that it allows the evaluation of real traffic response to different network configurations (and vice-versa), using components, equipment and network functions of the target network under test. After the description of the rationale and the methodology for defining FWTG models and their implementation, we defined a set of Key Performance Indicators (KPI) generated by the tool (e.g., number of requested objects and size, request rate, throughput, etc.). Then, we present initial validation tests on a Linux-based testbed representing the 5G-based scenarios of interest, emulating also a satellite backhaul as representative of a use-case requiring cost-effective dimensioning. Numerical outcomes of running FWTG in different configurations are then provided, considering a variation of the number of users sharing the same resources, their composition and traffic type, showing with concrete examples the usefulness and flexibility of FWTG in supporting 5G network configuration optimizations. Definitively, the tool is released as Open Source software on Gitlab.
Keywords: Web traffic generation; QoS; 5G; NPN backhaul

Matthew Bradbury, Arshad Jhumka, Tim Watson,
Information management for trust computation on resource-constrained IoT devices,
Future Generation Computer Systems,
Volume 135,
2022,
Pages 348-363,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.05.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001698)
Abstract: Resource-constrained Internet of Things (IoT) devices are executing increasingly sophisticated applications that may require computational or memory intensive tasks to be executed. Due to their resource constraints, IoT devices may be unable to compute these tasks and will offload them to more powerful resource-rich edge nodes. However, as edge nodes may not necessarily behave as expected, an IoT device needs to be able to select which edge node should execute its tasks. This selection problem can be addressed by using a measure of behavioural trust of the edge nodes delivering a correct response, based on historical information about past interactions with edge nodes that are stored in memory. However, due to their constrained memory capacity, IoT devices will only be able to store a limited amount of trust information, thereby requiring an eviction strategy when its memory is full of which there has been limited investigation in the literature. To address this, we develop the concept of the memory profile of an agent and that profile’s utility. We formalise the profile eviction problem in a unified profile memory model and show it is NP-complete. To circumvent the inherent complexity, we study the performance of eviction algorithms in a partitioned profile memory model using our utility metric. Our results show that localised eviction strategies which only consider one specific type of information do not perform well. Thus we propose a novel eviction strategy that globally considers all types of trust information stored and we show that it outperforms local eviction strategies for the majority of memory sizes and agent behaviours. In this paper, we develop a concept of information utility to a trust model and formalise the problem of information eviction, which we prove to be NP-complete. We then investigate the usefulness of different eviction strategies to maximise the utility of information stored to enable trust-based task offloading.
Keywords: Trust; Information management; IoT; Resource-constraints; Edge nodes; Offloading

Fernando Zanferrari Morais, Cristiano André da Costa, Antonio Marcos Alberti, Cristiano Bonato Both, Rodrigo da Rosa Righi,
When SDN meets C-RAN: A survey exploring multi-point coordination, interference, and performance,
Journal of Network and Computer Applications,
Volume 162,
2020,
102655,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102655.
(https://www.sciencedirect.com/science/article/pii/S1084804520301296)
Abstract: The transformation of mobile networks towards fifth generation (5G) is considered an unprecedented revolution when compared to the previous generations, mainly because of the support required for three scenarios: ultra reliable and low latency; high throughput broadband; and massive machine type communication. In this context, the evolution from RAN to C-RAN is a significant contribution to advance the deployment of this new generation. C-RAN concepts bring opportunities like centralized processing, improved interference control, flexible fronthaul networks, and support for ultra-dense heterogeneous radio access networks. SDN emerges as a candidate technology for the development of programmable 5G networks, with centralized orchestration and control/data planes decoupling. In this article, we propose a systematic literature review to cover SDN application in the context of 5G communication and C-RAN. In combination with the concepts as mentioned earlier, the article evaluates coordination and interference management features provided by Coordinated Multi-point and Inter-cell Interference Coordination techniques, besides reviewing fronthaul transport tier capabilities offered by SDN for 5G networks. A careful literature analysis shows that there are no revisions with the same focus as that developed in this survey. As contributions, this article presents a novel taxonomy of SDN technology, enabling C-RAN, as well as trending directions and development opportunities for SDN and C-RAN-based 5G networks.
Keywords: SDN; Cloud RAN; CoMP; ICIC; Fronthaul; 5G

Pronaya Bhattacharya, Farnazbanu Patel, Sudeep Tanwar, Neeraj Kumar, Ravi Sharma,
MB-MaaS: Mobile Blockchain-based Mining-as-a-Service for IIoT environments,
Journal of Parallel and Distributed Computing,
Volume 168,
2022,
Pages 1-16,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.05.008.
(https://www.sciencedirect.com/science/article/pii/S0743731522001228)
Abstract: In this paper, we propose a mobile blockchain (MB) based mining-as-a-service (MaaS) scheme, MB-MaaS for resource-constrained industrial internet-of-things (IIoT) environments. The scheme addresses the research gaps of fixed static allocation for miners to perform computationally intensive mining tasks through a multi-hop computational offloading (CO) scheme and addresses an auction mechanism for a fair bidding process among the miner nodes. The scheme operates in three phases. In the first phase, a multi-hop CO scheme with a fair incentive policy is formulated for miners. The CO schemes offer guaranteed offloading services to mobile devices from far-edge systems through a chain of neighbor nodes. Then, in the second phase, MaaS is proposed to leverage expensive mining tasks through 5G-enabled pico/femtocells. Integration of 5G allows massive end-to-end device and service connectivity. As IIoT ecosystems have limited memory and compute requirements, MaaS assures that the proposed consensus has a responsive validation and mining time. To make the data exchange in the consensus process lightweight, and allow a large number of sensors to share the data in a lightweight manner, an effective consensus mechanism Lightweight Proof-of-Proximity (LPoP), is proposed that forms group validations instead of single block validation. The data is exchanged through javascript object notation (JSON) format, maintaining a steady transaction rate. MB-MaaS is compared against the existing scheme for parameters bid thresholds and request servicing times, and mining and consensus formation. For example, the request serving time at 12 requests is improved by 56.78%, and a significant improvement of 26.47% is observed for processed blocks; parsing time on average is improved by 7.89%. The comparative analysis suggests that the scheme is more efficient than other competing approaches.
Keywords: Mobile blockchain; Mobile edge computing; 5G; Mining-as-a-Service; Data offloading

Emna Baccour, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani,
Reinforcement learning-based dynamic pruning for distributed inference via explainable AI in healthcare IoT systems,
Future Generation Computer Systems,
Volume 155,
2024,
Pages 1-17,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.01.021.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000232)
Abstract: Deep Neural Networks (DNNs) have become the key technique to revolutionize the healthcare sector. However, conducting online remote inference is often impractical due to privacy constraints and latency requirements. To enable local computation, researchers have attempted network pruning with minimal accuracy loss or DNN distribution without affecting the performance. Yet, distributed inference can be inefficient due to the energy overhead and fluctuation of communication channels between participants. On the other hand, given that realistic healthcare systems use pre-trained models, local pruning and retraining relying only on the available scarce data is not possible. Even pre-pruned DNNs are limited in their ability to customize to the local load of data and device dynamics. The online pruning of DNN inferences without retraining is viable; however, it was not considered in the literature as most well-known techniques do not perform well without adjustment. In this paper, we propose a novel pruning strategy using Explainable AI (XAI) to enhance the performance of pruned DNNs without retraining, a necessity due to the scarcity and bias of local healthcare data. We combine distribution and pruning techniques to perform online distributed inference assisted by dynamic pruning when needed for highest accuracy. We use Non-Linear Integer Programming (NLP) to formulate our approach as a trade-off between resources and accuracy, and Reinforcement Learning (RL) to relax the problem and adapt to dynamic requirements. Our pruning criterion shows high performance compared to other reference techniques and ability to assist distribution by reducing resource usage while keeping high accuracy.
Keywords: Healthcare; Scarce data; Resource constraints; Distributed inference; XAI; Pruning

Adel Djama, Badis Djamaa, Mustapha Reda Senouci,
Information-Centric Networking solutions for the Internet of Things: A systematic mapping review,
Computer Communications,
Volume 159,
2020,
Pages 37-59,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.05.003.
(https://www.sciencedirect.com/science/article/pii/S0140366419316901)
Abstract: Due to the similarity between the data-driven nature of sensor and actuator networks enabling the Internet of Things (IoT) and the data-oriented model of Information-Centric Networks (ICN), recent research began investigating ICN-based IoT systems. This paper provides a thorough systematic mapping review of such research with the aim to identify their strengths, weaknesses, and open-research issues. Thus, after introducing the IoT ecosystem, its main requirements, existing IP-based solutions, and their limitations, the survey investigates the ICN-IoT associations that have been proposed in the recent literature. To do so, a new taxonomy that captures the fundamental aspects of ICN-based IoT solutions is introduced along with a multidimensional framework that provides a comprehensive multi-criteria analysis of the reviewed research. This paper also summarizes the main observations learned from the analysis and draws recommendations about open research issues that require the attention of the community. Such issues include limited standardization efforts, hybrid ICN/IP deployments, push-based communications, efficient caching schemes, and QoS solutions.
Keywords: Internet of Things; TCP/IP; Information-Centric Networks; Named Data Networking; Content-Centric Networking; Host-Centric communication; Data-Centric communication

Haotian Wang, Wangdong Yang, Rong Hu, Renqiu Ouyang, Kenli Li, Keqin Li,
IAP-SpTV: An input-aware adaptive pipeline SpTV via GCN on CPU-GPU,
Journal of Parallel and Distributed Computing,
Volume 181,
2023,
104741,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104741.
(https://www.sciencedirect.com/science/article/pii/S0743731523001119)
Abstract: Sparse tensor-times-vector (SpTV) is the core computation of tensor decomposition. Optimizing the computational performance of SpTV on CPU-GPU becomes a challenge due to the complexity of the non-zero element sparse distribution of the tensor. To solve this problem, we propose IAP-SpTV, an input-aware adaptive pipeline SpTV via Graph Convolutional Network (GCN) on CPU-GPU. We first design the hybrid tensor format (HTF) and explore the challenges of the HTF-based Pipeline SpTV algorithm. Second, we construct Slice-GCN to overcome the challenge of selecting a suitable format for each slice of HTF. Third, we construct an IAP-SpTV performance model for pipelining to achieve the maximum overlap between transfer and computation time during pipelining. Finally, we conduct experiments on two CPU-GPU platforms of different architectures to verify the correctness, effectiveness, and portability of IAP-SpTV. Overall, IAP-SpTV provides a significant performance improvement of about 24.85% to 58.42% compared to the state-of-the-art method.
Keywords: Format selection; GCN; Hybrid format; Pipeline model; SpTV

Chia-Cheng Hu, Wen-Wu Liu, Jeng-Shyang Pan,
Minimizing traffic cost of content distribution and storage allocation in cloud radio access networks,
Computer Networks,
Volume 231,
2023,
109836,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109836.
(https://www.sciencedirect.com/science/article/pii/S1389128623002815)
Abstract: In Cloud Radio Access Networks (C-RANs), remote radio heads (RRHs) and baseband units (BBUs) are deployed for storing contents to alleviate the bottleneck of the data cloud, shorten the delay of content request, and reduce the cost of network service. In this paper, we advocate the issue of content distribution in C-RANs to meet the ever-increasing contents of 5G cellular networks. Our goal is to minimize the total traffic cost of content transmission by jointly distributing content and allocating storage in RRHs and BBUs of C-RANs. The problem is formulated as a mix-integer linear programming (MIP). Then, an algorithm with a bounded approximation ratio is proposed to obtain the feasible solution of the MIP. The simulation results validate that the proposed algorithm outperforms two very recent heuristic algorithms in the metric of traffic cost. They also demonstrate that the difference between the solution obtained by the algorithm and the optimal solution of the MIP is very small. In addition, the experimental results show that the proposed algorithm is feasible in C-RANs. Further, another algorithm is proposed for exchanging contents between neighbor RRHs in content provision.
Keywords: Cloud radio access network (C-RAN); Content distribution; Storage allocation

Simar Preet Singh, Anju Sharma, Rajesh Kumar,
Designing of fog based FBCMI2E Model using machine learning approaches for intelligent communication systems,
Computer Communications,
Volume 163,
2020,
Pages 65-83,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.09.005.
(https://www.sciencedirect.com/science/article/pii/S0140366420319198)
Abstract: The work discusses the evolution of communication models and technological aspects for developing inclusive platforms. The paper gives illustrations and graphical presentations about components of the inclusive platform. An inclusive platform consists of fog devices that can collect the fitness data, geospatial data and mobile call details from the people who are in need of help from the government agencies. The paper discusses the controversies around the definition and qualification of the informal economy as well as presents a simulated scenario. The case presented here gives an insight on how fog devices and data mining can be used for bringing people into main fold of the economy. The inclusiveness index of the person is computed on the basis of four aspects. The first aspect is the fitness, the second is his inner social circle, third is her/his reliability to remain in a place, and last is call analysis. While computing the fitness index, it was found that Naive Bayes (NB) algorithm has the maximum accuracy with respect to K-Nearest Neighbors (KNN), Decision Tree (DT) and Linear Discriminant Analysis (LDA). For computing inner social circle, Louvain algorithm helped to compute stability and strength of socio-economic ties of the individual. For geospatial and call analysis, insights from knowledge discovery algorithm such as FP-Growth helped to arrive at decision to qualify the person for inclusive program. The paper ends with details on how to automate the inclusiveness index computation using neural network. The research indicates that energy is the key constraint for implementing such programs. Hence, a theoretical analysis about energy efficiency is also explained in the paper.
Keywords: Inclusive technology; Fog computing; Communication model; Machine learning; Fitness index; Energy efficiency

Sebastiano Miano, Giuseppe Lettieri, Gianni Antichi, Gregorio Procissi,
Accelerating network analytics with an on-NIC streaming engine,
Computer Networks,
Volume 241,
2024,
110231,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110231.
(https://www.sciencedirect.com/science/article/pii/S138912862400063X)
Abstract: Data Stream Processing engines have recently emerged as powerful tools for simplifying the analysis of network telemetry data. Motivated by the ever-growing volume of data requiring analysis, cutting-edge approaches integrate them with programmable switches to filter out less relevant traffic and enhance their processing capabilities. In this paper, we propose an alternative solution: leveraging SmartNICs as high-performance accelerators for stream processing operations. SmartNICs are commonly deployed in datacenter networks, and their architecture is often characterized by numerous low-power processors that align seamlessly with the highly parallelizable computational requirements of standard streaming analysis frameworks. Starting from WindFlow, a state-of-the-art stream processor, we present an innovative architecture that enables the offloading of a portion of its computation to a commodity Netronome SmartNIC. We implemented the offload logic using eBPF, making our solution compatible with any NIC supporting this programming paradigm. We developed a diverse range of applications (i.e., flow metering, port scan detection and SYN flood attack detection) and show that our solution can analyze up to 40% more traffic compared to a pure software approach.
Keywords: Stream processing; Computation offload; SmartNICs; Accelerated data path; eBPF/XDP

Tiechui Yao, Jue Wang, Meng Wan, Zhikuang Xin, Yangang Wang, Rongqiang Cao, Shigang Li, Xuebin Chi,
VenusAI: An artificial intelligence platform for scientific discovery on supercomputers,
Journal of Systems Architecture,
Volume 128,
2022,
102550,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102550.
(https://www.sciencedirect.com/science/article/pii/S1383762122001059)
Abstract: Since the machine learning platform can provide one-stop artificial intelligence (AI) application solutions, it has been widely used in the industrial and commercial internet fields in recent years. Based on the heterogeneous accelerator cards, scientific discovery using large-scale computation and massive data is a significant tendency in the future. However, building a platform for scientific discovery remains challenging, including large-scale heterogeneous resource scheduling and support for massive multi-source data. To free researchers from tedious resource management and environmental configuration, we propose a VenusAI platform for large-scale computing scenarios in scientific research, based on heterogeneous resources scheduling framework. This paper firstly illustrates the VenusAI platform architecture design scheme based on the supercomputers and elaborates on the virtualization and containerization of the underlying hardware resources. Next, a technical framework for heterogeneous resource aggregation and scheduling is proposed. A unified resource interface in the application service layer is introduced. Considering the core three parts of the AI scenario: data, model, and computing power, modularized service decoupling is carried out. Furthermore, three types of experiments are evaluated on the supercomputers and show that the performance of the scheduling framework on virtual clusters is better than that on common clusters. Finally, three scientific discovery applications deployed on VenusAI, i.e., new energy forecasting, materials design, and unmanned aerial vehicle planning, demonstrate the advantages of the platform in solving practical scientific problems.
Keywords: AI platform; Scientific discovery; Large-scale computation; Supercomputer; AI application

Ahlem Saddoud, Wael Doghri, Emna Charfi, Lamia Chaari Fourati,
5G radio resource management approach for multi-traffic IoT communications,
Computer Networks,
Volume 166,
2020,
106936,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106936.
(https://www.sciencedirect.com/science/article/pii/S1389128618303876)
Abstract: Nowadays, Internet of Things (IoT) communications have significant impact on our social life due to the exponential growth of many objects and devices creating a fully interconnected world. To bear this growth explosion of IoT communication traffics and to support the diversity of IoT applications and services, 5G network will be the suitable solution with a needed enhancements to reach emerging features such as Human-to-Human (H–H) and machine-to-machine (M–M) communications. In this context, several key issues for IoT communications in 5G networks should be addressed to satisfy quality of service (QoS) provisioning. Radio Resource Management or scheduling is an important key issue for IoT communications in 5G network. In this paper, we propose two schedulers for IoT communications based on QoS requirements. The proposed schemes give best scenarios that aim to provide a trade-off between the two types of traffics by guaranteeing the network performance and avoiding ineffective exploitation of available resources. The simulation results prove that the proposed schedulers operate efficiently in terms of maximizing the bandwidth utilisation rate which is the important challenge of the 5G radio resource management.
Keywords: Internet of Things; LTE-A; 5G; Static scheduler; Dynamic scheduler; Borrowing; Resource optimisation; QoS,

Shah Zeb, Aamir Mahmood, Sunder Ali Khowaja, Kapal Dev, Syed Ali Hassan, Mikael Gidlund, Paolo Bellavista,
Towards defining industry 5.0 vision with intelligent and softwarized wireless network architectures and services: A survey,
Journal of Network and Computer Applications,
Volume 223,
2024,
103796,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103796.
(https://www.sciencedirect.com/science/article/pii/S1084804523002151)
Abstract: Industry 5.0 vision, a step toward the next industrial revolution and enhancement to Industry 4.0, conceives the new goals of resilient, sustainable, and human-centric approaches in diverse emerging applications such as factories-of-the-future and digital society. The vision seeks to leverage human intelligence and creativity in nexus with intelligent, efficient, and reliable cognitive collaborating robots (cobots) to achieve zero waste, zero-defect, and mass customization-based manufacturing solutions. However, it requires merging distinctive cyber–physical worlds through intelligent orchestration of various technological enablers, e.g., cognitive cobots, human-centric artificial intelligence (AI), cyber–physical systems, digital twins, hyperconverged data storage and computing, communication infrastructure, and others. In this regard, the convergence of the emerging computational intelligence (CI) paradigm and softwarized next-generation wireless networks (NGWNs) can fulfill the stringent communication and computation requirements of the technological enablers of the Industry 5.0, which is the aim of this survey. In this article, we address this issue by reviewing and analyzing current emerging concepts and technologies, e.g., CI tools and frameworks, network-in-box architecture, open radio access networks, softwarized service architectures, potential enabling services, and others, elemental and holistic for designing the objectives of CI-NGWNs to fulfill the Industry 5.0 vision requirements. Furthermore, we outline and discuss ongoing initiatives, demos, and frameworks linked to Industry 5.0. Finally, we provide a list of lessons learned from our detailed review, research challenges, and open issues that should be addressed in CI-NGWNs to realize Industry 5.0.
Keywords: Industry 5.0; Computational intelligence; Next-generation wireless networks; Softwarized network architectures; Intelligent Network-in-Box; Multi-tenant service orchestration; Network management and orchestration; Open RAN; Factories-of-the-Future; Human–robots collaboration; Industrial communications; Industrial digital twins and Metaverse

Natalia Yarkina, Luis M. Correia, Dmitri Moltchanov, Yuliya Gaidamaka, Konstantin Samouylov,
Multi-tenant resource sharing with equitable-priority-based performance isolation of slices for 5G cellular systems,
Computer Communications,
Volume 188,
2022,
Pages 39-51,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.02.019.
(https://www.sciencedirect.com/science/article/pii/S0140366422000627)
Abstract: Network slicing is a promising technique to enable multi-tenant operation in fifth generation (5G) cellular systems. However, efficient implementation of this technique at the air interface requires an adequate resource allocation policy that provides slice isolation and takes into account the heterogeneity of services and their performance requirements. We propose a new slicing scheme aimed at slice performance isolation, efficient capacity utilization, and fair resource allocation among all users. The policy ensures a slice-specific minimum data rate to all slice users as long as their number remains within a contracted limit, yet provides higher data rates whenever capacity is available. Specifically, we incorporate the best features of the complete partitioning and complete sharing policies, while ensuring flexibility and customization for network operators. The proposed scheme is based on an iterative solution of a convex programming problem characterized by polynomial complexity, which makes it suitable for on-line implementation. Associated algorithms are also proposed. Our numerical results demonstrate that the proposed slicing scheme is capable to accommodate heterogeneous traffic and provides performance isolation of slices while maintaining resource utilization at the level of complete sharing. Depending on the system parameters, the proposed scheme allows to improve session loss probability by an order of magnitude as compared to static slicing, while keeping the user data rates comparable to that of the complete sharing scheme and improving the average user satisfaction index by up to 90%. Overall, the proposed scheme results in efficient resource utilization compared to slicing policies in which slice isolation is provided via reservation by setting fixed upper or lower capacity bounds.
Keywords: Network slicing; Quality assurance; Performance isolation; Prioritization; Preemption

Weifeng Lu, Weiduo Wu, Jia Xu, Pengcheng Zhao, Dejun Yang, Lijie Xu,
Auction design for cross-edge task offloading in heterogeneous mobile edge clouds,
Computer Communications,
Volume 181,
2022,
Pages 90-101,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.09.035.
(https://www.sciencedirect.com/science/article/pii/S0140366421003728)
Abstract: Task offloading is a promising technology to exploit the available resources in edge cloud efficiently. Many incentive mechanisms for offloading systems have been proposed. However, most of existing works study the centralized incentive mechanisms under the assumption that all mobile edge infrastructures are operated by a central cloud. In this paper, we aim to design the auction-based truthful incentive mechanisms for heavily loaded task offloading system in heterogeneous MECs. We first study the homogeneous MEC situation and present a global auction executed in the central cloud as a benchmark. For the heterogeneous MEC situation, we model the system as a dual auction framework, which enables the heterogeneous MECs to perform cross-edge task offloading without the participation of central servers. Specifically, we design two dual auction models: secondary auction-based model, which enables the system to offload tasks from a large-scale region in a single auction, and double auction-based model, which is suitable for the time sensitive tasks. Then the auctions for these two dual auction models are proposed. Through rigorous theoretical analysis, we demonstrate that the proposed auctions achieve desirable properties of computational efficiency, individual rationality, budget balance, truthfulness, and guaranteed approximation. The simulation results show that the secondary auction and double auction can obtain 14.5% and 4.2% more social welfare than comparison algorithm on average, respectively. In addition, the double auction has great advantage in terms of computation efficiency.
Keywords: Mobile edge computing; Task offloading; Incentive mechanism; Auction

Taspon Gonggiatgul, Ghassan Shobaki, Pınar Muyan-Özçelik,
A parallel branch-and-bound algorithm with history-based domination and its application to the sequential ordering problem,
Journal of Parallel and Distributed Computing,
Volume 172,
2023,
Pages 131-143,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.10.007.
(https://www.sciencedirect.com/science/article/pii/S0743731522002167)
Abstract: In this paper, we describe the first parallel Branch-and-Bound (B&B) algorithm with a history-based domination technique. Although history-based domination substantially speeds up a B&B search, it makes parallelization much more challenging. Our algorithm is the first parallel exact algorithm for the Sequential Ordering Problem using a pure B&B approach. To effectively explore the solution space, we have developed three novel parallelization techniques: thread restart, parallel history domination, and history-table memory management. The proposed algorithm was experimentally evaluated using the SOPLIB and TSPLIB benchmarks on multi-core processors. Using 32 threads with a time limit of one hour, the algorithm gives geometric-mean speedups of 72x and 20x on the medium-difficulty SOPLIB and TSPLIB instances, respectively. On the hard instances, it solves 12 instances that the sequential algorithm does not solve, with geometric-mean speedups of 16x on SOPLIB and 32x on TSPLIB. Super-linear speedups up to 366x are seen on 16 instances.
Keywords: Parallel branch-and-bound; Sequential ordering problem; Combinatorial optimization; NP-complete problems; History domination

Duc Thang Ha, Lila Boukhatem, Megumi Kaneko, Nhan Nguyen-Thanh, Steven Martin,
Adaptive beamforming and user association in heterogeneous cloud radio access networks: A mobility-aware performance-cost trade-off,
Computer Networks,
Volume 160,
2019,
Pages 130-143,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.05.005.
(https://www.sciencedirect.com/science/article/pii/S138912861830714X)
Abstract: Heterogeneous Cloud Radio Access Network (H-CRAN) is a promising network architecture for the future 5G mobile communication system to address the increasing demand for mobile data traffic. In this work, we consider the design of efficient joint beamforming and user clustering (user-to-Remote Radio Head (RRH) association) in the downlink of a H-CRAN where users have different mobility profiles. Given the rapidly time-varying nature of such wireless environment, it becomes very challenging to enable optimized beamforming and user clustering without incurring large Channel State Information (CSI) and signaling overheads. The main objective of this work is to investigate and evaluate the trade-off between system throughput and the incurred costs in terms of complexity and signaling overhead, including the impact of different CSI feedback strategies given different user mobility profiles. We propose the Adaptive Beamforming and User Clustering (ABUC) algorithm which adapts its feedback parameters, namely the period of dynamic user clustering and the type of CSI feedback, in function of user mobility. Furthermore, we design a reinforcement-learning framework which enables the proposed ABUC algorithm to optimize its scheduling parameters on-the-fly, given each user mobility profile. Based on computer simulations, an analysis of the effect of mobility on system performance metrics is presented and conclusions are drawn regarding the algorithm’s adequate parameter tuning for different mobility scenarios.11This is an extended version of the conference paper presented at IEEE PIMRC 2017 [1].
Keywords: H-CRAN; Beamforming; Clustering; User-to-RRH association; CSI Overhead

Ahmed Alioua, Houssem-eddine Djeghri, Mohammed Elyazid Tayeb Cherif, Sidi-Mohammed Senouci, Hichem Sedjelmaci,
UAVs for traffic monitoring: A sequential game-based computation offloading/sharing approach,
Computer Networks,
Volume 177,
2020,
107273,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107273.
(https://www.sciencedirect.com/science/article/pii/S1389128619315798)
Abstract: Recently, UAVs or Unnamed Aerial Vehicles have been proposed as flexible aerial support to assist ground vehicles for different applications such as rescue and traffic surveillance missions. UAVs can collect different data information about the road/traffic state usually as aerial photography and videos. The processing of this kind of data consists usually on pattern recognition and video processing which are complex tasks that necessitate powerful computing and energy resources. Unfortunately, the moderate UAV's computational and energy capabilities restrict local data processing. Fortunately, UAVs can leverage the computation resources of the surrounding edge network entities to enhance their computational capabilities. In this paper, we aim to achieve efficient data processing for the data collected by UAVs in the context of UAVs-aided vehicular networks for traffic monitoring missions. For this purpose, we propose a new system model where UAVs can offload and/or share intensive computation tasks with other nearby network nodes. Then, we use the computation response time, the energy consumed for the computation, the cost of cellular communication and the computation cost as the main system metrics to make any computation offloading/sharing decisions that optimize the system performance. We then modele the offloading/sharing decision-making problem as a sequential game, where we provide complete proof of the existence of the Nash equilibrium and propose an algorithm to reach such an equilibrium. The simulation results showed that the proposed game-based model outperforms other approaches by delivering better performance in terms of overall system utility with a data processing efficiency that varies between 43% and 97% depending on the computation approach, and provides a more efficient computation time and energy average.
Keywords: Unnamed arial vehicles; Vehicular networks; Computation offloading/sharing; Game theory

Fatemeh Abdi, Mahmood Ahmadi, Montajab Ghanem,
LA-MDPF: A forwarding strategy based on learning automata and Markov decision process in named data networking,
Future Generation Computer Systems,
Volume 134,
2022,
Pages 22-39,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.03.038.
(https://www.sciencedirect.com/science/article/pii/S0167739X22001194)
Abstract: Recently, Named Data Networking (NDN) has emerged as a leading paradigm among several Informa- tion-Centric Networking (ICN) schemes promising to address the shortcomings of TCP/IP architecture. Despite the great importance of the forwarding strategy in NDN, researches in this area are still somewhat limited. This paper presents a new strategy to improve the forwarding of request packets to the best interfaces, which improves the overall performance in NDN. Markov Decision Process (MDP) is employed to model this problem, where a router is presented as a decision-maker. To improve the performance of forwarding strategy, a Learning Automata (LA)-based algorithm is proposed, in which the experiences are considered in forwarding decisions. The proposed strategy (LA-MDPF) evaluates the previous decisions and responds by a penalty or a reward, affecting the probabilities of making the same decisions in the future. LA-MDPF can trace the best interface up to the original content or its cached replicas and overcome link failures and resource constraints. The LA-MDPF performance is evaluated by computer simulations using ndnSIM. Simulation results show that, the proposed algorithm improves content delivery performance in terms of network load balancing, average throughput, average packet drop, average Interest satisfaction ratio (ISR), and average content retrieval time at a reasonable computational cost compared to existing works.
Keywords: LA-MDP; Learning automata; Markov decision process; Named data networking; NDN-forwarding

Balázs Sonkoly, Dávid Haja, Balázs Németh, Márk Szalay, János Czentye, Róbert Szabó, Rehmat Ullah, Byung-Seo Kim, László Toka,
Scalable edge cloud platforms for IoT services,
Journal of Network and Computer Applications,
Volume 170,
2020,
102785,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102785.
(https://www.sciencedirect.com/science/article/pii/S1084804520302599)
Abstract: Nowadays, online applications are moving to the cloud, and for delay-sensitive ones, the cloud is being extended with edge/fog domains. Emerging cloud platforms that tightly integrate compute and network resources enable novel services, such as versatile IoT (Internet of Things), augmented reality or Tactile Internet applications. Virtual infrastructure managers (VIMs), network controllers and upper-level orchestrators are in charge of managing these distributed resources. A key and challenging task of these orchestrators is to find the proper placement for software components of the services. As the basic variant of the related theoretical problem (Virtual Network Embedding) is known to be NP-hard, heuristic solutions and approximations can be addressed. In this paper, we propose two architecture options together with proof-of-concept prototypes and corresponding embedding algorithms, which enable the provisioning of delay-sensitive IoT applications. On the one hand, we extend the VIM itself with network-awareness, typically not available in today's VIMs. On the other hand, we propose a multi-layer orchestration system where an orchestrator is added on top of VIMs and network controllers to integrate different resource domains. We argue that the large-scale performance and feasibility of the proposals can only be evaluated with complete prototypes, including all relevant components. Therefore, we implemented fully-fledged solutions and conducted large-scale experiments to reveal the scalability characteristics of both approaches. We found that our VIM extension can be a valid option for single-provider setups encompassing even 100 edge domains (Points of Presence equipped with multiple servers) and serving a few hundreds of customers. Whereas, our multi-layer orchestration system showed better scaling characteristics in a wider range of scenarios at the cost of a more complex control plane including additional entities and novel APIs (Application Programming Interfaces).
Keywords: Edge computing; Resource orchestration; SDN; NFV; IoT

Yaser Azimi, Saleh Yousefi, Hashem Kalbkhani, Thomas Kunz,
Mobility aware and energy-efficient federated deep reinforcement learning assisted resource allocation for 5G-RAN slicing,
Computer Communications,
Volume 217,
2024,
Pages 166-182,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.01.028.
(https://www.sciencedirect.com/science/article/pii/S0140366424000276)
Abstract: Network slicing is one of the foundations for the realization of 5G and beyond. However, due to the mobility of the users and the network dynamics, flexible and efficient radio access network (RAN) resource slicing is still a challenge. In this paper, we allocate the power and radio block (RB) resources in a multi-RAN scenario to the users of both rate-based and resource-based slices. We propose a mobility-aware and energy-efficient federated deep reinforcement learning-assisted resource allocation (ME-FDRL-RA) method for RAN slicing in a large multiple RAN environment. ME-FDRL-RA includes both federated deep reinforcement learning (FDRL) and deep learning (DL) models as follows: Stacked and bidirectional long-short-term-memory (SBiLSTM), allocate resources to slices on a large time-scale. Additionally, federated advantage actor-critic (F-A2C) allocates resources on a small time-scale and speeds up convergence. Moreover, to solve the optimization problem of determining the required resources for the users of slices, we propose an efficient iterative algorithm called the interference-aware and energy-efficient power allocation (IA-EPA) method. According to simulation results, ME-FDRL-RA outperforms competitive methods in terms of convergence speed, computational complexity, energy efficiency, and the number of accepted users while addressing the challenges of user mobility and maintaining a desirable degree of inter-slice isolation.
Keywords: RAN slicing; 5G; Deep reinforcement learning; Mobility awareness; Power allocation; Federated learning

Emna Baccour, Aiman Erbad, Amr Mohamed, Fatima Haouari, Mohsen Guizani, Mounir Hamdi,
RL-OPRA: Reinforcement Learning for Online and Proactive Resource Allocation of crowdsourced live videos,
Future Generation Computer Systems,
Volume 112,
2020,
Pages 982-995,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.06.038.
(https://www.sciencedirect.com/science/article/pii/S0167739X20306269)
Abstract: With the advancement of rich media generating devices, the proliferation of live Content Providers (CP), and the availability of convenient internet access, crowdsourced live streaming services have witnessed unexpected growth. To ensure a better Quality of Experience (QoE), higher availability, and lower costs, large live streaming CPs are migrating their services to geo-distributed cloud infrastructure. However, because of the dynamics of live broadcasting and the wide geo-distribution of viewers and broadcasters, it is still challenging to satisfy all requests with reasonable resources. To overcome this challenge, we introduce in this paper a prediction driven approach that estimates the potential number of viewers near different cloud sites at the instant of broadcasting. This online and instant prediction of distributed popularity distinguishes our work from previous efforts that provision constant resources or alter their allocation as the popularity of the content changes. Based on the derived predictions, we formulate an Integer-Linear Program (ILP) to proactively and dynamically choose the right data center to allocate exact resources and serve potential viewers, while minimizing the perceived delays. As the optimization is not adequate for online serving, we propose a real-time approach based on Reinforcement Learning (RL), namely RL-OPRA, which adaptively learns to optimize the allocation and serving decisions by interacting with the network environment. Extensive simulation and comparison with the ILP have shown that our RL-based approach is able to present optimal results compared to heuristic-based approaches.
Keywords: Live streaming; QoE; Geo-distributed clouds; Machine and reinforcement learning

The-Vinh Nguyen, Ngoc Phi Nguyen, Cheonshik Kim, Nhu-Ngoc Dao,
Intelligent aerial video streaming: Achievements and challenges,
Journal of Network and Computer Applications,
Volume 211,
2023,
103564,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103564.
(https://www.sciencedirect.com/science/article/pii/S1084804522002053)
Abstract: Deploying aerial infrastructure to support traditional terrestrial wireless communications has enhanced the maturity of 5G networks and beyond. This improvement has certainly benefited video streaming services whose data makes up the bulk of mobile traffic and continues to soar. Realising this fashion, artificial intelligence is considered as one of the important tools, which can effectively deal with various problems to improve and optimise many aspects of video streaming systems. However, there is still a lack of a systematic investigation of recent works in this field to facilitate researchers with a comprehensive reference framework to advance the development of the technology. Realising this shortcoming, this paper has dedicatedly investigated the recent achievements in using AI and the future challenges of the system. First, the basic knowledge and overall architecture of the aerial video streaming (AVS) system are given. Subsequently, achievable utilities are thoroughly evaluated by highlighting each system performance mathematically and analysing recent technical works to address them. Next, typical application scenarios are acquired to demonstrate the advantages to human life and the necessity of their implementation. Finally, open challenges are discussed to drive future technological developments towards video streaming over aerial infrastructure.
Keywords: Aerial infrastructure; Artificial intelligence; Video streaming; Wireless communication

Guanjie Lin, Mingyuan Zeng, Zhiguang Shan, Kaishun Wu, Guan Wang, Kai Lei,
Blockchain-based cooperative game bilateral matching architecture for shared storage,
Future Generation Computer Systems,
Volume 158,
2024,
Pages 122-137,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.04.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X24001456)
Abstract: The development of IPFS (InterPlanetary File System) and blockchain-based distributed storage projects has brought new possibilities to the field of storage. This paper proposes a blockchain-based cooperative game bilateral matching architecture as a novel approach for shared storage networks. In traditional competitive (non-cooperative) game models, the allocation of storage resources is centered around pricing, leading to a scenario where node providers often engage in price competition to obtain greater rewards, resulting in an imbalance in resource allocation for both buyers and sellers. In contrast, a distributed storage model based on cooperative game theory can better facilitate cooperation and resource sharing among node providers. This paper designs a storage resource allocation algorithm based on the stable marriage matching algorithm, demonstrating the stability of this algorithm as a matching solution. The paper also analyzes the differences between cooperative and non-cooperative game models in the market, and explores an equilibrium pricing mechanism guided by supply and demand. Furthermore, the paper introduces a trading mechanism for storage resources, including publication standards and matching schemes, ensuring efficient and trustworthy interaction between storage suppliers and demanders in a decentralized network centered around storage resources, thus enabling the circulation of the value of storage resources. A prototype of a blockchain-based shared storage trading system is implemented in this paper, utilizing bilateral matching for tradings. System evaluation and experimental testing are conducted, with results showing that the average utility value of the matching trading mechanism proposed in this paper outperforms the Double Auction-based matching model under any Poisson distribution (λ = 0.1, 0.2, 0.3, …, 0.9) conditions set in the experiments. Additionally, compared to the traditional approach of directly storing complete data content on the chain, the design proposed in this paper effectively reduces on-chain storage consumption by approximately 27.06%.
Keywords: Decentralized shared storage network; Bilateral matching; Decentralized storage; Blockchain

Fillipe Santos, Roger Immich, Edmundo R.M. Madeira,
Multimedia services placement algorithm for cloud–fog hierarchical environments,
Computer Communications,
Volume 191,
2022,
Pages 78-91,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.04.009.
(https://www.sciencedirect.com/science/article/pii/S014036642200113X)
Abstract: With the rapid development of mobile communication, multimedia services have experienced explosive growth in the last few years. The high quantity of mobile users, both consuming and producing these services to and from the Cloud Computing (CC), can outpace the available bandwidth capacity. Fog Computing (FG) presents itself as a solution to improve on this and other issues. With a reduction in network latency, real-time applications benefit from improved response time and greater overall user experience. Taking this into account, the main goal of this work is threefold. Firstly, it is proposed a method to build an environment based on Cloud–Fog Computing (CFC). Secondly, it is designed two models based on Autoregressive Integrated Moving Average (ARIMA) and Long Short-Term Memory (LSTM). The goal is to predict demand and reserve the nodes’ storage capacity to improve the positioning of multimedia services. Later, an algorithm for the multimedia service placement problem which is aware of data traffic prediction is proposed. The goal is to select the minimum number of nodes, considering their hardware capacities for providing multimedia services in such a way that the latency for servicing all the demands is minimized. An evaluation with actual data showed that the proposed algorithm selects the nodes closer to the user to meet their demands. This improves the services delivered to end-users and enhances the deployed network to mitigate provider costs. Moreover, reduce the demand to Cloud allowing turning off servers in the data center not to waste energy.
Keywords: Cloud-to-fog networks; Multimedia services; Placement strategies

Fabiana Rossi, Valeria Cardellini, Francesco Lo Presti, Matteo Nardelli,
Geo-distributed efficient deployment of containers with Kubernetes,
Computer Communications,
Volume 159,
2020,
Pages 161-174,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.04.061.
(https://www.sciencedirect.com/science/article/pii/S0140366419317931)
Abstract: Software containers are changing the way applications are designed and executed. Moreover, in the last few years, we see the increasing adoption of container orchestration tools, such as Kubernetes, to simplify the management of multi-container applications. Kubernetes includes simple deployment policies that spread containers on computing resources located in the cluster and automatically scale them out or in based on some cluster-level metrics. As such, Kubernetes is not well-suited for deploying containers in a geo-distributed computing environment and dealing with the dynamism of application workload and computing resources. To tackle the problem, in this paper we present ge-kube (Geo-distributed and Elastic deployment of containers in Kubernetes), an orchestration tool that relies on Kubernetes and extends it with self-adaptation and network-aware placement capabilities. Ge-kube introduces flexible and decentralized control loops that can be easily equipped with different deployment policies. Specifically, we propose a two-step control loop, in which a model-based reinforcement learning approach dynamically controls the number of replicas of individual containers on the basis of the application response time, and a network-aware placement policy allocates containers on geo-distributed computing resources. To address the placement issue, we propose an optimization problem formulation and a network-aware heuristic, which explicitly take into account the non-negligible network delays among computing resources so to satisfy Quality of Service requirements of latency-sensitive applications. Using a surrogate CPU-intensive application and a real application (i.e., Redis), we conducted an extensive set of experiments, which show the benefits arising from the combination of elasticity and placement policies, as well as the advantages of using network-aware placement solutions.
Keywords: Kubernetes; Containers; Elasticity; Placement; Self-management; Geographically distributed resources

Arshdeep Singh, Gulshan Kumar, Rahul Saha, Mauro Conti, Mamoun Alazab, Reji Thomas,
A survey and taxonomy of consensus protocols for blockchains,
Journal of Systems Architecture,
Volume 127,
2022,
102503,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102503.
(https://www.sciencedirect.com/science/article/pii/S1383762122000777)
Abstract: Blockchain is an emerging decentralized and distributed technology. Along with the beneficial features of decentralization, transparency, and security the consensus algorithms of blockchains form key building blocks for this technology. Consensus protocol/algorithm helps to provide a decentralized decision making process. An efficient consensus algorithm is inclusive that engages all the participants to make their decision based on the conflicts of the blockchain networks. These consensus decisions lead to better quality outcomes of the blockchains and help to obtain the finality. Rigorous research is in process to upgrade or optimize the existing consensus protocols. The optimized or enhanced consensus protocols objectify to be suitable for Internet-of-Thing (IoT) as the current versions of the protocols are not suitable for the resource-constrained environments due their complexity, hard configurations, mining techniques, high resource consumption, and explicit security loophole. In this paper, we present a survey of consensus protocols with a purpose to identify and discuss the existence of various consensus protocols available in literature. We emphasize on the genesis of the consensus protocols, particularly for Proof-of-X, byzantine fault tolerance, Paxos, and RAFT; we also include Directed Acyclic Graph (DAG) orientation of some contemporary algorithms. We discuss the variants of these genesis protocols. Our survey analyzes the advantages, disadvantages, and their applicability in IoTs. We enlist the categorical use of consensus algorithms in blockchains and other applications. Finally, we present several research trends and open issues emphasizing for consensus protocols emphasizing on IoTs. Compared to the other surveys in the field, our present survey objectifies to provide a more thorough summary of the most relevant protocols and application issues; this survey helps the researchers and the application developers to obtain an insight on the current status of the consensus protocols’ suitability to deliver the desired functionalities in IoTs. The notified disadvantages of each of the protocol provide future scope for the industries and academia. To the best of our knowledge, such a comprehensive and summarized survey of consensus protocols including DAG-based protocols is unavailable in the literature and thus, our contribution claims are significant.
Keywords: Consensus; Blockchain; Bitcoin; Protocol; Decentralization

Yanling Shao, Chunlin Li, Zhao Fu, Leyue Jia, Youlong Luo,
Cost-effective replication management and scheduling in edge computing,
Journal of Network and Computer Applications,
Volume 129,
2019,
Pages 46-61,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.01.001.
(https://www.sciencedirect.com/science/article/pii/S1084804519300013)
Abstract: The high volumes of data are continuously generated from Internet of Things (IoT) sensors in an industrial landscape. Especially, the data-intensive workflows from IoT systems require to be processed in a real-time, reliable and low-cost way. Edge computing can provide a low-latency and cost-effective computing paradigm to deploy workflows. Therefore, data replication management and scheduling for delay-sensitive workflows in edge computing have become challenge research issues. In this work, first, we propose a replication management system which includes dynamic replication creator, a specialized cost-effective scheduler for data placement, a system watcher and some data security tools for collaborative edge and cloud computing systems. And then, considering task dependency, data reliability and sharing, the data scheduling for the workflows is modeled as an integer programming problem. And we present the faster meta-heuristic algorithm to solve it. The experimental results show that our algorithms can achieve much better system performance than comparative traditional strategies, and they can create a suitable number of data copies and search the higher quality replica placement solution while reducing the total data access costs under the deadline constraint.
Keywords: Replica creation; Data scheduling; Replication management; Edge computing

Ibnu Febry Kurniawan, A. Taufiq Asyhari, Fei He, Ye Liu,
Mobile computing and communications-driven fog-assisted disaster evacuation techniques for context-aware guidance support: A survey,
Computer Communications,
Volume 179,
2021,
Pages 195-216,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.07.020.
(https://www.sciencedirect.com/science/article/pii/S0140366421002802)
Abstract: The importance of an optimal solution for disaster evacuation has recently raised attention from researchers across multiple disciplines. This is not only a serious, but also a challenging task due to the complexities of the evacuees’ behaviors, route planning, and demanding coordination services. Although existing studies have addressed these challenges to some extent, mass evacuation in natural disasters tends to be difficult to predict and manage due to the limitation of the underlying models to capture realistic situations. It is therefore desirable to have on-demand mechanisms of locally-driven computing and data exchange services in order to enable near real-time capture of the disaster area during the evacuation. For this purpose, this paper comprehensively surveys recent advances in information and communication technology-enabled disaster evacuations, with the focus on fog computation and communication services to support a massive evacuation process. A numerous variety of tools and techniques are encapsulated within a coordinated on-demand strategy of an evacuation platform, which is aimed to provide a situational awareness and response. Herein fog services appear to be one of the viable options for responsive mass evacuation because they enable low latency data processing and dissemination. They can additionally provide data analytics support for autonomous learning for both the short-term guidance supports and long-term usages. This work extends the existing data-oriented framework by outlining comprehensive functionalities and providing seamless integration. We review the principles, challenges, and future direction of the state-of-the-art strategies proposed to sit within each functionality. Taken together, this survey highlights the importance of adaptive coordination and reconfiguration within the fog services to facilitate responsive mass evacuations as well as open up new research challenges associated with analytics-embedding network and computation, which is critical for any disaster recovery activities.
Keywords: Disaster recovery; Evacuation guidance; Fog; Fog computing; Fog communications; Collaborative analytics

Christophe Cérin, Keiji Kimura, Mamadou Sow,
Data stream clustering for low-cost machines,
Journal of Parallel and Distributed Computing,
Volume 166,
2022,
Pages 57-70,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.04.009.
(https://www.sciencedirect.com/science/article/pii/S0743731522000831)
Abstract: Nowadays, the operations performed by the Internet of Things (IoT) systems are no more trivial since they rely on more sophisticated devices than in the past. The IoT system is physically composed of connected computing, digital, mechanical devices such as sensors or actuators. Most of the time, each of them incorporates a logical arithmetic unit that can pre-compute or compute on the device. To extract value from the data produced at the edge, processing power offered by cloud computing is still utilized. However, streaming data to the cloud exposes some limitations related to the increased communication and data transfer, which introduces delays and consumes network bandwidth. Clustering data is one example of a treatment that can be executed in the cloud. In this paper, we propose a methodology for solving the data stream clustering problem at the edge. Data Stream clustering is defined as the clustering of data that arrive continuously, such as telephone records, multimedia data, sensors data, financial transactions, etc. Since we use low-cost and low-capacity devices, the objective is, given a sequence of points, to construct a good clustering of the stream using a small amount of memory and time. We propose a ‘windowing’ scheme, coupled with a sampling scheme to respect the objective. Under the experimental conditions, experiments show that the clustering solutions can be controlled, with difficulties for time-stamped data but not for random data or data with well-delimited clusters. The main advantage of our schema is that we are clustering data “on the fly” with no knowledge or assumption regarding the available data. We do not assume that all the data are known before a treatment batch by batch. Our schema also has the potential to be adapted to other classes of machine learning algorithms.
Keywords: Edge AI; Machine-learning algorithms; Online data stream clustering; Experiments on heterogeneous and low cost hardware

Xin Chen, Xu Liu, Ying Chen, Libo Jiao, Geyong Min,
Deep Q-Network based resource allocation for UAV-assisted Ultra-Dense Networks,
Computer Networks,
Volume 196,
2021,
108249,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108249.
(https://www.sciencedirect.com/science/article/pii/S138912862100284X)
Abstract: With the rapid development of the fifth-generation (5G) wireless communications, the number of users is increasing dramatically and Ultra-Dense Networks (UDN) are becoming more important for supporting numerous users and emerging mission-critical applications. In order to conquer the communication restrictions caused by natural disasters, an emergency communication system using Unmanned Aerial Vehicles (UAV) as a flying base station (BS) to assist UDN is proposed. By virtue of the resource allocation scheme of UAV-assisted UDN systems, communication resources can be reasonably and effectively allocated to improve the quality of user experience. Firstly, aiming to maximize the system energy efficiency (EE), a UDN system model including the BS selection is constructed. Secondly, Markov Decision Process (MDP) theory is applied to transform the system model into a stochastic optimization problem. Finally, by using deep reinforcement learning (DRL) technique, we propose a Deep Q-Network (DQN) based resource allocation scheme to maximize the system energy efficiency. Simulation results exhibit that the proposed DQN-based resource allocation scheme can significantly improve the system EE compared with the legacy Q-Learning, random and maximum resource allocation algorithms.
Keywords: Ultra-Dense Networks (UDN); Unmanned Aerial Vehicles (UAV); Resource allocation; Markov Decision Process (MDP); Deep Q-Network (DQN)

Ying Xing, Hui Shu, Fei Kang,
PeerRemove: An adaptive node removal strategy for P2P botnet based on deep reinforcement learning,
Computers & Security,
Volume 128,
2023,
103129,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103129.
(https://www.sciencedirect.com/science/article/pii/S0167404823000391)
Abstract: Botnets have become one of the major intrusion threats to cybersecurity. P2P botnets have high concealment and resilience because of their distributed structure, which are difficult to be completely dismantled and destroyed. Existing methods based on traffic statistics and vulnerability tracing cannot effectively solve the problem of P2P botnet disintegration. Although P2P networks emphasize that each node is peer-to-peer, the difference in processing power, resource distribution, and node bandwidth can lead to a certain heterogeneity. The critical nodes bridge the underlying bot nodes and the upper control server. Traditional methods for ranking the importance of nodes mainly relies on classical graph-theoretic feature statistics method, such as degree, betweenness, clustering coefficient, feature vector centrality, PageRank, etc. In this paper, botnet defense strategies are investigated from the perspective of complex network graph theory, and graph embedding and deep reinforcement learning combination optimization methods are adopted to handle the critical nodes identification problem of P2P botnets. Then, a novel adaptive node removal model called PeerRemove is proposed. The model uses Structure2vec graph embedding to characterize the network structure information as a low-dimensional embedding space, and it uses n-step Q-learning to train the model to learn complex topological patterns to find the critical nodes that effectively disintegrate the network. To evaluate the effectiveness of the proposed method, the Area Under the Curve (AUC) of the Largest Connected Component (LCC) size during node removal is used as an evaluation indicator, and six different types real or synthetic P2P botnets are selected, namely Sality, ZeroAccess, NSIS, Mozi, Gnutella, and Peer sampling service. Experiments are conducted on many real and model networks with node sizes reaching thousands and tens of thousands, and our method is compared with five classical static or dynamic node attack methods of HAD, PageRank, CI, BPD, and HPRA. The experimental results show that the overall AUC curve of the PeerRemove method is lower than that of the benchmark method, which can minimize botnet resiliency at a small cost. The proposed method is superior to the existing node removal methods and shows good robustness and feasibility. To demonstrate the generality of this method, it is tested on a centralized topological dataset and good experimental results are obtained.
Keywords: Botnet; Critical node; Network dismantling; Deep reinforcement learning; Adaptive

Jine Tang, Xinming Lu, Yong Xiang, Chaochen Shi, Junhua Gu,
Blockchain search engine: Its current research status and future prospect in Internet of Things network,
Future Generation Computer Systems,
Volume 138,
2023,
Pages 120-141,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.08.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002692)
Abstract: Blockchain has recently triggered extensive attention and concern due to the flourish of cryptocurrencies and decentralized applications. It can be considered as a database, and is utilized to store a large collection of time ordered data records in data intensive application scenarios. In such cases, search engines are put forward to access the data in a blockchain storage in order to perform analysis for different purposes. However, the research on blockchain search engine is still in its initial phase and there is no systematic study about this research area. This paper tries to analyze blockchain search engines from two aspects: its current research status and future prospect in Internet of Things (IoT) domain. In the research status aspect, the paper mainly focuses on the technique division and performance comparison of blockchain search engines from different angles. To elaborate the future prospect of blockchain search engine in IoT domain, the present search works related to IoT domains are also introduced. In the remaining parts of the paper, we outline the blockchain background. Then, the search fundamentals are introduced, including the classification principles and search requirements. After that, we present a detailed review on the state-of-the-art works for blockchain search and give some problem analysis on blockchain search in IoT domains. Finally, we outline the challenges of blockchain search and give the prospect of possible research directions in future in IoT domains.
Keywords: Blockchain search engine; Search fundamentals; Search requirements

Anish Jindal, Gagangeet Singh Aujla, Neeraj Kumar,
SURVIVOR: A blockchain based edge-as-a-service framework for secure energy trading in SDN-enabled vehicle-to-grid environment,
Computer Networks,
Volume 153,
2019,
Pages 36-48,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.02.002.
(https://www.sciencedirect.com/science/article/pii/S138912861831106X)
Abstract: Electric vehicles (EVs) have transformed the smart transportation sector by providing diverse energy management solutions to the smart grid. Energy trading among EVs and charging stations (CS) in a vehicle-to-grid (V2G) environment is one of the popular verticals in smart grid. However, processing the energy trading decisions at remote control centers lead to an increase in delay and network overhead. Apart from these issues, the security concerns while trading the energy in such an environment remain persistent. Therefore, to handle the aforementioned issues, this paper presents SURVIVOR: A Blockchain based Edge-as-a-Service Framework for Secure Energy Trading in software defined networking (SDN)-enabled V2G Environment. In the proposed framework, the energy trading decisions are processed closer to the location of EVs through edge nodes. Moreover, for securing the energy trading transactions, blockchain is used wherein the approver nodes are selected amongst all the present nodes on the basis of a utility function and are made responsible for validating the transactions. Once such nodes are selected, a consensus-based blockchain mechanism for secure energy trading in SDN-enabled V2G environment is presented. In this mechanism, edge nodes are responsible for generating proof-of-work puzzles. The proof-of-work is a unique hash value which is computed for each EV and the transactions for which the approver nodes compute the same proof-of-work for each EV are added in the blockchain. The complete scheme is backed by the SDN architecture to reduce the overall latency and increase the throughput of the smart transportation network. The results obtained prove that the proposed scheme is effective for trading the energy between EVs and CS while securing the underlying trading transactions using blockchain. Moreover, the communication and computation cost of the proposed scheme comes out to be small which proves that it can be used in real-world applications. The latency in the complete transportation sector is also greatly reduced by using the SDN-architecture.
Keywords: Blockchain; Edge-as-a-service; Energy trading; Smart city; Software defined networking

Ihsan Mert Ozcelik, Cem Ersoy,
ALVS: Adaptive Live Video Streaming using deep reinforcement learning,
Journal of Network and Computer Applications,
Volume 205,
2022,
103451,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103451.
(https://www.sciencedirect.com/science/article/pii/S1084804522001035)
Abstract: Achieving a high Quality of Experience (QoE) in live event streaming is a challenging problem given a low-latency requirement and time-varying network conditions. Adaptive video bitrate and adaptive playback speed techniques are two separate control knobs to address this challenge. In this paper, we consider these two control parameters in a joint optimization problem and present a deep reinforcement learning (DRL) framework to maximize QoE for live streaming without any assumption about the environment or fixed rule-based heuristics. With the proposed DRL framework, our approach (ALVS) constructs the inference model to make a joint decision of adaptive playback speed and video quality level for the next video segment. Simulation results through real network traces show that ALVS outperforms both state-of-the-art DRL-based and rule-based algorithms in terms of QoE without sacrificing live latency and skipping any content.
Keywords: Adaptive playback speed; Deep reinforcement learning; Live streaming media and video quality

Feng Yu, Hui Lin, Xiaoding Wang, Abdussalam Yassine, M. Shamim Hossain,
Blockchain-empowered secure federated learning system: Architecture and applications,
Computer Communications,
Volume 196,
2022,
Pages 55-65,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.09.008.
(https://www.sciencedirect.com/science/article/pii/S0140366422003474)
Abstract: Federated learning (FL) is a promising paradigm to realize distributed machine learning on heterogeneous clients without exposing their private data. However, there is the risk of single point failure with FL because it relies on a central server to gather the model updates from clients, moreover, malicious behaviors of some clients may lead to low-quality or even poisoned global models. Blockchain as a revolutionary distributed ledger technology can alleviate the above problems to significantly enhance the security and scalability of FL systems. Therefore, this article presents a general framework of Blockchain-based Federated Learning (BFL) system with detailed description of its key technologies and operation steps. We then review and compare the most recent representative BFL applications. And we outlook some key challenges and opportunities of the future BFL system in terms of security, cost, and scalability. Finally, we propose PoS-BFL in IoT scenarios with malicious devices. The validator voting mechanism and role switching mechanism in PoS-BFL ensure the stakes of legitimate nodes, and effectively reduce the impact of malicious nodes on the accuracy of the system model. And the experiments are conducted to demonstrate that PoS-BFL can achieve 86% accuracy, which is much higher than vanilla FL and pFedMe, and PoS-BFL is robust to some extent by adjusting the ratio of workers, validators and miners.
Keywords: Blockchain; Federated learning; Deep learning; Internet of Things; Intelligent transportation

Hafiz Farooq Ahmad, Wajid Rafique, Raihan Ur Rasool, Abdulaziz Alhumam, Zahid Anwar, Junaid Qadir,
Leveraging 6G, extended reality, and IoT big data analytics for healthcare: A review,
Computer Science Review,
Volume 48,
2023,
100558,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100558.
(https://www.sciencedirect.com/science/article/pii/S1574013723000254)
Abstract: In recent years, the healthcare industry has faced new challenges around staffing, human interaction, and the adoption of telehealth. Technological innovations can improve efficiency, productivity, and patient outcomes, but healthcare has been slow to adopt them. However, the promise of 6G communication, extended reality (XR), and the Internet of Things (IoT) big data analytics may revolutionize healthcare policies. Next-generation healthcare systems can utilize these technologies to offer novel healthcare services such as telepresence, holographic, and haptic communication. XR can provide immersive experiences that are revolutionizing many domains of the healthcare ecosystem, from self-care to surgical procedures. IoT can connect miniature healthcare objects with the Internet to provide smart healthcare services, generating big data that can be analyzed using deep learning to improve the quality of healthcare services, disease diagnosis, and treatment. There is a lack of reviews that focus on analyzing the latest perspectives and future trends on how a convergence of these technologies will influence future healthcare systems. This research paper aims to fill that gap by providing a detailed review of how the convergence of these technologies will influence the future of healthcare systems. It provides a detailed literature review of how healthcare systems have utilized technologies based on 6G, XR and IoT big data analytics for effective and scalable healthcare services provisioning. It also highlights how future healthcare systems can synergistically leverage 6G, XR, and IoT data analytics and provide insightful taxonomies based on different parameters along with a description of current challenges and future directions.
Keywords: 6G; Metaverse; IoT; Healthcare; Big data analytics; Extended reality

Weiwei Jiang,
Graph-based deep learning for communication networks: A survey,
Computer Communications,
Volume 185,
2022,
Pages 40-54,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.12.015.
(https://www.sciencedirect.com/science/article/pii/S0140366421004874)
Abstract: Communication networks are important infrastructures in contemporary society. There are still many challenges that are not fully solved and new solutions are proposed continuously in this active research area. In recent years, to model the network topology, graph-based deep learning has achieved the state-of-the-art performance in a series of problems in communication networks. In this survey, we review the rapidly growing body of research using different graph-based deep learning models, e.g. graph convolutional and graph attention networks, in various problems from different types of communication networks, e.g. wireless networks, wired networks, and software defined networks. We also present a well-organized list of the problem and solution for each study and identify future research directions. To the best of our knowledge, this paper is the first survey that focuses on the application of graph-based deep learning methods in communication networks involving both wired and wireless scenarios. To track the follow-up research, a public GitHub repository is created, where the relevant papers will be updated continuously.
Keywords: Graph; Deep learning; Graph Neural Network; Communication network; Software Defined Networking

Raktim Deb, Sudipta Roy,
A comprehensive survey of vulnerability and information security in SDN,
Computer Networks,
Volume 206,
2022,
108802,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108802.
(https://www.sciencedirect.com/science/article/pii/S1389128622000299)
Abstract: SDN changes the networking vision with an impressive thought of segregating the networking control from the data management hardware and brings new functionalities such as programmability, elasticity, flexibility, and adoption capability in the network, which are difficult to think of in traditional rigid network architecture. However, a wide range of vulnerable surfaces directly or indirectly affect the SDN-based system’s information security and launch various attacks. The paper begins with a glimpse of the advantages of SDN over the traditional network but, the findings of the research work take off the wraps regarding vulnerabilities and their consequences on information security. Consequently, the threat surfaces are exposed that exist in SDN architecture due to weak information security. In addition, the research findings also disclose other prominent issues irrespective of information security issues. The inclusion intends to ring the bell in the maximum SDN aspects and make researchers or professionals aware of current trends of SDN in the best possible way. The comprehensiveness of this work is retained by detailing every part of SDN, which helps the researchers or professionals to improve SDN structurally or functionally.
Keywords: Software defined network; Component vulnerability; SDN information security and threats

Jia Wu,
A fast-iterative reconstruction algorithm for sparse angle CT based on compressed sensing,
Future Generation Computer Systems,
Volume 126,
2022,
Pages 289-294,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.08.013.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003216)
Abstract: When conducting computed tomography (CT) image reconstruction, the existing algorithms do not analyze the physical and mathematical bases of CT image reconstruction, which leads to the lack of detailed information of reconstructed images and the problem of poor reconstruction effect. In this research, compressed sensing theory is applied in the process of fast-iterative CT reconstruction. The physical and mathematical basis for CT reconstruction is analyzed. A fast-iterative reconstruction algorithm for sparse angle CT images is proposed. Combined with algebraic iterative algorithm and gradient total variation minimization algorithm, CT reconstruction problem is transformed into gradient optimal solution problem, to complete the fast-iterative reconstruction of sparse angle CT images. Experimental results show that the proposed algorithm has high signal-to-noise ratio for CT image reconstruction, small reconstruction error, and good reconstruction effect for sparse angle CT images.
Keywords: Compressed sensing theory; Sparse angle; CT image reconstruction; Iterative reconstruction; Gradient total variation minimization algorithm

Naveed Islam, Yasir Faheem, Ikram Ud Din, Muhammad Talha, Mohsen Guizani, Mudassir Khalil,
A blockchain-based fog computing framework for activity recognition as an application to e-Healthcare services,
Future Generation Computer Systems,
Volume 100,
2019,
Pages 569-578,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.05.059.
(https://www.sciencedirect.com/science/article/pii/S0167739X19309860)
Abstract: In modern e-Healthcare systems, human activity recognition (HAR) is one of the most challenging tasks in remote monitoring of patients suffering from mental illness or disabilities for necessary assistance. One of the major issues is to provide security to a number of different connected devices to the Internet, known as Internet of Things (IoT). A potential solution to this problem is the blockchain-based architecture. In addition, the complex nature of activities performed by humans in diverse healthcare environments reduces the qualitative measures for extracting distinct features representing various human actions. To answer this challenge, we propose an activity monitoring and recognition framework, which is based on multi-class cooperative categorization procedure to improve the activity classification accuracy in videos supporting the fog or cloud computing-based blockchain architecture. In the proposed approach, frame-based salient features are extracted from videos consisting of different human activities, which are further processed into action vocabulary for efficiency and accuracy. Similarly, the classification of activities is performed using support vector machine (SVM) based on the error-correction-output-codes (ECOC) framework. It has been observed through experimental results that the proposed approach is more efficient and achieves higher accuracy regarding human activity recognition as compared to other state-of-the-art action recognition approaches.
Keywords: Human action recognition; e-Health; Fog computing; Cloud computing; Support vector machine; Error-correcting-output-code

Sree Krishna Das, Fatma Benkhelifa, Yao Sun, Hanaa Abumarshoud, Qammer H. Abbasi, Muhammad Ali Imran, Lina Mohjazi,
Comprehensive review on ML-based RIS-enhanced IoT systems: basics, research progress and future challenges,
Computer Networks,
Volume 224,
2023,
109581,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109581.
(https://www.sciencedirect.com/science/article/pii/S1389128623000269)
Abstract: Sixth generation (6G) internet of things (IoT) networks will modernize the applications and satisfy user demands through implementing smart and automated systems. Intelligence-based infrastructure, also called reconfigurable intelligent surfaces (RISs), have been introduced as a potential technology striving to improve system performance in terms of data rate, latency, reliability, availability, and connectivity. A huge amount of cost-effective passive components are included in RISs to interact with the impinging electromagnetic waves in a smart way. However, there are still some challenges in RIS system, such as finding the optimal configurations for a large number of RIS components. In this paper, we first provide a complete outline of the advancement of RISs along with machine learning (ML) algorithms and overview the working regulations as well as spectrum allocation in intelligent IoT systems. Also, we discuss the integration of different ML techniques in the context of RIS, including deep reinforcement learning (DRL), federated learning (FL), and FL-deep deterministic policy gradient (FL-DDPG) techniques which are utilized to design the radio propagation atmosphere without using pilot signals or channel state information (CSI). Additionally, in dynamic intelligent IoT networks, the application of existing integrated ML solutions to technical issues like user movement and random variations of wireless channels are surveyed. Finally, we present the main challenges and future directions in integrating RISs and other prominent methods to be applied in upcoming IoT networks.
Keywords: 6G; Reconfigurable intelligent surface; Machine learning; Deep learning; IoT; Resource management

Montida Pattaranantakul, Chalee Vorakulpipat, Takeshi Takahashi,
Service Function Chaining security survey: Addressing security challenges and threats,
Computer Networks,
Volume 221,
2023,
109484,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109484.
(https://www.sciencedirect.com/science/article/pii/S1389128622005187)
Abstract: Service function chaining (SFC) is a trending paradigm and it has attracted considerable attention from both the industry and academia because of its potential to improve dynamicity and flexibility in service chain provisioning significantly. SFC makes it easier and more convenient to compose on-demand service chains customized for application-specific requirements. In addition to SFC, network functions virtualization (NFV) and software-defined networking (SDN) are two other technology enablers that drive software-based service chain solutions. SFC leverages NFV for flexible deployment and for the placement of virtual resources and virtual network functions (VNFs); further, it employs SDN to provide traffic steering and network connectivity between the deployed VNF instances to form an application-specific service chain. Although SFC introduces many promising advantages, security is a major concern and a potential barrier for the widespread adoption of SFC technology. The integration of these technologies introduces a wide variety of security risks in the different levels of SFC stacks because SFC relies on NFV and SDN, and this results in a greater attack surface. Therefore, this survey aims to conduct a comprehensive analysis of SFC from a security perspective. To this end, we examine the SFC architecture in detail, including the design principles and relationships between other functional components, to obtain a clear understanding of SFC. The significant enhancements achieved by adopting SFC are highlighted. Further, we exemplify its deployment in several realistic use cases. Based on the SFC layering model, we analyze security threats to identify all possible risk exposures and establish a layer-specific threat taxonomy. We then systematically analyze the existing defensive solutions and propose a set of security recommendations to secure an SFC-enabled domain. Our goal is to help network operators deploy cost-effective security hardening based on their specific requirements. Finally, several open research challenges and future directions of SFC are also discussed.
Keywords: Service Function Chaining (SFC); Network Functions Virtualization (NFV); Software-Defined Networking (SDN); Traffic steering; Security threats; Vulnerabilities; Attack detection; Threat mitigation

Jens Sparsø, Hans Jakob Damsgaard, Dimitrios Katsamanis, Martin Schoeberl,
Comparing timed-division multiplexing and best-effort networks-on-chip,
Journal of Systems Architecture,
Volume 133,
2022,
102766,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102766.
(https://www.sciencedirect.com/science/article/pii/S138376212200251X)
Abstract: Best-effort (BE) networks-on-chips (NOCs) are usually preferred over time-division multiplexed (TDM) NOCs in multi-core platforms because they are work-conserving and have lower (zero-load) latency. On the other hand, BE NOCs are significantly more expensive to implement than TDM NOCs because of their virtual channel buffers, allocators/arbiters, and (credit-based) flow control; functionality that a TDM NOC avoids altogether. The objective of this paper is to compare the performance of BE and TDM NOCs, taking hardware cost into consideration. The networks are compared using graphs showing average latency as a function of offered load. For the BE NOCs, we use the BookSim simulator, and for the TDM NOCs, we derive a queuing theory model and an associated TDM NOC simulator. Through experiments with both router architectures, packet length, link width, and different traffic patterns, we show that for the same hardware cost, a TDM NOC can provide higher bandwidth and comparable latency. We also show that the packet length is the most important factor affecting the TDM period, which again is the primary factor affecting latency. The best TDM NOC design for BE traffic uses single flit packets, wide links/flits, and a router with two pipeline stages: link and router traversal.
Keywords: Multi-core/single-chip multiprocessors; On-chip interconnection networks; Time-division-multiplexing; Performance analysis; Queuing theory model

Amira Chriki, Haifa Touati, Hichem Snoussi, Farouk Kamoun,
FANET: Communication, mobility models and security issues,
Computer Networks,
Volume 163,
2019,
106877,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106877.
(https://www.sciencedirect.com/science/article/pii/S1389128618309034)
Abstract: In the last decades, technological progress in electronic and avionic systems, mainly device miniaturization and cost reduction, has boosted the performance of the UAV (Unmanned Ariel Vehicles). In addition to the military area, UAVs are nowadays very widespread in the field of civil application. Multiples UAVs system can cooperatively carry out missions more economically and efficiently compared to one UAV systems. Therefore, this choice lead to the development of new networking technologies between UAVs and ground control station. The UAVs network is known as FANET (Flying Ad-Hoc Network), and it is a subset of the MANET (Mobile Ad-Hoc Network). There are many problems to be addressed before effective use of FANET can be made in order to provide reliable and stable context specific networks. In this paper, a view of FANETs is presented from the networking communication challenges perspective. The scope of this survey is to give a comprehensive overview about the existing communications architectures proposed for the FANET networks. We expose the routing protocols, mobility and trajectory optimization models that have been used in FANET to solve communication and collaboration issues between UAVs, we outline the security challenges that need to be overcome and discuss FANET networking open issues. Our goal is to provide a general idea to the researchers about the different topics to be addressed in this area.
Keywords: UAV; FANET; Communication issues; Routing protocols; Mobility models; Security

Milad Mohseni, Ahmad Habibized Novin,
A survey on techniques for improving Phase Change Memory (PCM) lifetime,
Journal of Systems Architecture,
Volume 144,
2023,
103008,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.103008.
(https://www.sciencedirect.com/science/article/pii/S138376212300187X)
Abstract: PCMs are Non-Volatile Memories (NVMs) that store data using phase-change semiconductors, such as silicon-chalcogenide glass. In addition to increased integration density, PCMs have high durability and data transfer rates and consume less power during read/write operations. Compared with flash memory, PCM operates at much faster speeds and is close to the performance of DRAM. As a result, it is one of the most popular semiconductor memories which is used in various applications. However, PCM cells are limited to a certain number of write operations, and because of that, values cannot be changed when this limit is reached. Hence, PCMs do not last for a long time. The limited lifespan of PCM memory has led to considerable research in recent years. Several architectural levels and methods have been offered and examined to overcome this challenge. In this study, functional and practical techniques are identified and analyzed to extend the longevity of PCMs. These schemes can also be used to prolong the life of other NVM technologies since they share similar characteristics with PCMs. Furthermore, we will present some improvements to PCM performance to make it competitive with common NVMs. In conclusion, this survey will be helpful to both researchers who are considering starting PCM projects and those already proficient in PCM.
Keywords: Hardware; Memory architecture; NVM; Write Latency; PCM; DRAM

Quanwei Zhang, Qingjun Xiao, Yuexiao Cai,
A generic sketch for estimating super-spreaders and per-flow cardinality distribution in high-speed data streams,
Computer Networks,
Volume 237,
2023,
110059,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110059.
(https://www.sciencedirect.com/science/article/pii/S1389128623005042)
Abstract: For a high-speed network, it is an important task to process the IP packet stream using limited memory and measure its statistical metrics of interest. While many algorithms have been proposed to estimate the cardinality of a single data stream (i.e., the number of distinct elements), it remains a great challenge when a stream contains numerous sub-streams, called flows. In this paper, we focus on a problem of designing a generic data structure to measure multiple types of per-flow statistics in a high-speed stream, including per-flow cardinality, top-K super-spreading flows with the greatest cardinalities, per-flow cardinality moments and per-flow cardinality distribution. Previous solutions for generic measurement mainly focus on the frequency-related statistics measurement, while this paper makes a step forward to support deduplication, i.e., cardinality-related measuring. To address this new problem, we propose a generic sketch named M2D. The challenge is that the per-flow cardinality distribution is often highly skewed with a small proportion of super-spreaders. To tame the skewness, we adopt the adjustable progressive sampling technique, which samples subsets of flows by an exponentially decreasing probability according to their cardinalities. Based on the sampled super-spreaders, we estimate the moments of per-flow cardinalities with different orders. We finally apply the method of moments to reconstruct the per-flow cardinality distribution with no priori knowledge about its formula. Our experiments show M2D’s high memory efficiency (average savings of 38%) and satisfactory distribution estimation accuracy (2% to 98% improvement) than other algorithms.
Keywords: Data streams; Network measurements; Generic sketch; Cardinality estimation; Vertex degree distribution

Ali Akbar Sadri, Amir Masoud Rahmani, Morteza Saberikamarposhti, Mehdi Hosseinzadeh,
Fog data management: A vision, challenges, and future directions,
Journal of Network and Computer Applications,
Volume 174,
2021,
102882,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102882.
(https://www.sciencedirect.com/science/article/pii/S1084804520303465)
Abstract: Cloud computing with its key facets and its inherent advantages still faces several challenges in the Internet of Things (IoT) ecosystem. The distance among the IoT end devices and cloud computing might be a problem for latency-sensitive applications such as catastrophe management and content transference applications. Fog computing is a novel paradigm to address such issues that playacts a significant role in massive and real-time data management systems in an IoT environment. Particularly IoT data management by fog computing is one important phase for latency reduction in latency-sensitive applications and necessary to generate more skilled knowledge and intelligent decisions. In this study, we used the SLR (systematic literature review) method to survey fog data management to understand the various topics and main contexts in this domain that have been newly offered. The target of this article is classifying and analyzing the researches about the fog data management domain which has been released from 2014 to 2019. A context-based taxonomy is offered for fog data management including data processing, data storage and data security based on the context of papers that are elected with the SLR method in our study. Based on presented technical taxonomy, the grouped papers in any context are compared with each other pursuant to some metrics of fog data management reference model. Then, for any selected research, the new findings, advantages, and weaknesses are debated. Finally, based on studies the open issues in fog data management and their related challenges for future researches are highlighted.
Keywords: Fog computing; Internet of things; Data management; Data processing; Data analytics; Data storage; Data security; Systematic literature review

Issam W. Damaj, Hadi Al-Mubasher, Mahmoud Saadeh,
An extended analytical framework for heterogeneous implementations of light cryptographic algorithms,
Future Generation Computer Systems,
Volume 141,
2023,
Pages 154-172,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.11.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003673)
Abstract: The increased need for data, combined with the emergence of powerful Internet of Things (IoT) devices, has resulted in major security concerns. The decision-making related to choosing an adequate cryptographic algorithm to use is, indeed, an example concern that affects the performance of an implementation. Lightweight or tiny ciphers are considered to be the go-to algorithms when talking about embedded systems and IoT devices. Such ciphers, when properly integrated, are expected to have a minimal effect on the overall device utilization and thus provide effective performance. In this paper, we propose a unified analytical framework for lightweight ciphers as implemented within heterogeneous computing environments. This framework considers a carefully identified set of metrics that can adequately enable the capturing, ranking, and classifying the attained performance. To that end, a designer can make effective evaluations and exact adjustments to an implementation. This framework uses three decision-making approaches, namely the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS), Preference Ranking Organization Method for Enrichment Evaluation (PROMETHEE) II, and Fuzzy TOPSIS. Such approaches take into account both hardware and software metrics when deciding on a suitable cryptographic algorithm to adopt. Validation entails a thorough examination and evaluation of several performance classification schemes. The results confirm that the framework is both valid and effective.
Keywords: Cryptography; Algorithms; Heterogeneous computing; Decision making; Performance evaluation; Classification

Yasser Aldwyan, Richard O. Sinnott,
Latency-aware failover strategies for containerized web applications in distributed clouds,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 1081-1095,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.07.032.
(https://www.sciencedirect.com/science/article/pii/S0167739X19304224)
Abstract: Despite advances in Cloud computing, ensuring high availability (HA) remains a challenge due to varying loads and the potential for Cloud outages. Deploying applications in distributed Clouds can help overcome this challenge by geo-replicating applications across multiple Cloud data centers (DCs). However, this distributed deployment can be a performance bottleneck due to network latencies between users and DCs as well as inter-DC latencies incurred during the geo-replication process. For most web applications, both HA and Performance (HAP) are essential and need to meet pre-agreed Service Level Objectives (SLOs). Efficiently placing and managing primary and backup replicas of applications in distributed Clouds to achieve HAP is a challenging task. Existing solutions consider either HA or performance but not both. In this paper we propose an approach for automating the process of providing a latency-aware failover strategy through a server placement algorithm leveraging genetic algorithms that factor in the proximity of users and inter-DC latencies. To facilitate the distributed deployment of applications and avoid the overheads of Clouds, we utilize container technologies. To evaluate our proposed approach, we conduct experiments on the Australia-wide National eResearch Collaboration Tools and Resources (NeCTAR - www.nectar.org.au) Research Cloud. Our results show at least a 23.3% and 22.6% improvement in response times under normal and failover conditions respectively compared to traditional, latency-unaware approaches. Also, the 95th percentile of response times in our approach are at most1.5 ms above the SLO compared to 11–32 ms using other approaches.
Keywords: Distributed Clouds; High availability; Performance; Container technologies; Cloud outages; Web applications; Distributed deployment

Prabhakar Krishnan, Subhasri Duttagupta, Krishnashree Achuthan,
VARMAN: Multi-plane security framework for software defined networks,
Computer Communications,
Volume 148,
2019,
Pages 215-239,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.09.014.
(https://www.sciencedirect.com/science/article/pii/S0140366419308217)
Abstract: In the context of future networking technologies, Software-Defined paradigm offers compelling solutions and advantages for traffic orchestration and shaping, flexible and dynamic routing, programmable control and smart application-driven resource management. But the SDN operation has to confront critical issues and technical vulnerabilities, security problems and threats in the enabling technical architecture itself. To address the critical security problems in SDN enabled data centers, we propose a collaborative “Network Security and Intrusion Detection System(NIDS)” scheme called ‘VARMAN: adVanced multi-plAne secuRity fraMework for softwAre defined Networks’. The SDN security scheme comprises of coarse-grained flow monitoring algorithms on the dataplane for rapid anomaly detection and prediction of network-centric DDoS/botnet attacks. In addition, this is combined with a fine-grained hybrid deep-learning based classifier pipeline on the control plane. It is observed that existing ML-based classifiers improve the accuracy of NIDS, however, at the cost of higher processing power and memory requirement, thus unrealistic for real-time solutions. To address these problems and still achieve accuracy and speed, we designed a hybrid model, combining both deep and shallow learning techniques, that are implemented in an improved SDN stack. The data plane deploys attack prediction and behavioral trigger mechanisms, efficient data filtering, feature selection, and data reduction techniques. To demonstrate the practical feasibility of our security scheme in real modern datacenters, we utilized the popular NSL-KDD dataset, most recent CICIDS2017 dataset, and refined it to a balanced dataset containing a comparable number of normal traffic and malware samples. We further augmented the training by organically generating datasets from lab-simulated and public-network hosted hackathon websites. The results show that VARMAN framework is capable of detecting attacks in real-time with accuracy more than 98% under attack intensities up to 50k packets/second. In a multi-controller interconnected SDN domain, the flow setup time improves by 70% on an average, and controller response time reduces by 40%, without incurring additional latency due to security intelligence processing overhead in SDN stack. The comparisons of VARMAN under similar attack scenarios and test environment, with related recent works that utilized ML-based NIDS, demonstrate that our scheme offers higher accuracy, less than 5% false positive rate for various attack intensities and significant training space/time reduction.
Keywords: SDN; NFV; SDNFV; IoT; Cloud; Edge networks; DDoS; Botnet; Malware; Network security; Threat analytics; Security; IDS; IPS; NIDS; Machine learning; Deep learning; CICIDS2017

Loreto Pescosolido, Marco Conti, Andrea Passarella,
D2D data offloading in vehicular environments with optimal delivery time selection,
Computer Communications,
Volume 146,
2019,
Pages 63-84,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.07.016.
(https://www.sciencedirect.com/science/article/pii/S0140366418310181)
Abstract: Within the framework of a Device-to-Device (D2D) data offloading system for cellular networks, we propose a Content Delivery Management System (CDMS) in which the instant for transmitting a content to a requesting node, through a D2D communication, is selected to minimize the energy consumption required for the transmission. The proposed system is particularly fit to highly dynamic scenarios, such as vehicular ones, where the network topology changes at a rate which is comparable with the order of magnitude of the delay tolerance. We present an analytical framework able to predict the system performance, in terms of energy consumption, considering concurrent requests for contents with different popularity. We validate the analytical model through simulations, and provide a thorough performance evaluation of the proposed CDMS, in terms of energy consumption and spectrum use. The proposed CDMS can be configured in two ways: (i) either use I2D transmissions only as a last resort, if it is not possible to serve request through D2D offloading within the limit of the delay tolerance; or (ii) include the possibility for the Base Stations (BSs) to transmit contents even before the deadline, if this is found to be more energy efficient from the point of view of the overall energy consumption, renouncing a portion of the offloading opportunities. The proposed scheme is benchmarked against a plain classic cellular scheme, a cellular (I2D only) scheme with optimal Infrastructure-to-Device (I2D) transmission scheduling for delay-tolerant applications, and a D2D data offloading scheme (proposed in previous works) in which the D2D transmission is performed as soon as a close-by device with the required content is found (i.e., without optimizing the transmission instant). We implemented a quite detailed simulator complete of Radio Resource Management (RRM) over the MAC layer of a multi carrier system, in a multi-cell scenario. Our results show that the proposed CDMS can induce a reduction of an order of magnitude, i.e. 90% or higher, of the energy consumed by the devices to implement D2D offloading, with respect to the benchmark D2D offloading scheme. At the same time, we observed a reduction of the energy consumption of the overall system in the order of 30%, under conservative assumptions. Finally, the system induces a remarkable degree of spatial frequency reuse, thus reducing the overall spectrum occupation.
Keywords: D2D Data offloading; Delay-tolerant applications; Radio resource management; MEC

Ayaz Ali Khan, Muhammad Zakarya, Izaz Ur Rahman, Rahim Khan, Rajkumar Buyya,
HeporCloud: An energy and performance efficient resource orchestrator for hybrid heterogeneous cloud computing environments,
Journal of Network and Computer Applications,
Volume 173,
2021,
102869,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102869.
(https://www.sciencedirect.com/science/article/pii/S1084804520303349)
Abstract: In major Information Technology (IT) companies such as Google, Rackspace and Amazon Web Services (AWS), virtualisation and containerisation technologies are usually used to execute customers' workloads and applications. The computational resources are provided through large-scale datacenters, which consume substantial amount of energy and have, therefore, ecological impacts. Since long, Google runs users' applications in containers, Rackspace offers bare-metal hardware, whereas AWS runs them either in VMs (EC2), containers (ECS) and/or containers inside VMs (Lambda); therefore, making resource management a tedious activity. The role of a resource management system is of the greatest importance, principally, if IT companies practice various kinds of sand-boxing technologies, for instance, bare-metal, VMs, containers, and/or nested containers in their datacenters (hybrid platforms). The absence of centralised, workload-aware resource managers and consolidation policies produces questions on datacenters energy efficiency, workloads performance, and users' costs. In this paper, we demonstrate, through several experiments, using the Google workload data for 12,583 hosts and approximately one million tasks that belong to four different kinds of workload, the likelihood of: (i) using workload-aware resource managers in hybrid clouds; (ii) achieving energy and cost savings, in heterogeneous hybrid datacenters such that the workload performance is not affected, negatively; and (iii) how various allocation policies, combined with different migration approaches, will impact on datacenter's energy and performance efficiencies. Using plausible assumptions for hybrid datacenters set-up, our empirical evaluation suggests that, for no migration, a single scheduler is at most 16.86% more energy efficient than distributed schedulers. Moreover, when migrations are considered, our resource manager can save up to 45.61% energy and can improve up to 17.9% workload performance.
Keywords: Datacenters; Virtualisation; Containerisation; Resource management; Server consolidation; Workload migration; Energy efficiency; Performance

Madhusanka Liyanage, Quoc-Viet Pham, Kapal Dev, Sweta Bhattacharya, Praveen Kumar Reddy Maddikunta, Thippa Reddy Gadekallu, Gokul Yenduri,
A survey on Zero touch network and Service Management (ZSM) for 5G and beyond networks,
Journal of Network and Computer Applications,
Volume 203,
2022,
103362,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103362.
(https://www.sciencedirect.com/science/article/pii/S1084804522000297)
Abstract: Faced with the rapid increase in smart Internet-of-Things (IoT) devices and the high demand for new business-oriented services in the fifth-generation (5G) and beyond network, the management of mobile networks is getting complex. Thus, traditional Network Management and Orchestration (MANO) approaches cannot keep up with rapidly evolving application requirements. This challenge has motivated the adoption of the Zero-touch network and Service Management (ZSM) concept to adapt the automation into network services management. By automating network and service management, ZSM offers efficiency to control network resources and enhance network performance visibility. The ultimate target of the ZSM concept is to enable an autonomous network system capable of self-configuration, self-monitoring, self-healing, and self-optimization based on service-level policies and rules without human intervention. Thus, the paper focuses on conducting a comprehensive survey of E2E ZSM architecture and solutions for 5G and beyond networks. The article begins by presenting the fundamental ZSM architecture and its essential components and interfaces. Then, a comprehensive review of the state-of-the-art for key technical areas, i.e., ZSM automation, cross-domain E2E service lifecycle management, and security aspects, are presented. Furthermore, the paper contains a summary of recent standardization efforts and research projects towards the ZSM realization in 5G and beyond networks. Finally, several lessons learned from the literature and open research problems related to ZSM realization are also discussed in this paper.
Keywords: Zero-touch network and Service Management; Machine learning; Artificial intelligence; Security; 5G; 6G; Service management; Automation; Orchestration

Adarsh Kumar, Neelu Jyothi Ahuja, Monika Thapliyal, Sarthika Dutt, Tanesh Kumar, Diego Augusto De Jesus Pacheco, Charalambos Konstantinou, Kim-Kwang Raymond Choo,
Blockchain for unmanned underwater drones: Research issues, challenges, trends and future directions,
Journal of Network and Computer Applications,
Volume 215,
2023,
103649,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103649.
(https://www.sciencedirect.com/science/article/pii/S1084804523000681)
Abstract: Two-thirds of the earth's surface is surrounded by water and the majority of it is still unexplored. The underwater monitoring of the oceans and their surroundings is highly crucial from several perspectives, e.g., to unearth the hidden minerals/oils, to monitor the life of underwater species, military and rescue applications, surveillance of maritime borders, and predict tidal wave behaviors among others. The exploration and inspection activities of the underwater environment are mainly performed remotely by unmanned underwater vehicles or robots because human direct involvement in such a complex environment might not be a feasible option. Moreover, the recent maturity in digital automation and underwater drone technology along with the emergence and improvements in wireless, communication, and sensing technologies had already encouraged the research community to drive deep into this domain. Among several disruptive enabling technologies, Blockchain has emerged as a vital enabling technology to fulfil key requirements for secure data sharing, storage, process tracking, collaboration, and resource management. This study presents a comprehensive review of the utilization of Blockchain in different underwater applications, discussing various use cases along with detailing blockchain-based architectures, potential challenges, solutions, and future research directions. Potential challenges of underwater applications addressed by Blockchain have been detailed. This work identifies knowledge gaps between theoretical research and real-time Blockchain integration in realistic underwater drone applications. The key limitations for effective integration of Blockchain in real-time integration in Unmanned Underwater Drones (UUD) applications, along with directions for future research have been presented.
Keywords: Blockchain; Data management; Drone design; Privacy; Security; Unmanned underwater drones

Xiying Fan, Baolin Liu, Chuanhe Huang, Shaojie Wen, Bin Fu,
Utility maximization data scheduling in drone-assisted vehicular networks,
Computer Communications,
Volume 175,
2021,
Pages 68-81,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.04.033.
(https://www.sciencedirect.com/science/article/pii/S0140366421001778)
Abstract: In Vehicular Networks (VANETs), the rapid movement of vehicles leads to highly time-varying network topology and brings the problem of blind spots in signal coverage. To address these issues, drones are employed to assist data dissemination in VANETs attribute to the flexible development of drones. As random data transmission will result in decreased network performance and low efficiency in data retrieval service, it is urgent to develop efficient data scheduling schemes that meet the quality of service (QoS) of different applications in VANETs. In this context, to fulfill the request of real-time and reliable data transmission, we formulate a data scheduling problem that considers the factors such as priority of data transmission, link quality, link connection time and network fairness. Our goal is to reduce random data transmission, therefore maximizing network transmission utility. We utilize graph theory to describe network topology, in which vehicles and drones are represented as vertices and links between the nodes are represented as edges. The weight of edges indicates the utility obtained when data transmits between the corresponding nodes. We reduce the data scheduling problem to the maximum weighted matching problem and then propose a data scheduling scheme that can satisfy data requests of different vehicles to the maximum extent. The theoretical analysis derives the scheduling algorithm’s time complexity and the number of scheduling stages required to satisfy the data requests. Finally, simulation verifies the proposed scheme’s effectiveness in terms of service rate, service delay, fairness and throughput.
Keywords: VANETs; Drones; Data scheduling; Utility maximization; Maximum weighted matching

Meng Chen, Jiaxin Hou, Yongpan Sheng, Yingbo Wu, Sen Wang, Jianyuan Lu, Qilin Fan,
HA-D3QN: Embedding virtual private cloud in cloud data centers with heuristic assisted deep reinforcement learning,
Future Generation Computer Systems,
Volume 148,
2023,
Pages 1-14,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.05.025.
(https://www.sciencedirect.com/science/article/pii/S0167739X2300208X)
Abstract: The virtual network embedding problem is embedding virtual networks (VNs) in a substrate network so that revenue or accept ratio is maximized. Previous study usually assumes disclosed communication demand among the virtual nodes in a VN, mismatching real-world cloud computing scenarios. In this paper, we propose a new VN abstraction based on the widely used Virtual Private Cloud model, where internal communication demand is unknown to cloud providers. In contrast with the majority of existing research, we allow the co-location of the virtual nodes belonging to the same VN, and introduce the concept of switching capacity for practical resource reservation. We categorize the substrate resources in cloud data centers into additive and non-additive for the first time, and devise our algorithms accordingly. After formulating the problem, we propose a solution framework named HA-D3QN (Heuristic Assisted Dueling Double Deep Q Network). Essentially, HA-D3QN selects the best responses to different system states by combining the D3QN deep reinforcement learning structure and the candidate actions, which are generated by our proposed heuristic algorithms for addressing the exponentially large action space. Finally, we conduct extensive simulation experiments, the results of which verify the effectiveness of our approach.
Keywords: Virtual network embedding; Virtual private cloud; Deep reinforcement learning; Cloud data center; Switching capacity; Guaranteed bandwidth

Qiang Liu, XiaoShe Dong, Heng Chen, Xingjun Zhang,
H2Pregel : A partition-based hybrid hierarchical graph computation approach,
Future Generation Computer Systems,
Volume 104,
2020,
Pages 15-31,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.021.
(https://www.sciencedirect.com/science/article/pii/S0167739X18306010)
Abstract: A partition-based hybrid hierarchical graph computation approach, called H2Pregel is proposed to address the redundant supersteps and inefficient computation problems due to low access locality. The H2Pregel preprocesses the input graph through a distributed recode algorithm to ensure the continuity and sequence of vertex ids, then employs a hybrid approach to combine the advantages of both synchronous and asynchronous models, and hierarchically computes the high proportion of interior messages generated by high quality partition algorithms. Moreover, H2Pregel leverages configurable parallel threads to accelerate local computation by “sub-supersteps”, and employs an exterior messages stealing optimization to avoid extra communication overheads between tasks. We implemented H2Pregel on Giraph, a classic open source system based on Pregel. The evaluation results on large-scale graphs show that, compared with Pregel in three partition algorithms, H2Pregel can achieve average speedups by 1.12–4.52 times and decrease average communication messages by 23.5%-55.5%, and average supersteps by 15.8%-82.0%.
Keywords: Graph computing; Hybrid computation; BSP model; Hierarchical

Masoume Jabbarifar, Alireza Shameli-Sendi, Bettina Kemme,
A scalable network-aware framework for cloud monitoring orchestration,
Journal of Network and Computer Applications,
Volume 133,
2019,
Pages 1-14,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.02.006.
(https://www.sciencedirect.com/science/article/pii/S1084804519300505)
Abstract: Monitoring components are implanted in clouds to evaluate performance, detect the failures, and assess component interactions by message analysis. In this paper, we propose Monitoring as a Service (MaaS) installed across its software defined network in the cloud. All switches in datacenter only forward traffic. Specific SDN application on top of the network controller has been implemented in order to orchestrate multiple network tenants monitoring needs. Applications declare the required observation for traffics and their specific monitoring needs to the Maas. Therefore, a set of virtual monitoring functions (vMF) are prescribed to be placed in datacenter for flows. An optimal placement algorithm places vMF with respect to the network and computing utilization maximization objectives. Network objective refers to minimize traffic delay for flows needed to be monitored and computing objective expresses balancing nodes computing resources. The optimal placement of virtual network functions is known to be an NP-hard problem. Compared to the existing work, we discovered different problem which how a vMF can be split into smaller pieces to decrease the total placement cost for a set of flows required, based on four patterns: free, parser-collocation, job-collocation, and full-collocation. Moreover, we proposed three heuristics to make our placement algorithm scalable for a large network, called, chain partitioning, topology partitioning, and zoning. We show the feasibility of our approach in a large dataset consists of 540k nodes and 11.5M edges, with about 40 requests (flows) at the same time. Furthermore, the proposed solution saves at least 20 percent of total monitoring cost, in average, compared to the latest related work.
Keywords: Cloud computing; SDN; NFV; Service chaining; Monitoring functions; Optimal placement

Gonçalo Marques, Carlos Senna, Susana Sargento, Luís Carvalho, Luís Pereira, Ricardo Matos,
Proactive resource management for cloud of services environments,
Future Generation Computer Systems,
Volume 150,
2024,
Pages 90-102,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.08.005.
(https://www.sciencedirect.com/science/article/pii/S0167739X23003059)
Abstract: Microservices offer advantages such as better fault isolation, smaller and faster deployments, scalability, and speeding up the development of new applications through the composition of services. However, its large-scale use and specific requirements increase the challenges of monitoring and management. To meet these challenges, we propose a monitoring and management system for microservices, containers and container clusters that autonomously predicts load variations and resource scarcity, which is capable of making new resources available in order to ensure the continuity of the process without interruptions. Our solution’s architecture allows for customizable service-specific metrics that are used by the load predictor to anticipate resource consumption peaks and proactively allocate them. In addition, our management system, by identifying/predicting low demand, frees up resources making the system more efficient. We evaluated our service management solution in the AWS environment, environment characterized by high mobility, dynamic topologies caused by disconnection, dropped packets and delay issues. Our results show that our solution improves the efficiency of escalation policies, and reduces response time by improving the QoS/QoE of the system.
Keywords: Cloud service management; Cloud monitoring; Service monitoring; Proactive management; Autonomic management; Machine learning

Wafa Ben Jaballah, Mauro Conti, Chhagan Lal,
Security and design requirements for software-defined VANETs,
Computer Networks,
Volume 169,
2020,
107099,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107099.
(https://www.sciencedirect.com/science/article/pii/S1389128619306553)
Abstract: The evolving of Fifth Generation (5G) networks is becoming more readily available as a significant driver of the growth of new applications and business models. Vehicular Ad hoc Networks (VANETs) and Software Defined Networking (SDN) represent the critical enablers of 5G technology with the development of next-generation intelligent vehicular networks and applications. In recent years, researchers have focused on the integration of SDN and VANET, and looked at different topics related to the architecture, the benefits of software-defined VANET services, and the new functionalities to adapt them. However, the security and robustness of the complete architecture is still questionable and have been largely neglected by the research community. Moreover, the deployment and integration of different entities and several architectural components drive new security threats and vulnerabilities. In this paper, first, we survey the state-of-the-art SDN based Vehicular ad-hoc Network (SDVN) architectures for their networking infrastructure design, functionalities, benefits, and challenges. Then we discuss these architectures against major security threats that violate the key security services such as availability, privacy, authentication, and data integrity. We also discuss different countermeasures for these threats. Finally, we present the lessons learned with the directions of future research work towards provisioning stringent security solutions in new SDVN architectures. To the best of our knowledge, this is the first work that presents a comprehensive survey and security analysis on SDVN architectures, and we believe that it will help researchers to address various challenges (e.g., flexible network management, control and high resource utilization, and scalability) in vehicular communication systems which are required to improve the future Intelligent Transportation Systems (ITS).
Keywords: Security; VANETs; Software defined networking; 5G; Networking attacks; Wireless channels; Edge/Fog computing

Frederik Hauser, Marco Häberle, Daniel Merling, Steffen Lindner, Vladimir Gurevich, Florian Zeiger, Reinhard Frank, Michael Menth,
A survey on data plane programming with P4: Fundamentals, advances, and applied research,
Journal of Network and Computer Applications,
Volume 212,
2023,
103561,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103561.
(https://www.sciencedirect.com/science/article/pii/S1084804522002028)
Abstract: Programmable data planes allow users to define their own data plane algorithms for network devices including appropriate data plane application programming interfaces (APIs) which may be leveraged by user-defined software-defined networking (SDN) control. This offers great flexibility for network customization, be it for specialized, commercial appliances, e.g., in 5G or data center networks, or for rapid prototyping in industrial and academic research. Programming protocol-independent packet processors (P4) has emerged as the currently most widespread abstraction, programming language, and concept for data plane programming. It is developed and standardized by an open community, and it is supported by various software and hardware platforms. In the first part of this paper we give a tutorial of data plane programming models, the P4 programming language, architectures, compilers, targets, and data plane APIs. We also consider research efforts to advance P4 technology. In the second part, we categorize a large body of literature of P4-based applied research into different research domains, summarize the contributions of these papers, and extract prototypes, target platforms, and source code availability. For each research domain, we analyze how the reviewed works benefit from P4’s core features. Finally, we discuss potential next steps based on our findings.
Keywords: P4; SDN; Programmable data planes

Cenk Gündoğan, Jakob Pfender, Peter Kietzmann, Thomas C. Schmidt, Matthias Wählisch,
On the impact of QoS management in an Information-centric Internet of Things,
Computer Communications,
Volume 154,
2020,
Pages 160-172,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.02.046.
(https://www.sciencedirect.com/science/article/pii/S0140366419315579)
Abstract: The Internet of Things (IoT) comprises a relevant class of applications that require Quality of Service (QoS) assurances. Information Centric Networking (ICN) has shown promising characteristics in constrained wireless networks, but differentiated QoS has not yet fully emerged. In this paper, we design and analyze a QoS scheme that manages the NDN resources forwarding and queuing priorities, as well as the utilization of caches and of forwarding state space. In constrained wireless networks, these resources are scarce with a potentially high impact due to lossy radio transmission. We explore the two basic service qualities (i) prompt and (ii) reliable traffic forwarding. We treat QoS resources not only in isolation, but correlate their use on local nodes and between network members. Network-wide coordination is based on simple QoS code points that can be distributed via a routing protocol. Fairness measures that prevent resource starvation are part of this management scheme. Our findings indicate that our coordinated QoS management in ICN does not only effectively prioritize the privileged data chunks, but also improves regular data communication. We can show that appropriate QoS coordination can enhance the overall network performance by more than the sum of its parts and that it exceeds the impact QoS can have in the IP world.
Keywords: Information-Centric Networking; Quality of Service; Network performance measurement; Wireless sensor network

Laaziz Lahlou, Nadjia Kara, Claes Edstrom,
DAVINCI: online and Dynamic Adaptation of eVolvable vIrtual Network services over Cloud Infrastructures,
Future Generation Computer Systems,
Volume 127,
2022,
Pages 396-408,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.09.017.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003630)
Abstract: Service function chains, or more generally, virtual network services, evolve throughout their life-cycle due to traffic fluctuations, resource usage, and the stringent requirements of the SLO (Service Level Objective) that must be fulfilled. Network operators resort to elasticity mechanisms (e.g., vertical and horizontal scaling) and migration procedures to adapt the service function chains dynamically to meet their requirements constantly. However, most state-of-the-art solutions do not evaluate the overall impact of such mechanisms on the service function chains or the resulting penalty costs (e.g., migration downtime and SLO violation). To bridge this gap, we propose DAVINCI, a decision-making tool with different adaptation policies that allows the network operator to adapt the service function chains to minimize network changes (e.g., reallocating paths and VNFs) while keeping penalty costs at their lowest level. Moreover, DAVINCI allows for the adaptations (e.g., migration, vertical and horizontal scaling) to be expressed as a set of decisions and leverages the Min Cost Flow problem to estimate migration downtime. The analytical evaluation shows that DAVINCI outperforms the existing state-of-the-art solution (NFV-PEAR) in terms of migration, traffic costs, and the overhead induced by the migration and elasticity mechanisms while significantly reducing penalty costs.
Keywords: Network function virtualization; Mathematical optimization; Orchestration; Service function placement and chaining; Dynamic adaptation

Guilherme Araujo, Maycon Peixoto, Leobino Sampaio,
A comprehensive and configurable simulation environment for supporting vehicular named-data networking applications,
Computer Networks,
Volume 235,
2023,
109949,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109949.
(https://www.sciencedirect.com/science/article/pii/S1389128623003948)
Abstract: The Named Data Networking (NDN) architecture, with its network-layer features, services, and properties, is well-suited for vehicular applications and inter-vehicle communication (IVC). In contrast, IP-based host-centric architectures struggle with challenges inherent to the vehicular ad-hoc network (VANET) context, such as node mobility, data security, efficient data forwarding, and routing. Named Data Networking takes a fundamental departure from today’s IP-based architectures for VANET/IVC and thus requires extensive experimentation and evaluation. To facilitate experimentation with Vehicular Named-Data Networking, we present the NDN4IVC, an open-source simulator/framework that facilitates testing of more realistic VANET applications via the NDN stack. This project utilizes two popular simulators for VANET simulation: the NS3 network simulator with the ndnSIM module and SUMO, a simulator of urban mobility. NDN4IVC allows real-time bidirectional communication between SUMO and NS3 to support more data about road traffic and vehicular mobility. The framework can model the impact of vehicular networks on road traffic and investigate complex interactions between the two domains. We included two sample applications in VANET to demonstrate how different NDN properties can be used. Experiments were conducted as proof-of-concept studies to demonstrate the potential of the framework and its functionality.
Keywords: Vehicular named-data networking; Vehicular applications; VANET simulation; Simulation architecture

Bander Alzahrani, Omar Sami Oubbati, Ahmed Barnawi, Mohammed Atiquzzaman, Daniyal Alghazzawi,
UAV assistance paradigm: State-of-the-art in applications and challenges,
Journal of Network and Computer Applications,
Volume 166,
2020,
102706,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102706.
(https://www.sciencedirect.com/science/article/pii/S1084804520301806)
Abstract: Unmanned Aerial Vehicles (UAVs) are an emerging technology with the potential to be used in industries and various sectors of human life to provide a wide range of applications and services. During the last decade, there has been a growing focus of research in the UAV's assistance paradigm as a fundamental concept resulting in the constant improvement between different kinds of ground networks and the hovering UAVs in the sky. Recently, the wide availability of embedded wireless interfaces in the communicating entities has allowed the deployment of such a paradigm simpler and easiest. Moreover, due to UAVs' controlled mobility and adjustable altitudes, they can be considered as the most appropriate candidate to enhance the performance and overcome the restrictions of ground networks. This comprehensive survey both studies and summarizes the existing UAV-assisted research, such as routing, data gathering, cellular communications, Internet of Things (IoT) networks, and disaster management that supports existing enabling technologies. Descriptions, classifications, and comparative studies related to different UAV-assisted proposals are presented throughout the paper. By pointing out numerous future challenges, it is expected to simulate research in this emerging and hot research area. To the best of our knowledge, there are many survey papers on the topic from a technology perspective. Nevertheless, this survey can be considered as the first attempt at a comprehensive analysis of different types of existing UAV-assisted networks and describes the state-of-the-art in UAV-assisted research.
Keywords: UAV; IoT; Cellular communications; Routing; Data gathering; Fog computing

Biagio Peccerillo, Mirco Mannino, Andrea Mondelli, Sandro Bartolini,
A survey on hardware accelerators: Taxonomy, trends, challenges, and perspectives,
Journal of Systems Architecture,
Volume 129,
2022,
102561,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102561.
(https://www.sciencedirect.com/science/article/pii/S1383762122001138)
Abstract: In recent years, the limits of the multicore approach emerged in the so-called “dark silicon” issue and diminishing returns of an ever-increasing core count. Hardware manufacturers, out of necessity, switched their focus to accelerators, a new paradigm that pursues specialization and heterogeneity over generality and homogeneity. They are special-purpose hardware structures separated from the CPU with aspects that exhibit a high degree of variability. We define a taxonomy based on fourteen of these aspects, grouped in four macro-categories: general aspects, host coupling, architecture, and software aspects. According to it, we categorize around 100 accelerators of the last decade from both industry and academia, and critically analyze emerging trends. We complete our discussion with throughput and efficiency figures. Then, we discuss some prominent open challenges that accelerators are facing, analyzing state-of-the-art solutions, and suggesting prospective research directions for the future.
Keywords: Accelerators; Domain-Specific Architectures; Survey; Taxonomy; Classification; Data-parallel; Machine Learning; PIM; CGRA; Open challenges; Future research directions

Khaled Sarieddine, Malak Charaf, Mohammad Ayad, Hassan Artail,
A framework for mobile relay node selection for serving outdoor cell edge users,
Computer Networks,
Volume 178,
2020,
107359,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107359.
(https://www.sciencedirect.com/science/article/pii/S1389128620302164)
Abstract: Relaying has received significant interest in the wireless communication community and standardization bodies. It aims to improve the cell edge coverage and performance by extending the reach of the base station. Mobile relays can offer attractive advantages over fixed relays in terms of reduced cost and suitability to varying network connection demands, especially when exploiting the availability of public vehicles, such as busses, taxis, and delivery vehicles that roam streets at relatively low speeds. The use of Mobile Relay Nodes (MRNs) to enhance the indoor user signal quality has been studied in the literature, and shows that a significant coverage gain can be achieved. In this work, we explore the use of mobile relay nodes for outdoor UE connectivity enhancement. We propose a UE-MRN association framework and identify cases of relay distribution where the use of mobile relays can be most beneficial. We evaluate two association strategies, taking into consideration two interference scenarios. Our simulation results illustrate that the operator can achieve a significant gain by enabling mobile relaying using public transportation vehicles that could normally be found roaming within the area that is close to the cell edge.
Keywords: Mobile relay node; MRN; Cell-edge; SINR; Time of connection

Imane El Mensoum, Omar Abdul Wahab, Nadjia Kara, Claes Edstrom,
MuSC: A multi-stage service chains embedding approach,
Journal of Network and Computer Applications,
Volume 159,
2020,
102593,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102593.
(https://www.sciencedirect.com/science/article/pii/S1084804520300679)
Abstract: Network function virtualization is an emerging concept that is attracting increasing attention in the industry because it offers high levels of agility and flexibility to the network allowing easier software updates, resource adjustment and scalable changes in the configuration of the services. Network functions traditionally delivered on hardware and purpose-built platforms in legacy networks can now be provided through shared virtual resources called VNFs (Virtual network functions) hosted over shared physical network infrastructures. Using network functions as absolute software components would certainly improve the deployment process and the management of the VNF life cycle. Since VNFs are hardware-independent, there is no need to check whether or not a network function is compatible with the rest of the network's physical parts; this also helps reduce the maintenance costs, which can increase drastically in the case of major upgrades to the network infrastructure. Along with all these benefits, shifting to virtual-based networks comes with a significant number of challenges, especially in terms of placing such virtual components, ensuring their interoperability and maintaining the quality of service at a level that is at least as good as what is offered by hardware based architectures. In this paper, we propose a cluster-based placement and chaining solution. The overall proposed approach consists of: 1) formulating an Integer Linear Programming (ILP) model aimed at finding an optimal tradeoff between multiple objective functions that might be sometimes conflicting (e.g hardware resource and energy consumption minimization, transmission delays, bandwidth usage, etc…), 2) classifying the substrate network into a set of on-demand clusters that are efficient for a predefined set of metrics, and 3) using meta-heuristic-based algorithms to find near-optimal solutions for the formulated ILP.
Keywords: Network function virtualization; Service function chains; Virtual functions placement and chaining; Genetic algorithm; Chemical reaction optimization

Namrata Singh, Ayan Kumar Das,
Energy-efficient fuzzy data offloading for IoMT,
Computer Networks,
Volume 213,
2022,
109127,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109127.
(https://www.sciencedirect.com/science/article/pii/S1389128622002493)
Abstract: The advancement of Internet of Things based technologies in healthcare monitoring systems has evolved a new terminology, named Internet of Medical Things for medical services and devices. It is integrated with cloud-fog computing environment to facilitate load balancing solutions and enhance the quality of service using message exchange protocols. However, challenges for the quality parameters such as energy consumption, latency, resource utilization, scalability and packet loss associated with the existing architectural models are still of a great concern for researchers. This paper presents an energy efficient fuzzy data offloading scheme with a four tier cloud-fog architectural design to improve the quality parameters. The proposed Message Queuing Telemetry Transfer protocol based message exchange mechanism encapsulates the payload area of messages with client-ID and timestamp to provide authentication and ordering in packet transmission. Fuzzy logic based categorization of medical data has been used to classify it as emergency data, significant data and general data. Clustering of fog nodes has been done based on CPU speed and remaining energy. An adaptive scheduling technique has been proposed which considers the memory value for assigning the weight to different classified queues. The proposed scheme is evaluated and validated using iFogSim toolkit. The performance analysis shows maximum of 77% reduction in energy consumption, 48%, 60% and 44% reduction in end to end delay for different Quality of Services QoS 0, QoS 1, and QoS 2, respectively compared to other existing schemes.
Keywords: IoMT; MQTT; Load balancing; Scheduling; Clustering; Latency

Ikram Senoussaoui, Giuseppe Lipari, Houssam-Eddine Zahaf, Mohammed Kamal Benhaoua,
Memory-processor co-scheduling of AECR-DAG real-time tasks on partitioned multicore platforms with scratchpads,
Journal of Systems Architecture,
Volume 150,
2024,
103117,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103117.
(https://www.sciencedirect.com/science/article/pii/S1383762124000547)
Abstract: Multicore systems with core-level scratchpad memories offer appealing architectures for constructing efficient and predictable real-time systems. In this work, we aim to improve the usability of scratchpad memories and exploit their predictability to hide access latency to shared resources. We use a genetic algorithm to derive scheduling parameters for a set of directed acyclic task-graphs (DAGs). DAGs consist of dependent subtasks and their respective communications, following the Acquisition Execution Restitution (AER) model. Subtasks are partitioned onto the multicore platform while scheduling their memory requests and relative communications onto the shared buses, in order to prevent interference and ensure predictability. Specifically, all subtasks and communications are assigned appropriate intermediate offsets and deadlines to guarantee that they comply with the system’s timing constraints. We conducted a large set of synthetic experiments to demonstrate the effectiveness of the proposed technique.
Keywords: Real-time systems; Genetic-algorithm; Scratchpad; Multicore architecture; Memory-processor co-scheduling; DAG task

Yinghui Sai, Xiaotong Li, Ruiting Zhou, Zongpeng Li,
An efficient online auction for resource leasing in cloud radio access networks,
Computer Networks,
Volume 177,
2020,
107316,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107316.
(https://www.sciencedirect.com/science/article/pii/S138912861930790X)
Abstract: This work studies the emerging C-RAN market in a 5G wireless network where mobile operators lease computation and communication resources from the tower company to serve wireless users. We propose an online C-RAN auction where each mobile operator bids for three types of resources in a future time window: wireless spectrum at base stations (BSs), front-haul link capacities, and mobile BS instances at the mobile cloud. We target an online C-RAN auction that executes in polynomial time, elicits truthful bids from mobile operators, and maximizes the social welfare of the C-RAN eco-system with both spectrum cost at BSs and server cost at the mobile cloud considered. We consider two business models of the tower company, and show how the marriage of i) a new Fenchel dual approach to convex optimization with ii) the posted pricing framework for online auction design can help achieve the three goals simultaneously, and evaluate the efficiency of our online C-RAN auction through both theoretical analysis and empirical studies.
Keywords: Online auction; C-RAN; Resource leasing

Sagar Arora, Adlen Ksentini, Christian Bonnet,
Cloud native Lightweight Slice Orchestration (CLiSO) framework,
Computer Communications,
Volume 213,
2024,
Pages 1-12,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.10.010.
(https://www.sciencedirect.com/science/article/pii/S0140366423003742)
Abstract: Cloud-native network functions are becoming promising for 5G and beyond networks. They provide the much needed agility and flexibility that was missing from virtual machines. Though, this paradigm shift of using cloud-native application design principles, containerization, microservices, high resilience, and on-demand scaling has created challenges for legacy orchestration systems. They were designed for handling virtual machine-based network functions. Indeed, network slice orchestration requires interaction with multiple technological domain orchestrators, access, transport, core network, and edge computing. The specifications and existing orchestrators are made on top of the legacy virtual machine based network function orchestration. Hence, this limitation constrains their approach to managing a cloud-native network function. To overcome their challenges, we propose a novel Cloud-native Lightweight Slice Orchestration (CLiSO) framework extending our previously proposed Lightweight edge Slice Orchestration (LeSO) framework. In addition, we present a technology-agnostic and deployment-oriented network slice template. To allow zero-touch management of network slices, our framework provides a concept of Domain Specific Handlers. The framework has been thoroughly evaluated via orchestrating OpenAirInterface container network functions on public and private cloud platforms.
Keywords: Network slice orchestration; Cloud-native; Microservices; Containers

T. Murugeswari, S. Rathi,
Priority and interference aware multipath routing based communications for extreme surveillance systems,
Computer Communications,
Volume 150,
2020,
Pages 537-546,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.11.050.
(https://www.sciencedirect.com/science/article/pii/S0140366419311806)
Abstract: Increased natural disaster in various urban and rural areas requires immediate attention to avoid the major causes. In real world, immediate recovery to the disaster areas is ensured by adapting the unmanned aerial vehicles that enable people to reach the disaster areas immediately. Here specific task in the disaster area can be completed efficiently by working with multi Unmanned Aerial Vehicle (UAV) instead of single UAV. Here data communication between the UAV needs to be very reliable to ensure the proper disaster management outcome. It is more complex to provide the required services to the users when there is situation arise to switch between the heterogeneous networks. The QoS-Oriented Distributed routing protocol (QOD) is used in the existing methods to give solution to this problem. The data is transferred between hybrid networks with required QoS. In the existing work, routing is done by considering the QoS consideration thus the efficient and reliable distributed routing is guaranteed. However the existing work lacks from the following issues: It avoids the data transfer through the path in which data transmission is going already to avoid the interference problems which might reduce the throughput rate. Priority of the data transferred from multiple sources are not considered in the previous work and also existing work focused on reducing delay alone as QoS factor. Priority and Interference aware Multipath Routing Protocol (PIMRP) is introduced to rectify this issue in the proposed method. In this method, interference aware and priority routing is ensured by introducing the following research methods. Here, Multipath interference based routing method is used allow transmission of data from multiple path that resides within interference range with guaranteed interference avoidance and increased throughput. Here route path nodes are selected with multiple objectives such as delay, bandwidth, and energy consumption. To provide more preference to the prioritized data transmission nodes with more resource availability is provided to the prioritized data packets which are tiny segmented. The performance of the proposed method is evaluated using NS2 simulation tool. The simulation results confirm the efficiency of the proposed method.
Keywords: Multipath routing; Interference aware routing; Throughput; QoS parameters; Hybrid wireless network

L. Atzori, J.L. Bellido, R. Bolla, G. Genovese, A. Iera, A. Jara, C. Lombardo, G. Morabito,
SDN&NFV contribution to IoT objects virtualization,
Computer Networks,
Volume 149,
2019,
Pages 200-212,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.11.030.
(https://www.sciencedirect.com/science/article/pii/S1389128618312933)
Abstract: The Internet of Things paradigm will certainly be one of the main drivers of the tomorrow's 5th generation (5G) wireless networks. In order to support the algorithms required for real time exchange and analysis of great amounts of data among the involved smart devices, Virtual Objects (VO) have become a key component to improve the objects energy management efficiency, as well as to address heterogeneity and scalability issues. Following the research trend of exploiting on-demand cloud computing resources to augment the processing and storage capabilities of IoT devices, this paper addresses the design of a novel infrastructure and paradigm to support the deployment of new personal IoT services inside the infrastructure provider premises. The goal is to bring cloud-computing services much closer to the end-users and to be able to replace physical IoT devices with their “Virtual Images”. Among other benefits, this approach will ensure a longer lifetime to IoT constrained devices and enable the inclusion of new protocols making the development of logic and the configuration of IoT smart application environment technology-agnostic. To achieve this goal, we have developed an open-source software platform that exploits OpenStack APIs and leverages a web interface providing all the functionality typically available in a home gateway through the OpenWRT Linux distribution. Results show this approach brings a manifest reduction in the amount of data transmitted, with benefits in terms of reduced workload and power consumption as well as extended device lifetime.
Keywords: Internet of Things; Network functions virtualization; Virtual Object; Mobile Edge Computing

Aleksandr Saprykin, Ndaona Chokani, Reza S. Abhari,
Accelerating agent-based demand-responsive transport simulations with GPUs,
Future Generation Computer Systems,
Volume 131,
2022,
Pages 43-58,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.01.009.
(https://www.sciencedirect.com/science/article/pii/S0167739X22000176)
Abstract: A novel GPU-accelerated simulation model of large-scale fleet deployment, which can run country-wide, multi-modal scenarios with millions of agents and fleets of tens of thousands of vehicles within a couple of minutes, is presented. Multiple scenarios of the deployment of fleets of automated vehicles in Switzerland’s largest city, Zurich, are assessed. The simulations include the whole population of Switzerland (3.5 million car owners and 1.7 million public transit users) with their detailed travel demand, the road network (1.1 million links and 0.5 million intersections), and public transit (30 000 stops and 20 000 routes). It is demonstrated that in Zurich one automated vehicle could replace 7–8 private cars with an average increase in the road travel time of 44% and with wait times in the range of 10–15 min, provided travel demand remains constant. Furthermore, for the same fleet size, this novel accelerated simulation model runs up to 9 times faster compared to existing state-of-the-art tools.
Keywords: GPU; Traffic simulation; Agent-based simulation; Fleet; Automated vehicle; Taxi

Thomas Dangl, Stewart Sentanoe, Hans P. Reiser,
Active and passive virtual machine introspection on AMD and ARM processors,
Journal of Systems Architecture,
Volume 149,
2024,
103101,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2024.103101.
(https://www.sciencedirect.com/science/article/pii/S1383762124000389)
Abstract: Active and passive virtual machine introspection mechanisms are pivotal for monitoring virtual machines on top of a hypervisor. They enable external tools to monitor and inspect the state from the outside. Active virtual machine introspection mechanisms intercept the execution at predetermined locations of interest synchronous to the execution of the system. Such mechanisms, in particular, require support from the processor vendor by facilitating interpositioning. This support is missing on AMD x86 processors, leading to inferior introspection solutions. We outline implicit assumptions about active introspection mechanisms in previous work, offer constructions for solution strategies on AMD systems, and discuss stealthiness and correctness. We show empirically that such retrofitted software solutions exhibit performance metrics in the same order of magnitude as native hardware solutions. Moreover, we highlight that the open problems for virtual machine introspection on ARM systems and those encountered on AMD x86 are related. Hence, we present an introspection architecture based on KVMi that addresses these open problems. Finally, we demonstrate comparable and, in many cases, superior performance to state-of-the-art solutions on Intel x86.
Keywords: Virtual machine introspection; Monitoring; System security; Reliability; Stealthiness; Cloud computing

Aparna Kumari, Sudeep Tanwar, Sudhanshu Tyagi, Neeraj Kumar, Reza M. Parizi, Kim-Kwang Raymond Choo,
Fog data analytics: A taxonomy and process model,
Journal of Network and Computer Applications,
Volume 128,
2019,
Pages 90-104,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2018.12.013.
(https://www.sciencedirect.com/science/article/pii/S1084804518304016)
Abstract: Through the exponential growth of sensors and smart gadgets (collectively referred to as smart devices or Internet of Things (IoT) devices), significant amount of heterogeneous and multi-modal data, termed as Big Data (BD), is being generated. To deal with such BD, we require efficient and effective solutions such as data mining, analytics, and reduction to be deployed at the edge of fog devices on a cloud. Existing research and development efforts generally focus on performing BD analytics overlook the difficulty of facilitating fog data analytics (FDA). In this paper, we discuss the unique nature and complexity of fog data analytics. A detailed taxonomy for FDA is abstracted into a process model. The proposed model addresses various research challenges, such as accessibility, scalability, fog nodes communication, nodal collaboration, heterogeneity, reliability, and quality of service (QoS) requirements. To demonstrate the proposed process model, we present two case studies.
Keywords: Big data; Fog data analytics; Data collection; Data storage; Data reduction; Data security and privacy

Nan Wang, Blesson Varghese,
Context-aware distribution of fog applications using deep reinforcement learning,
Journal of Network and Computer Applications,
Volume 203,
2022,
103354,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103354.
(https://www.sciencedirect.com/science/article/pii/S1084804522000236)
Abstract: Fog computing is an emerging paradigm that aims to meet the increasing computation demands arising from the billions of devices connected to the Internet. Offloading services of an application from the Cloud to the edge of the network can improve the overall latency of the application since it can process data closer to user devices. Diverse Fog nodes ranging from Wi-Fi routers to mini-clouds with varying resource capabilities makes it challenging to determine which services of an application need to be offloaded. In this paper, a context-aware mechanism for distributing applications across the Cloud and the Fog is proposed. The mechanism dynamically generates (re)deployment plans for the application to maximise the performance efficiency of the application by taking operational conditions, such as hardware utilisation and network state, and running costs into account. The mechanism relies on deep Q-networks to generate a distribution plan without prior knowledge of the available resources on the Fog node, the network condition, and the application. The feasibility of the proposed context-aware distribution mechanism is demonstrated on two use-cases, namely a face detection application and a location-based mobile game. The benefits are increased utility of dynamic distribution by 50% and 20% for the two use-cases respectively when compared to a static distribution approach used in existing research.
Keywords: Fog computing; Decentralised cloud; Edge computing; Context-aware distribution

Donald J.P, Linda Joseph,
Information centric wireless communication for variation detection and Mitigation Model in industrial internet of Things,
Computer Communications,
Volume 211,
2023,
Pages 1-10,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.09.001.
(https://www.sciencedirect.com/science/article/pii/S0140366423003110)
Abstract: The Industrial Internet of Things (IIoT) harmonizes smart devices, machines, and intelligent technologies to improve production efficiency through the information-centric network. Information-Centric Networking (ICN) is a potentially useful method in control sequences and intelligent operations due to adversary impacts. Here, the sharing of content chunks between different machines' varying control spans and operation cycles has been partially connected to the internet. This article introduces a Variant Loop Detection and Mitigation Model (VLDMM) for preventing illegitimate entries in ICN. The proposed model is backboned by recurrent learning for training the loop closures and time achievements in ICN. The harmonization is utilized under different production outputs observed from the previous cycles. The interruptions and process halts are identified using recurrent training by correlating the previous operation logs in ICN. Therefore, ICN may solve the challenges based on the available control slots and variations. This learning retains the production efficiency and prevents halts under controlled time and loop closures.
Keywords: Information-centric networking; Industrial internet of Things; Variant loop detection model; Recurrent learning; Machine cycles

Karan Sheth, Keyur Patel, Het Shah, Sudeep Tanwar, Rajesh Gupta, Neeraj Kumar,
A taxonomy of AI techniques for 6G communication networks,
Computer Communications,
Volume 161,
2020,
Pages 279-303,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.07.035.
(https://www.sciencedirect.com/science/article/pii/S0140366420318478)
Abstract: With 6G flagship program launched by the University of Oulu, Finland, for full future adaptation of 6G by 2030, many institutes worldwide have started to explore various issues and challenges in 6G communication networks. 6G offers ultra high-reliable and massive ultra-low latency while opening the doors for many applications currently not viable by today’s 4G and 5G communication standards. The current 5G technology has security and privacy issues which makes its usage in limited applications. In such an environment, we believe that AI can offer efficient solutions for the aforementioned issues having low communication overhead cost. Keeping focus on all these issues, in this paper, we presented a comprehensive survey on AI-enabled 6G communication technology, which can be used in wide range of future applications. In this article, we explore how AI can be integrated into different applications such as object localization, UAV communication, surveillance, security and privacy preservation etc. Finally, we discussed a use case that shows the adoption of AI techniques in intelligent transport system.
Keywords: Artificial Intelligence; 6G; Communication networks; Mobile edge computing; Intelligent transportation system

Alessio Sacco, Matteo Flocco, Flavio Esposito, Guido Marchetto,
An architecture for adaptive task planning in support of IoT-based machine learning applications for disaster scenarios,
Computer Communications,
Volume 160,
2020,
Pages 769-778,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.07.011.
(https://www.sciencedirect.com/science/article/pii/S0140366419316779)
Abstract: The proliferation of the Internet of Things (IoT) in conjunction with edge computing has recently opened up several possibilities for several new applications. Typical examples are Unmanned Aerial Vehicles (UAV) that are deployed for rapid disaster response, photogrammetry, surveillance, and environmental monitoring. To support the flourishing development of Machine Learning assisted applications across all these networked applications, a common challenge is the provision of a persistent service, i.e., a service capable of consistently maintaining a high level of performance, facing possible failures. To address these service resilient challenges, we propose APRON, an edge solution for distributed and adaptive task planning management in a network of IoT devices, e.g., drones. Exploiting Jackson’s network model, our architecture applies a novel planning strategy to better support control and monitoring operations while the states of the network evolve. To demonstrate the functionalities of our architecture, we also implemented a deep-learning based audio-recognition application using the APRON NorthBound interface, to detect human voices in challenged networks. The application’s logic uses Transfer Learning to improve the audio classification accuracy and the runtime of the UAV-based rescue operations.
Keywords: Network of queues; Machine Learning

Miao Hu, Xianzhuo Luo, Jiawen Chen, Young Choon Lee, Yipeng Zhou, Di Wu,
Virtual reality: A survey of enabling technologies and its applications in IoT,
Journal of Network and Computer Applications,
Volume 178,
2021,
102970,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102970.
(https://www.sciencedirect.com/science/article/pii/S1084804520304215)
Abstract: Virtual Reality (VR) has shown great potential to revolutionize the market by providing users immersive experiences with freedom of movement. Compared to traditional video streaming, VR is with ultra high-definition and dynamically changes with users’ head and eye movements, which poses significant challenges for the realization of such potential. In this paper, we provide a detailed and systematic survey of enabling technologies of virtual reality and its applications in Internet of Things (IoT). We identify major challenges of virtual reality on system design, view prediction, computation, streaming, and quality of experience evaluation. We discuss each of them by extensively surveying and reviewing related papers in the recent years. We also introduce several use cases of VR for IoT. Last, issues and future research directions are also identified and discussed.
Keywords: Virtual reality; Video streaming; Quality of experience; Internet of things

Daniele D’Agostino, Alfonso Quarati, Andrea Clematis, Lucia Morganti, Elena Corni, Valentina Giansanti, Daniele Cesini, Ivan Merelli,
SoC-based computing infrastructures for scientific applications and commercial services: Performance and economic evaluations,
Future Generation Computer Systems,
Volume 96,
2019,
Pages 11-22,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.01.024.
(https://www.sciencedirect.com/science/article/pii/S0167739X18311622)
Abstract: Energy consumption represents one of the most relevant issues by now in operating computing infrastructures, from traditional High Performance Computing Centers to Cloud Data Centers. Low power System-on-Chip (SoC) architectures, originally developed in the context of mobile and embedded technologies, are becoming attractive also for scientific and industrial applications given their increasing computing performances, coupled with relatively low costs and power demands. In this paper, we investigate the performance of the most representative SoCs for a computational intensive N-body benchmark, a simple deep learning based application and a real-life application taken from the field of molecular biology. The goal is to assess the trade-off among time-to-solution, energy-to-solution and economical aspects for both scientific and commercial purposes they are able to achieve in comparison to traditional server-grade architectures adopted in present infrastructures.
Keywords: Low power Systems-on-Chip; N-body benchmark; Deep learning; Next-Generation Sequencing; Performance and economic evaluations

Prabhakar Krishnan, Subhasri Duttagupta, Rajkumar Buyya,
OpenPATH: Application aware high-performance software-defined switching framework,
Journal of Network and Computer Applications,
Volume 193,
2021,
103196,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103196.
(https://www.sciencedirect.com/science/article/pii/S1084804521002022)
Abstract: Currently, core networking architecture is facing disruptive developments, due to the emergence of SDN for control, NFV for services and so on. SDN promises more versatility in routing and managing traffic flows, while NFV represents a large shift in how network functions and services are built, deployed, and managed. We present OpenPATH (aPplication Aware software-defined swiTcHing framework)—A software-defined switching framework for NFV processing and orchestration of Network Functions (NFs) and steering the flows through service chains. Inspired by the potential benefits of encapsulating the application logic into the SDN dataplane, OpenPATH is built on the concept of a modular dataplane, which consists of two layers - switching fabric layer to control packet forwarding; and switch management layer, which inspects the incoming packets, steers the flows through a sequence of NFs and determines the next forward/drop action. The application logic of the NFs can be introduced and pushed to the dataplane at runtime and the framework offers fast packet processing and I/O functionalities to support NF parallelism in the Service Function Chaining (SFC) scenarios. OpenPATH is a modular framework for software switches and offers flexibility for programming run time functions depending on the dynamic behavior of the network traffic and cyberattacks. The architecture components are not hard-coded or rigidly implementations in conventional switches/bridges and standard OpenFlow based SDN stacks. The design allows the vendors, operators, or developers to configure policies at run time and deploy custom logic and NF (also series of NFs) through software programs embedded in the switching fabric. While the basic concept is similar to some pioneering works in this area, OpenPATH does not sacrifice portability, performance, or security for programmability. The OpenPATH as a programmable switching platform takes a different approach to meet most of the requirements of application-aware and intent-based networking. OpenPATH helps administrators to quickly configure network security services using a rich set of standard APIs, with simplified flow tables. The evaluation shows that our design can leverage complex states in the data plane without overloading the SDN controller. Compared to conventional SDN methods, this provides much greater versatility and precision. The key findings indicate that OpenPATH achieves lower cost for scaling, higher overall throughput, and reductions in latency for real-world service chains.
Keywords: SDN; NFV; Service function chaining; Network function parallelism; Intent-based networking; Software switching

B. Asvija, R. Eswari, M.B. Bijoy,
Security in hardware assisted virtualization for cloud computing—State of the art issues and challenges,
Computer Networks,
Volume 151,
2019,
Pages 68-92,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.01.013.
(https://www.sciencedirect.com/science/article/pii/S1389128618302998)
Abstract: The advantages of virtualization technology have resulted in its wide spread adoption in cloud computing infrastructures. However it has also introduced a new set of security threats that are serious in nature. Many of these threats are unique in virtualized environments and not pertinent in the traditional computing scenarios. Hence these threats have been less studied and thus less addressed by most of the security application vendors. For this reason, it becomes important to carefully analyze the various threats arising at different components of virtualization and thus effectively create solutions to defend the systems against them. This survey attempts to highlight the significant vulnerabilities and expose the readers to the various existing attacks related to Hardware assisted virtualization, as it has become the most widely used form of virtualization in building modern day massive data centers and cloud infrastructures. A Bayesian attack graph model is presented for evaluating the risks associated with the identified threats. A detailed discussion of various countermeasures proposed against the identified threats is presented along with the enumeration of challenges in adopting them.
Keywords: Hardware assisted virtualization; Hypervisor; VMM; Side channels; Bayesian attack graphs; Risk assessment; Security

Bart Spinnewyn, Steven Latré, Juan Felipe Botero,
Delay-constrained NFV orchestration for heterogeneous cloud networks,
Computer Networks,
Volume 180,
2020,
107420,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107420.
(https://www.sciencedirect.com/science/article/pii/S1389128620311099)
Abstract: Recently, network services are increasingly connecting computational elements within and across datacenters. In the Network Functions Virtualization (NFV) environment, to successfully orchestrate a network service, first a VNF-Forwarding Graph (VNF-FG) must be composed that realizes the required functionality. Second, this VNF-FG must be embedded onto the infrastructure, that is increasingly becoming heterogeneous. To efficiently allocate service demands, intelligent mechanisms and algorithms are needed to effectively tailor the VNF-FG to the cloud network onto which the service will be deployed. This paper introduces the first service model supporting on-demand orchestration of services with bidirectional chaining and delay constraints, optional VNFs and traffic aggregation. Building on this service model, we propose algorithms looking for the optimization of the order and number of VNF instances, to adapt the VNF-FG to the available resources in the substrate network. Numerical experiments show that, through coordination of composition and embedding tasks, our proposed algorithms can significantly improve the acceptance ratio, compared to algorithms that perform these tasks in two separate stages.
Keywords: Heuristics; Network function virtualization; Optimization; Orchestration; Resource allocation

Grzegorz Gurgul, Bartosz Baliś, Maciej Paszyński,
Cloud-native alternating directions solver for isogeometric analysis,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 151-172,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.10.017.
(https://www.sciencedirect.com/science/article/pii/S0167739X22003338)
Abstract: Computer simulations with isogeometric analysis (IGA) have multiple applications, from phase-field modeling to tumor-growth simulations. We focus on the alternating-directions solver (ADS) algorithm, in which the matrix equation representing a computational problem is decomposed into parallel tasks following the binary and balanced structure of an elimination tree. In this paper, we explore the possibility of running large-scale IGA simulations using linear computational cost alternating direction solvers on top of modern data-parallel cloud computing frameworks. To this end, we propose a new way of decomposition of the elimination tree which makes the IGA alternating-direction solver effectively a large graph problem suitable for modern cloud-computing frameworks. On this basis, we propose a new algorithm for isogeometric analysis alternating-directions solver based on the Pregel computational model, used for large-scale graph-processing in the cloud. We implement a cloud-native solver using this algorithm in the Apache Giraph framework, and show that it can be applied for solution of challenging higher-order PDEs. We evaluate the solver in terms of various scalability models and run configurations. The results indicate linear scalability of the proposed algorithm with respect to the number of elements in the mesh.
Keywords: Cloud computing; Isogeometric analysis (IGA); Alternating-direction implicit solver (ADI); Think like a vertex (TLAV) paradigm; Parallel programming paradigm Pregel; Apache Giraph framework

Zhaolin Ma, Jiali You, Haojiang Deng,
DINNRS: A Distributed In-Network Name Resolution System for information-centric networks,
Computer Communications,
Volume 213,
2024,
Pages 188-198,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.11.008.
(https://www.sciencedirect.com/science/article/pii/S0140366423003997)
Abstract: Information-Centric Networking (ICN) is a promising network paradigm designed to address the challenges arising from the inherent ambiguity of IP addresses. ICN prioritizes the content itself as the main factor in network transmission. The Name Resolution System (NRS) is the primary component of the ICN architecture, relying on flat naming. Unfortunately, existing NRS frequently encounters scalability and performance issues. In this paper, we present the Distributed In-Network Name Resolution System (DINNRS), an NRS that leverages the software-defined networking and ICN paradigm. DINNRS offers global-scale name-based routing with high scalability and minimal request delay. Additionally, we propose a modular control domain architecture for easy implementation and an enhanced marked cuckoo filter for fast resolving. Simulation experiments using the ICN emulator demonstrate that our methods result in significant performance gains, with a 61.2% reduction in data acquire latency and a 55.2% lower link load control overhead compared to MDHT.
Keywords: Information-centric network; Software-defined networks; Name resolution; Cuckoo filter; Distributed system

Guanglei Li, Bohao Feng, Huachun Zhou, Yuming Zhang, Keshav Sood, Shui Yu,
Adaptive service function chaining mappings in 5G using deep Q-learning,
Computer Communications,
Volume 152,
2020,
Pages 305-315,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.035.
(https://www.sciencedirect.com/science/article/pii/S0140366419313817)
Abstract: With introduction of Software-Defined Networking (SDN) and Network Functions Virtualization (NFV) technologies, mobile network operators are able to provide on-demand Service Function Chaining (SFC) to meet various needs from users. However, it is challenging to map multiple SFCs to substrate networks efficiently, particularly in a number of key scenarios of forthcoming 5G, where user requests have different priorities and various resource demands. To this end, we first formulate the mapping of multiple SFCs with priorities as a multi-step Linear Integer Programming (ILP) problem, of which the mapping strategy (i.e., the objective function) in each step is configurable to improve overall CPU and bandwidth resource utilization rates. Secondly, to solve the strategy selection problem in each step and alleviate the complexity of ILP, we propose an adaptive deep Q-learning based SFC mapping approach (ADAP), where an agent is learned to make decisions from two low-complexity heuristic SFC mapping algorithms. Finally, we conduct extensive simulations using multiple SFC requests with randomly generated CPU and bandwidth demands in a real-world substrate network topology. Related results demonstrate that compared with a single strategy or random selections of strategies under the ILP-based approach or the proposed heuristic algorithms, our ADAP approach can improve whole-system resource efficiency by scheduling this two simply designed heuristic algorithms properly after limited training episodes.
Keywords: Resource allocation; Service function chaining; Network function virtualization; Deep reinforcement learning

Gustavo Diel, Charles Christian Miers, Maurício Aronne Pillon, Guilherme Piêgas Koslovski,
RSCAT: Towards zero touch congestion control based on actor–critic reinforcement learning and software-defined networking,
Journal of Network and Computer Applications,
Volume 215,
2023,
103639,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103639.
(https://www.sciencedirect.com/science/article/pii/S1084804523000589)
Abstract: Network congestion is a phenomenon present in contemporaneous data centers (DCs) independently of scale and underlying technologies. The small-scale presence of congestion causes information delay for the DC hosted services, while in large-scale data losses or even service downtimes are not uncommon. Several congestion control algorithms out in the market have no network support, and as such, they must act and rely only on the data provided by the servers they are running on. Some algorithms have used network’s feedback to improve the performance, however these solutions depend on tuning parameters for marking queued packets, an arduous task on DCs shared by multiple applications with different workloads. In turn, Software-Defined Networking (SDN) created an opportunity to avoid congestion once the centralized controller can gather ongoing and historical information from all network switches and flows. The data gathered is enormous, and fast-computing algorithms are crucial for decision-making. In this sense, this work proposes Reinforcement Learning and SDN-aided Congestion Avoidance Tool (RSCAT), which uses data classification to determine if the network is congested and actor–critic reinforcement learning to find better Transmission Control Protocol (TCP) parameters. We evaluated RSCAT based on three workloads: a diversified DC traffic simulation; high-performance computing from NAS benchmark; and Hadoop TeraSort application. Our results show RSCAT fundamentals (reinforcement learning, data classification, and SDN) are potential candidates to achieve zero touch network congestion control. Specifically, our experimental analysis shows RSCAT, running alongside with traditional TCP congestion control algorithms like DCTCP and CUBIC, can decrease the flow completion time and applications’ runtimes in several cases, without requiring any software update on DC end-points.
Keywords: Congestion control; Network; Actor–critic; Deep reinforcement learning; Classification; Zero touch

Yashar Deldjoo, Markus Schedl, Peter Knees,
Content-driven music recommendation: Evolution, state of the art, and challenges,
Computer Science Review,
Volume 51,
2024,
100618,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2024.100618.
(https://www.sciencedirect.com/science/article/pii/S1574013724000029)
Abstract: The music domain is among the most important ones for adopting recommender systems technology. In contrast to most other recommendation domains, which predominantly rely on collaborative filtering (CF) techniques, music recommenders have traditionally embraced content-based (CB) approaches. In the past years, music recommendation models that leverage collaborative and content data – which we refer to as content-driven models – have been replacing pure CF or CB models. In this survey, we review 55 articles on content-driven music recommendation. Based on a thorough literature analysis, we first propose an onion model comprising five layers, each of which corresponds to a category of music content we identified: signal, embedded metadata, expert-generated content, user-generated content, and derivative content. We provide a detailed characterization of each category along several dimensions. Second, we identify six overarching challenges, according to which we organize our main discussion: increasing recommendation diversity and novelty, providing transparency and explanations, accomplishing context-awareness, recommending sequences of music, improving scalability and efficiency, and alleviating cold start. Each article addresses one or more of these challenges and is categorized according to the content layers of our onion model, the article’s goal(s), and main methodological choices. Furthermore, articles are discussed in temporal order to shed light on the evolution of content-driven music recommendation strategies. Finally, we provide our personal selection of the persisting grand challenges which are still waiting to be solved in future research endeavors.
Keywords: Content-based; Music; Recommender Systems; Onion model

Fahimeh Yazdanpanah,
A two-level network-on-chip architecture with multicast support,
Journal of Parallel and Distributed Computing,
Volume 172,
2023,
Pages 114-130,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.10.011.
(https://www.sciencedirect.com/science/article/pii/S0743731522002258)
Abstract: It is essential for implementing processing systems of edge computing, internet of things (IoT) and wireless multimedia sensor networks (WMSN) to use low-power parallel and distributed architectures with high speed and low power consumption. Most of the artificial intelligence and machine learning applications need to be executed on efficient distributed architectures with multicast support. In this paper, TLAM, a high-performance and low-cost NoC architecture is proposed. TLAM stands for Two-Level network-on-chip Architecture with Multicast support and includes a hybrid path/tree based multicast routing algorithm and a specific two-level mesh topology. The routing algorithm of TLAM is basically working according to the Hamiltonian paths. In the proposed architecture, the topology is partitioned and the two-level links provide an efficient infrastructure for low-cost and low-latency transmission of multicast and broadcast messages between partitions. In TLAM, in addition to multicasting routing algorithm, hardware components for handling multicasting packets are designed in order to achieve performance improvements. The goal is to improve the efficiency and performance, especially latency, while handling both unicast and multicast packets. Experimental evaluations including network-level and router-level analysis with different configurations under various traffic patterns were performed. The evaluations in terms of latency, throughput, area and power consumption, indicate that TLAM provides higher performance, especially for dense multicasting and broadcasting, in comparison with the existing architectures.
Keywords: Network-on-chip; Multicast routing algorithm; Two-level mesh; Hamiltonian principle

Meriem Achir, Abdelkrim Abdelli, Lynda Mokdad, Jalel Benothman,
Service discovery and selection in IoT: A survey and a taxonomy,
Journal of Network and Computer Applications,
Volume 200,
2022,
103331,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103331.
(https://www.sciencedirect.com/science/article/pii/S1084804521003167)
Abstract: Recently, Internet has evolved into a new generation, called Internet of Things, thus enabling the connection between the physical and the digital worlds by creating an ubiquitous and self-organizing network. A huge number of smart objects are becoming now identifiable and addressable while being able to communicate with each other. Moreover, the integration of cloud infrastructures in the design of IoT, has moved this new trademark technology into a new dimension, enabling virtualisation and service provisioning. Billions of cloud services with different performance levels, requirements and functionalities are thus being offered in IoT, raising however the issues of their management, discovery and selection. In the literature, a considerable effort has been invested to address service discovery and selection in the context of IoT, despite the lack of standardization that meets the IoT requirements. In this paper, we propose an exhaustive taxonomy to classify service discovery approaches in the context of IoT, that we subsequently evaluate according to different aspects and criteria. Then, we discuss the gaps and advantages of each class of our taxonomy and locate the context and the requirements under which each can operate. Finally, we identify the challenges and future research directions in this domain.
Keywords: Taxonomy; Service discovery; Service selection; IoT; QoS; QoE; Classification; Architecture; Object discovery

Jing Wang, Huyin Zhang, Xing Tang, Zongpeng Li,
Delay-tolerant routing and message scheduling for CR-VANETs,
Future Generation Computer Systems,
Volume 110,
2020,
Pages 291-309,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.04.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X19320503)
Abstract: Cognitive radio vehicular ad hoc networks (CR-VANETs) can solve the problem of the limited spectrum resource and improve the vehicle-to-vehicle (V2V) communication efficiency. Nevertheless, the fast moving characteristic of vehicles and the occasional sparse density of vehicles lead to the intermittent CR-VANETs. Recent literature of routing in CR-VANETs focuses on a fully interconnected environment yet fails to address solutions for non-real-time applications. To fill this gap, we propose the delay-tolerant routing and message scheduling (DTRM) schemes for non-real-time applications, aiming to maximize the delivery ratio and decrease the delivery overhead in CR-VANETs. First, we propose a routing scheme that builds a concurrent forwarding set and designs a forwarding strategy. In this set, we select relay candidates by considering both the CR channel availability and the V2V contact duration. Moreover, we evaluate the priority of the relay candidate to provide a reliable forwarding strategy. Second, based on the routing scheme, we propose a message scheduling scheme that combines an optimized-binary-tree replication algorithm and a buffer management policy. The proposed algorithm can spread messages and terminate the replication process quickly. The buffer management policy facilitates the tradeoff between the delivery ratio and the delivery overhead, taking into consideration the node priority, the message tag, and the message remaining time-to-live. Simulation results show that DTRM presents a higher packet delivery ratio and a lower overhead ratio than the counterparts. It presents on average gains of 25.1% and 10.9% in terms of delivery ratio and overhead ratio, respectively, when compared with four CR-VANET routing schemes. It also presents on average gains of 35.6% and 34.1% in terms of delivery ratio and overhead ratio, when compared with three message scheduling schemes.
Keywords: CR-VANETs; Delay-tolerant routing scheme; Message replication; Buffer management

Roberto Rodrigues-Filho, Barry Porter,
Hatch: Self-distributing systems for data centers,
Future Generation Computer Systems,
Volume 132,
2022,
Pages 80-92,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.02.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X22000516)
Abstract: Designing and maintaining distributed systems remains highly challenging: there is a high-dimensional design space of potential ways to distribute a system’s sub-components over a large-scale infrastructure; and the deployment environment for a system tends to change in unforeseen ways over time. For engineers, this is a complex prediction problem to gauge which distributed design may best suit a given environment. We present the concept of self-distributing systems, in which any local system built using our framework can learn, at runtime, the most appropriate distributed design given its perceived operating conditions. Our concept abstracts distribution of a system’s sub-components to a list of simple actions in a reward matrix of distributed design alternatives to be used by reinforcement learning algorithms. By doing this, we enable software to experiment, in a live production environment, with different ways in which to distribute its software modules by placing them in different hosts throughout the system’s infrastructure. We implement this concept in a framework we call Hatch, which has three major elements: (i) a transparent and generalized RPC layer that supports seamless relocation of any local component to a remote host during execution; (ii) a set of primitives, including relocation, replication and sharding, from which to create an action/reward matrix of possible distributed designs of a system; and (iii) a decentralized reinforcement learning approach to converge towards more optimal designs in real time. Using an example of a self-distributing web-serving infrastructure, Hatch is able to autonomously select the most suitable distributed design from among ≈700,000 alternatives in about 5 min.
Keywords: Self-distributing systems; Emergent systems; Autonomic computing

Supraja G., Jeyalakshmi V.,
Throughput maximization and reliable wireless communication in NOMA using chained fog structure and weighted energy efficiency power allocation approach,
Computer Communications,
Volume 208,
2023,
Pages 147-157,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.05.024.
(https://www.sciencedirect.com/science/article/pii/S0140366423001901)
Abstract: Novel radio access strategies must be developed to support an unprecedented number of connected devices to increase radio spectral efficiency of 5G and outside, and non-orthogonal multiple features (NOMA) arisen as a possible contender. To support cognitive NOMA networks, power allocation and NOMA-secondary user allocation is the effective technique for enhancing the resource utilization proficiency in power and spectrum domain. NOMA approach provides throughput enhancement to fulfil the demands of next group of Wireless Communication Networks. In this manuscript, a Chained Fog Structure (CFS) with Weighted Energy Efficiency Power Allocation (WEE-PA) method is proposed for throughput maximization and reliable wireless communication (CFS-WEE-NOMA). The aim of this study is “to enhance the throughput by involving underlay NOMA with Chained Fog Structure and WEE-PA method”. Here, the proposed WEE-PA approach is applied to multicarrier NOMA with chained fog structure for enhancing the throughput. During this analysis, the computational complexity due to the consideration of high users and different subcarrier values are solved by the proposed CFS-WEE-NOMA approach. Thus, the proper user pairing, power allocation, and bandwidth sharing of the proposed methodology ensure seamless communication. The simulation of this work is done in MATLAB and the performance metrics like throughput, computational complexity, power consumption, energy efficiency, delay, sum rate is analysed. The proposed CFS-WEE-NOMA approach has achieved 8.6%, 7.7%, and 6.7% high throughput based on transmitted power, 13.6%, 8.6%, and 11.7% high throughput based on subcarrier, and 13.6%, 9.6%, and 12.7% high energy efficiency compared with the existing methods, like Dynamic Network Resource Allocation (DNRA) algorithm (DNRA-NOMA), Multiple Carrier Cell-Less Non-Orthogonal Multiple Access (MC-CL-NOMA), dynamic programming (DP) recursion framework based multicarrier NOMA systems (DPRF-MC-NOMA) methods respectively.
Keywords: Chained fog structure; Non-orthogonal multiple access; Throughput; Wireless communication; Resource allocation

Chunlin Li, Hezhi Sun, Yi Chen, Youlong Luo,
Edge cloud resource expansion and shrinkage based on workload for minimizing the cost,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 327-340,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.05.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X18331418)
Abstract: The joint use of cloud and edge has been paid more and more attention by enterprises. Enterprises use the cloud’s utility computing and elasticity to placement resources flexibly under the adaptive load. However, there are still many challenges in how to minimize edge cloud costs and ensure data integrity in the resource expansion and shrinkage of edge cloud. This paper studies the resource management problem based on load balance in edge and cloud environments. The novelty is that we consider the resource granularity of the cloud service provider and the data loss problem that resource shrinkage may bring. Firstly, the resource expansion and shrinkage model based on service cost is proposed to reduce the cost of edge cloud clusters under the condition of satisfying the cluster load. Secondly, due to the problem of unbalanced cluster load caused by resource expansion and the data reliability caused by resource shrinkage, a data migration model is proposed aiming at the data loss problem that resource shrinkage may bring. Finally, the proposed algorithms are evaluated with extensive comparisons among the benchmarks and sensitivity tests to various environment factors.
Keywords: Edge cloud architecture; Resource expansion and shrinkage; Data migration

Swapnil Dhamal, Walid Ben-Ameur, Tijani Chahed, Eitan Altman, Albert Sunny, Sudheer Poojary,
Strategic investments in distributed computing: A stochastic game perspective,
Journal of Parallel and Distributed Computing,
Volume 169,
2022,
Pages 317-333,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.07.012.
(https://www.sciencedirect.com/science/article/pii/S0743731522001770)
Abstract: We study a stochastic game with a dynamic set of players, for modeling and analyzing their computational investment strategies in distributed computing. Players obtain a certain reward for solving a problem, while incurring a certain cost based on the invested time and computational power. We present our framework while considering a contemporary application of blockchain mining, and show that the framework is applicable to certain other distributed computing settings as well. For an in-depth analysis, we consider a particular yet natural scenario where the rate of solving the problem is proportional to the total computational power invested by the players. We show that, in Markov perfect equilibrium, players with cost parameters exceeding a certain threshold, do not invest; while those with cost parameters less than this threshold, invest maximal power. We arrive at an interesting conclusion that the players need not have information about the system state as well as each others' parameters, namely, cost parameters and arrival/departure rates. With extensive simulations and insights through mean field approximation, we study the effects of players' arrival/departure rates and the system parameters on the players' utilities.
Keywords: Stochastic games; Markov perfect equilibrium; Distributed computing; Blockchain mining

Pushpita Chatterjee, Debashis Das, Danda B. Rawat,
Digital twin for credit card fraud detection: Opportunities, challenges, and fraud detection advancements,
Future Generation Computer Systems,
2024,
,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.04.057.
(https://www.sciencedirect.com/science/article/pii/S0167739X24001997)
Abstract: Credit cards are widely used for payments due to their convenience and broad acceptance. Their popularity comes with the critical challenge of safeguarding personal and payment information from fraud and unauthorized access. Robust security measures are crucial to maintaining trust and confidence among users. In response to this pressing issue, this paper focuses on credit card fraud detection, its challenges, and innovative solutions using digital twins and blockchain. This research highlights the importance of understanding and reducing credit card fraud to protect consumers and financial institutions. The study provides a detailed overview of credit card fraud analysis and categorizes its different types to clarify the threat landscape. It introduces a new digital twin approach to improve fraud detection. Digital twins are virtual replicas of physical systems that show promise for enhancing anomaly detection and behavioral analysis for more precise and timely fraud identification. In addition, the paper examines blockchain-enabled federated learning (BFL) as a decentralized method that uses blockchain’s security features to improve collaborative learning. By merging digital twins with federated learning (FL), the study presents a dynamic strategy for identifying known and emerging fraud patterns effectively. These advanced technologies represent a significant step forward in combating credit card fraud. Overall, the research not only focuses on creating more robust fraud detection systems but also emphasizes the importance of continuous innovation and adaptation to enhance financial security measures.
Keywords: Credit card fraud; Digital twin; Federated learning; Blockchain; Financial security; Anomaly detection

Gor Mack Diouf, Halima Elbiaze, Wael Jaafar,
On Byzantine fault tolerance in multi-master Kubernetes clusters,
Future Generation Computer Systems,
Volume 109,
2020,
Pages 407-419,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.03.060.
(https://www.sciencedirect.com/science/article/pii/S0167739X19307939)
Abstract: Docker container virtualization technology is being widely adopted in cloud computing environments because of its lightweight and efficiency. However, it requires adequate control and management via an orchestrator. As a result, cloud providers are adopting the open-access Kubernetes platform as the standard orchestrator of containerized applications. To ensure applications’ availability in Kubernetes, the latter uses Raft protocol’s replication mechanism. Despite its simplicity, Raft assumes that machines fail only when shutdown. This failure event is rarely the only reason for a machine’s malfunction. Indeed, software errors or malicious attacks can cause machines to exhibit Byzantine (i.e. random) behavior and thereby corrupt the accuracy and availability of the replication protocol. In this paper, we propose a Kubernetes multi-Master Robust (KmMR) platform to overcome this limitation. KmMR is based on the adaptation and integration of the BFT-SMaRt fault-tolerant replication protocol into Kubernetes environment. Unlike Raft protocol, BFT-SMaRt is resistant to both Byzantine and non-Byzantine faults. Experimental results show that KmMR is able to guarantee the continuity of services, even when the total number of tolerated faults is exceeded. In addition, KmMR provides on average a consensus time 1000 times shorter than that achieved by the conventional platform (with Raft), in such condition. Finally, we show that KmMR generates a small additional cost in terms of resource consumption compared to the conventional platform.
Keywords: Cloud computing; Docker containers; Kubernetes; Byzantine and non-Byzantine faults; Fault tolerance; Service continuity

Kallia Chronaki, Miquel Moretó, Marc Casas, Alejandro Rico, Rosa M. Badia, Eduard Ayguadé, Mateo Valero,
On the maturity of parallel applications for asymmetric multi-core processors,
Journal of Parallel and Distributed Computing,
Volume 127,
2019,
Pages 105-115,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.01.007.
(https://www.sciencedirect.com/science/article/pii/S0743731519300267)
Abstract: Asymmetric multi-cores (AMCs) are a successful architectural solution for both mobile devices and supercomputers. By maintaining two types of cores (fast and slow) AMCs are able to provide high performance under the facility power budget. This paper performs the first extensive evaluation of how portable are the current HPC applications for such supercomputing systems. Specifically we evaluate several execution models on an ARM big.LITTLE AMC using the PARSEC benchmark suite that includes representative highly parallel applications. We compare schedulers at the user, OS and runtime levels, using both static and dynamic options and multiple configurations, and assess the impact of these options on the well-known problem of balancing the load across AMCs. Our results demonstrate that scheduling is more effective when it takes place in the runtime system level as it improves the baseline by 23%, while the heterogeneous-aware OS scheduling solution improves the baseline by 10%.
Keywords: Parallel programming; Scheduling; Runtime systems; Asymmetric multi-cores; HPC

Rolando Martins, Manuel E. Correia, Luís Antunes, Fernando Silva,
Iris: Secure reliable live-streaming with opportunistic mobile edge cloud offloading,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 272-292,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.06.011.
(https://www.sciencedirect.com/science/article/pii/S0167739X19301037)
Abstract: The ever-increasing demand for higher quality live streams is driving the need for better networking infrastructures, specially when disseminating content over highly congested areas, such as stadiums, concerts and museums. Traditional approaches to handle this type of scenario relies on a combination of cellular data, through 4G distributed antenna arrays (DAS), with a high count of WiFi (802.11) access points. This obvious requires a substantial upfront cost for equipment, planning and deployment. Recently, new efforts have been introduced to securely leverage the capabilities of wireless multipath, including WiFi multicast, 4G, and device-to-device communications. In order to solve these issues, we propose an approach that lessens the requirements imposed on the wireless infrastructures while potentially expanding wireless coverage through the crowd-sourcing of mobile devices. In order to achieve this, we propose a novel pervasive approach that combines secure distributed systems, WiFi multicast, erasure coding, source coding and opportunistic offloading that makes use of hyperlocal mobile edge clouds. We empirically show that our solution is able to offer a 11 fold reduction on the infrastructural WiFi bandwidth usage without having to modify any existing software or firmware stacks while ensuring stream integrity, authorization and authentication.
Keywords: Mobile edge-clouds; Trusted cloudlets; Security; Reliability; Opportunistic offloading; Crowdsourcing; WiFi multicast; Network coding; Multipath network scheduling

Partha Pratim Ray,
A perspective on 6G: Requirement, technology, enablers, challenges and future road map,
Journal of Systems Architecture,
Volume 118,
2021,
102180,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102180.
(https://www.sciencedirect.com/science/article/pii/S1383762121001302)
Abstract: Mobile network operators are at the verge of distribution and allotment of existing mobile communications with 5G. It is a high time that we should be focused on the forthcoming sixth generation (6G) networking. Though, it is mandated that 6G would leverage best of the existing network functions and cover an extended geographical range, we need to first understand the importance of 6G. In this article, we discuss about the vision of 6G and elaborate how it should look like with relevant elaborations. Key objective of this paper is to present analysis of the crucial requirements to develop 6G network infrastructure. We present a list of important networking and communication technologies that shall indeed play the vital role to design 6G. We also present how key enablers of 6G shall highlight their significance to emerge 6G. The article also presents important use case scenarios which may be overserved in the 6G era. Lastly, we find open research challenges and discuss way outs. Discussion about future road map designing concludes this literature.
Keywords: 6G; Next generation communication; New radio; Wireless communications

Xiaoli Han, Bo Hu,
A congestion-aware user–cell association scheme to assist with traffic offloading in a three-tier heterogeneous network,
Computer Networks,
Volume 245,
2024,
110388,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110388.
(https://www.sciencedirect.com/science/article/pii/S1389128624002202)
Abstract: In this work, we aim to expand the cell range in congestion areas. To achieve this goal, we integrate UAV network into macro–micro cellular network to enhance coverage. However, the gain of deploying heterogeneous nodes is limit if there are no CRE (cell range expanding)-based method to steer users towards low-power nodes. Therefore, in order to further improve system access capacity, we propose a congestion-aware UA (user–cell association) scheme to offload queued traffic of cellular network into UAV network. To verify the superiority of proposed scheme, we utilize the low-complexity SG (stochastic-geometry) methods. Then in simulation analysis phase, we obtain five primary results: (a). The SG-based methods have lower computation complexity, and the convergence of derived expressions are verified. (b). The SG-based methods do not need the exact users’ location but the users’ density, which have improved the operability and robustness in infrastructure construction of wireless network. (c). The congestion-aware UA scheme can prompt network to adjust association tier flexibly, which is very suitable for traffic unevenly-distributed regions. (d). The optimal offsets of virtual power are estimated as B2 = −4 dB for small-cell and B3 = 3 dB for drone-cell, the resulting improvement in communication capacity is 8%. (e). We suggest the deployment density of UAV sites will no need to exceed λ3 = 41 (pks/km2).
Keywords: Traffic offloading; User–cell association scheme; Integrated aerial–terrestrial network; Stochastic geometry

Akshay Jain, Elena Lopez-Aguilera, Ilker Demirkol,
Are mobility management solutions ready for 5G and beyond?,
Computer Communications,
Volume 161,
2020,
Pages 50-75,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.07.016.
(https://www.sciencedirect.com/science/article/pii/S0140366419316032)
Abstract: Enabling users to move to different geographical locations within a network and still be able to maintain their connectivity and most essentially, continuity of service, is what makes any wireless network ubiquitous. Whilst challenging, modern day wireless networks, such as 3GPP-LTE, provision satisfactory mobility management (MM) performance. However, it is estimated that the number of mobile subscriptions will approximately touch 9 billion and the amount of data traffic will expand by 5 times in 2024 as compared to 2018. Further, it is expected that this trend of exponential growth will be maintained well into the future. To cope with such an exponential increase in cellular traffic and users alongside a burgeoning demand for higher Quality of Service (QoS), the future networks are expected to be highly dense and heterogeneous. This will severely challenge the existing MM solutions and ultimately render them ineffective as they will not be able to provide the required reliability, flexibility, and scalability. Consequently, to serve the 5G and beyond 5G networks, a new perspective to MM is required. Hence, in this article we present a novel discussion of the functional requirements from MM strategies for these networks. We then provide a detailed discussion on whether the existing mechanisms conceived by standardization bodies such as IEEE, IETF, 3GPP (including the newly defined 5G standards) and ITU, and other academic and industrial research efforts meet these requirements. We accomplish this via a novel qualitative assessment, wherein we evaluate each of the discussed mechanisms on their ability to satisfy the reliability, flexibility and scalability criteria for future MM strategies. We then present a study detailing the research challenges that exist in the design and implementation of MM strategies for 5G and beyond networks. Further, we chart out the potential MM solutions and the associated capabilities they offer to tackle the persistent challenges. We conclude this paper with a vision for the 5G and beyond MM mechanisms.
Keywords: 5G; Beyond 5G; 6G; Mobility management; SDN; Meta-surfaces

Wenhan Chang, Tianqing Zhu,
Gradient-based defense methods for data leakage in vertical federated learning,
Computers & Security,
Volume 139,
2024,
103744,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2024.103744.
(https://www.sciencedirect.com/science/article/pii/S0167404824000452)
Abstract: Research on federated learning has continued to develop over the past few years. Many federated learning algorithms and frameworks have been developed to ensure model accuracy and protect client data privacy, which has been extensively beneficial for the development of artificial intelligence security technology. However, it is possible to recover private training data from publicly shared gradients, which is referred to as a data leakage attack. In this paper, we propose two feasible defense methods, based on gradient sparsification and pseudo-gradient, to defend against the state-of-the-art attack methods and achieve maximum protection of the private data of all federated learning participants. Both methods use cosine similarity to measure the angular difference between the gradients updated by the clients during training and the gradients sent back by the server. Taking the cosine similarity as a reference and aiming to protect clients' privacy while maintaining the accuracy of the global model, the clients can choose an appropriate strategy for disguising their uploaded gradient. Through extensive experiments, we demonstrate that both defense methods can protect users' private data while preserving the accuracy of the global model in federated learning.
Keywords: Federated learning; Data leakage; Privacy preserving; Model security; Aggregation method

Sandeep Kumar, Diksha Moolchandani, Smruti R. Sarangi,
Hardware-assisted mechanisms to enforce control flow integrity: A comprehensive survey,
Journal of Systems Architecture,
Volume 130,
2022,
102644,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102644.
(https://www.sciencedirect.com/science/article/pii/S1383762122001643)
Abstract: Today, a vast amount of sensitive data worth millions of dollars is processed in untrusted data centers; hence, the confidentiality and integrity of the code and data are of paramount importance. Given the high incentive of mounting a successful attack, the complexity of attack methods has grown rapidly over the years. The attack methods rely on vulnerabilities present in the system to hijack the control flow of a process and use it to either steal sensitive information or degrade the quality of service. To thwart these attacks, the complexity of the defense methods has also increased in tandem. Researchers have explored different methods to ensure the secure execution of an application. The defense methods range from software-only to hardware-only to hybrid defense methods. In this survey, we focus on the relatively new hybrid form of defense methods where software and hardware work in tandem to protect the control flow of applications. We present a novel three-level taxonomy of these defense mechanisms based on first principles and use them to classify existing defense methods. After presenting the taxonomy, we critically analyze the proposed defense methods, study the evolution of the field and outline the challenges for future work.
Keywords: Control flow integrity; Hardware-assisted security; Code reuse attacks; Control flow bending attacks

Yong Tang, Ronghua Lin, Dingding Li, Yuguo Li, Deze Zeng,
FSbrain: An intelligent I/O performance tuning system,
Journal of Systems Architecture,
Volume 129,
2022,
102623,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102623.
(https://www.sciencedirect.com/science/article/pii/S1383762122001527)
Abstract: To bridge the performance gap between network and storage, performance tuning of file systems according to different workloads has become an important work on the Internet of Things (IoT). However, existing methods on file system tuning mainly focused on tailoring the file system kernel codes, or cannot scope to high dimensional parameters and dynamically changing workloads. In this paper, we propose an automatic I/O performance tuning system FSbrain that recommends reasonable and performance-efficient configurations for file systems by using machine learning. It makes the configuration recommendations based on current workloads within an acceptable time, however, without any manual interventions or kernel code modifications. In specific, FSbrain mainly consists of two phases which are model training and configuration tuning. Since tuning attempts will cause frequent remounting on target file systems, we deploy FSbrain and conduct the tuning sessions on an experimental node called “shadow server”. We evaluate FSbrain on three representative file systems (namely, Ext4, F2FS, and PMFS). The experimental results show that the configuration recommended by FSbrain can improve the I/O performance by up to 1.28× averagely than the default configuration. Besides, FSbrain reduces the overall tuning time by 90% compared to manual tuning.
Keywords: I/O performance tuning; File system; Machine learning; Internet of Things

Shishir Kumar Shandilya, Saket Upadhyay, Ajit Kumar, Atulya K. Nagar,
AI-assisted Computer Network Operations testbed for Nature-Inspired Cyber Security based adaptive defense simulation and analysis,
Future Generation Computer Systems,
Volume 127,
2022,
Pages 297-308,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.09.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003642)
Abstract: In the current ever-changing cybersecurity scenario, active cyber defense strategies are imperative. In this work, we present a standard testbed to measure the efficacy and efficiency of customized networks while analyzing various parameters during the active attack. The presented testbed can be used for analyzing the network behavior in presence of various types of attacks and can help in fine-tuning the proposed algorithm under observation. The proposed testbed will allow users to design, implement, and evaluate the active cyber defense mechanisms with good library support of nature-inspired and AI-based techniques. Network loads, number of clusters, types of home networks, and number of nodes in each cluster and network can be customized. While using the presented testbed and incorporating active-defense strategies on existing network architectures, users can also design and propose new network architectures for effective and safe operation. In this paper, we propose a unified and standard testbed for cyber defense strategy simulation and bench-marking, which would allow the users to investigate current approaches and compare them with others, while ultimately aiding in the selection of the best approach for a given network security situation. We have compared the network performance in difference scenarios namely, normal, under attack and under attack in presence of NICS-based adaptive defense mechanism and achieved stable experimental results. The experimental results clearly show that the proposed testbed is able to simulate the network conditions effectively with minimum efforts in network configuration. The simulation results of defense mechanisms verified on the proposed testbed got the improvement on almost 80 percent while increasing the turnaround time to 1–2 percent. The applicability of proposed testbed in modern technologies like Fog Computing and Edge Computing is also discussed in this paper.
Keywords: Nature-Inspired Cyber Security; Computer Network Operations; Cyber range; Adaptive cyber defense; Network simulation; Performance tuning

Deepika Suhag, Vivekanand Jha,
A comprehensive survey on mobile crowdsensing systems,
Journal of Systems Architecture,
Volume 142,
2023,
102952,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102952.
(https://www.sciencedirect.com/science/article/pii/S1383762123001315)
Abstract: In recent times, Mobile Crowdsensing (MCS) has garnered considerable attention and emerged as a promising sensing paradigm. The MCS approach leverages the capabilities of intelligent devices and human intelligence to collect and sense data. Moreover, the mobility of individuals and platform independence of MCS enables extensive coverage and contextual awareness, thereby providing valuable insights for various applications in the present era. However, these advancements also raise concerns about user privacy, network dynamics, data reliability, and data integrity. Over the years, researchers have proposed various solutions to address these issues while simultaneously enhancing MCS. In this paper, we extend the existing body of MCS research by providing a comprehensive survey on recent advancements. We present the MCS architecture from two perspectives and systematically categorize and classify MCS components. Additionally, we offer a taxonomy of incentive mechanisms, conduct a thorough analysis of privacy-preserving task allocation and truth discovery, and discuss potential research issues and existing solutions. Moreover, the paper presents the broader categorization of MCS applications. Furthermore, we examine existing platforms, simulators, and operating systems for MCS applications. The objective of this paper is not only to analyse and consolidate existing research but also to identify new opportunities for future research and establish connections with other research disciplines that can inspire further research endeavours in the MCS system and promote its advancement.
Keywords: Mobile Crowdsensing; Task Allocation; Incentive Mechanism; Differential Privacy; Cryptography; Blockchain; Edge Computing; Edge Intelligence

Aitor Arjona, Pedro García López, Josep Sampé, Aleksander Slominski, Lionel Villard,
Triggerflow: Trigger-based orchestration of serverless workflows,
Future Generation Computer Systems,
Volume 124,
2021,
Pages 215-229,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.06.004.
(https://www.sciencedirect.com/science/article/pii/S0167739X21001989)
Abstract: As more applications are being moved to the Cloud thanks to serverless computing, it is increasingly necessary to support the native life cycle execution of those applications in the data center. But existing cloud orchestration systems either focus on short-running workflows (like IBM Composer or Amazon Step Functions Express Workflows) or impose considerable overheads for synchronizing massively parallel jobs (Azure Durable Functions, Amazon Step Functions). None of them are open systems enabling extensible interception and optimization of custom workflows. We present Triggerflow: an extensible Trigger-based Orchestration architecture for serverless workflows. We demonstrate that Triggerflow is a novel serverless building block capable of constructing different reactive orchestrators (State Machines, Directed Acyclic Graphs, Workflow as code, Federated Learning orchestrator). We also validate that it can support high-volume event processing workloads, auto-scale on demand with scale down to zero when not used, and transparently guarantee fault tolerance and efficient resource usage when orchestrating long running scientific workflows.
Keywords: Event-based; Orchestration; Serverless

Vahid Arabnejad, Kris Bubendorfer, Bryan Ng,
Dynamic multi-workflow scheduling: A deadline and cost-aware approach for commercial clouds,
Future Generation Computer Systems,
Volume 100,
2019,
Pages 98-108,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.04.029.
(https://www.sciencedirect.com/science/article/pii/S0167739X18307386)
Abstract: Cloud computing, specifically its elastic, on demand, and pay per use instances, provide an ideal model for resourcing large scale state-of-the-art scientific analyses. Such scientific work is typically represented as workflows — the most common model for characterizing e-Science experiments and data analysis. Hosting and managing scientific applications on the cloud poses new challenges in terms of workflow scheduling which is key in leveraging its inherent cost and performance benefits. Prior research has studied static scheduling when the number of workflows is known in advance and all are submitted at the same time. However, in practice, a scheduler may have to schedule an unpredictable stream of workflows, for example, recent workflow management systems — such as Parsl, do not construct complete workflows at any stage during their execution, rather they generate partial workflows dynamically during execution — somewhat akin to lazy evaluation. This change in the way in which scientific data and workflows are created and processed represents a disruptive change to the way in which scheduling needs to occur. This paper represents a first and necessary step towards addressing scheduling problems of this nature, in which we present a new algorithm, Dynamic Workload Scheduler (DWS) that handles the dynamics of multiple deadline constrained workflows arriving randomly and scheduling these workflows with reducing cost in mind. Our results show that the DWS algorithm achieves an average 10% higher success rate in terms of fulfilling deadlines for different workloads and reduces the overall cost by an average 23% when compared to the most recent comparable algorithm.

Zeqin Wang, Ming Wen, Yuedong Xu, Yipeng Zhou, Jessie Hui Wang, Liang Zhang,
Communication compression techniques in distributed deep learning: A survey,
Journal of Systems Architecture,
Volume 142,
2023,
102927,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2023.102927.
(https://www.sciencedirect.com/science/article/pii/S1383762123001066)
Abstract: Nowadays, the training data and neural network models are getting increasingly large. The training time of deep learning will become unbearably long on a single machine. To reduce the computation and storage burdens, distributed deep learning has been put forward to collaboratively train a large neural network model with multiple computing nodes in parallel. The unbalanced development of computation and communication capabilities has led to training time being dominated by communication time, making the communication overhead a major challenge toward efficient distributed deep learning. Communication compression is an effective method to alleviate communication overhead, and it has evolved from simple random sparsification or quantization to versatile strategies or data structures. In this survey, existing communication compression techniques are reviewed and classified to provide a bird’s eye view. The main properties of each class of compression methods are analyzed, and their applications or theoretical convergence are described if necessary. This survey is potentially helpful for researchers and engineers to understand the up-to-date achievements on the communication compression techniques that accelerate the training of large deep learning models.
Keywords: Distributed deep learning; Communication compression; Sparsification; Quantization

Songshi Dou, Li Qi, Zehua Guo,
Mitigating the impact of controller failures on QoS robustness for software-defined wide area networks,
Computer Networks,
Volume 238,
2024,
110096,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110096.
(https://www.sciencedirect.com/science/article/pii/S1389128623005418)
Abstract: Emerging cloud services and applications pose different Quality of Service (QoS) requirements for the network, where Software-Defined Wide Area Networks (SD-WANs) play a crucial role in QoS provisioning by introducing network programmability into network flows to enable dynamic flow routing and ensure low data transmission latency for these applications. However, controller failures may happen in SD-WANs, and all programmable flows that the failed controller previously controlled will become offline and lose the network programmability, resulting in the degradation of QoS. Existing control recovery solutions propose to remap offline switches/flows to available active controllers but cannot promise good recovery performance due to the following two problems: (1) the recovery performance suffers from either coarse-grained remapping granularity or introducing extra processing delays, and (2) QoS robustness cannot be guaranteed in the design of recovery solution. To this end, we propose Predator, a QoS-aware network programmability recovery scheme that utilizes the P4 Runtime enabled by existing P4 switches to achieve fine-grained per-flow remapping without introducing extra delays. Specifically, our proposed Predator categorizes flows based on their QoS requirements and smartly recovers offline flows based on their priorities to guarantee the QoS robustness for high-priority flows. Simulation results under real-world topology demonstrate that our proposed Predator can improve the recovered network programmability of high-priority flows by up to 505.5%, and substantially reduce the communication overhead of high-priority flows, compared with baselines.
Keywords: Software-defined wide area networks; Quality of Service; Network programmability

Arzoo Miglani, Neeraj Kumar,
Blockchain management and machine learning adaptation for IoT environment in 5G and beyond networks: A systematic review,
Computer Communications,
Volume 178,
2021,
Pages 37-63,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.07.009.
(https://www.sciencedirect.com/science/article/pii/S0140366421002632)
Abstract: Keeping in view of the constraints and challenges with respect to big data analytics along with security and privacy preservation for 5G and B5G applications, the integration of machine learning and blockchain, two of the most promising technologies of the modern era is inevitable. In comparison to the traditional centralized techniques for security and privacy preservation, blockchain uses decentralized consensus algorithms for verification and validation of different transactions which are supposed to become an integral part of blockchain network. Starting with the existing literature survey, we introduce the basic concepts of blockchain and machine learning in this article. Then, we presented a comprehensive taxonomy for integration of blockchain and machine learning in an IoT environment. We also explored federated learning, reinforcement learning, deep learning algorithms usage in blockchain based applications. Finally, we provide recommendations for future use cases of these emerging technologies in 5G and B5G technologies.
Keywords: Blockchain; Machine learning; Federated learning; Internet of Things; Deep learning; 5G; 6G

Faheem Ullah, Shagun Dhingra, Xiaoyu Xia, M. Ali Babar,
Evaluation of distributed data processing frameworks in hybrid clouds,
Journal of Network and Computer Applications,
Volume 224,
2024,
103837,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2024.103837.
(https://www.sciencedirect.com/science/article/pii/S1084804524000146)
Abstract: Distributed data processing frameworks (e.g., Hadoop, Spark, and Flink) are widely used to distribute data among computing nodes of a cloud. Recently, there have been increasing efforts aimed at evaluating the performance of distributed data processing frameworks hosted in private and public clouds. However, there is a paucity of research on evaluating the performance of these frameworks hosted in a hybrid cloud, which is an emerging cloud model that integrates private and public clouds to use the best of both worlds. Therefore, in this paper, we evaluate the performance of Hadoop, Spark, and Flink in a hybrid cloud in terms of execution time, resource utilization, horizontal scalability, vertical scalability, and cost. For this study, our hybrid cloud consists of OpenStack (private cloud) and MS Azure (public cloud). We use both batch and iterative workloads for the evaluation. Our results show that in a hybrid cloud (i) the execution time increases as more nodes are borrowed by the private cloud from the public cloud, (ii) Flink outperforms Spark, which in turn outperforms Hadoop in terms of execution time, (iii) Hadoop transfers the largest amount of data among the nodes during the workload execution while Spark transfers the least amount of data, (iv) all three frameworks horizontally scale better as compared to vertical scaling, and (v) Spark is found to be least expensive in terms of $ cost for data processing while Hadoop is found the most expensive.
Keywords: Hybrid cloud; Hadoop; Spark; Flink; Big data

Gabriele Mencagli, Massimo Torquati, Dalvan Griebler, Alessandra Fais, Marco Danelutto,
General-purpose data stream processing on heterogeneous architectures with WindFlow,
Journal of Parallel and Distributed Computing,
Volume 184,
2024,
104782,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.104782.
(https://www.sciencedirect.com/science/article/pii/S0743731523001521)
Abstract: Many emerging applications analyze data streams by running graphs of communicating tasks called operators. To develop and deploy such applications, Stream Processing Systems (SPSs) like Apache Storm and Flink have been made available to researchers and practitioners. They exhibit imperative or declarative programming interfaces to develop operators running arbitrary algorithms working on structured or unstructured data streams. In this context, the interest in leveraging hardware acceleration with GPUs has become more pronounced in high-throughput use cases. Unfortunately, GPU acceleration has been studied for relational operators working on structured streams only, while non-relational operators have often been overlooked. This paper presents WindFlow, a library supporting the seamless GPU offloading of general partitioned-stateful operators, extending the range of operators that benefit from hardware acceleration. Its design provides high throughput still exposing a high-level API to users compared with the raw utilization of GPUs in Apache Flink.
Keywords: Data stream processing; Heterogeneous architectures; System-on-chip devices; Multicores; GPUs

Roxana Gabriela Stan, Catalin Negru, Florin Pop,
CloudWave: Content gathering network with flying clouds,
Future Generation Computer Systems,
Volume 98,
2019,
Pages 474-486,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.03.033.
(https://www.sciencedirect.com/science/article/pii/S0167739X18327705)
Abstract: The necessity of achieving high streaming quality requires to combine the benefits of cloud computing with the verticals of a content delivery network into a robust, reliable, flexible and fault-tolerant system. This paper presents a solution for data acquisition, processing and Internet-enabled distribution of multimedia content. The objective is to propose an elastic content gathering network which handles media files delivered, for instance, by unmanned aerial vehicles, being further served on-demand to end users scattered over the globe. We have implemented the framework architecture, the system components with their attached responsibilities and capabilities, evaluating the performance based on the extensive simulations. The designed framework has been validated in terms of ensured correctness. Experimental results have proven the proper behavior of the built system by handling both types of requests, as for the storage of massive incoming data sets and for distributing content through multiple employed servers strategically placed in the proximity of the initiated requests’ locations. Furthermore, once an autonomous and scalable network has been successfully designed, the number of required surrogate servers dynamically adjusts to consume the multimedia services in a cost-efficient manner.
Keywords: Data-intensive workloads; Content delivery network; Scalability; Cloud computing; Cloud storage; QoS

Umar Ghafoor, Mudassar Ali, Humayun Zubair Khan, Adil Masood Siddiqui, Muhammad Naeem,
NOMA and future 5G & B5G wireless networks: A paradigm,
Journal of Network and Computer Applications,
Volume 204,
2022,
103413,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103413.
(https://www.sciencedirect.com/science/article/pii/S1084804522000728)
Abstract: For the last few decades, wireless communication has been facing a technological revolution. High data rate and continuous connectivity are the necessities because the technology has turned from simple voice communication to newly high interactive multimedia applications. Also, the demands for mobile devices are growing tremendously. Researchers are developing fifth-generation (5G) and beyond fifth-generation (B5G) wireless communication networks to fulfill the needs in the future. Non-orthogonal multiple access (NOMA) can be a promising scheme to meet the demands of an enormously growing number of users, connectivity requirements, low-cost requirements, limited bandwidth requirements, and high coverage requirements for future wireless communication networks. NOMA-assisted wireless communication networks have to face several challenges along with several benefits. In the previous survey papers, the main focus of the researchers was on the concepts of NOMA, its comparison with other techniques, and issues related to NOMA. This paper presents a thorough survey on NOMA and future 5G and B5G wireless communication networks. We have arranged the paper in such a way that conveys all the aspects of 5G and B5G networks and NOMA’s application in 5G and B5G. This paper includes the study of existing survey papers, comparison with our paper, our contribution, uniqueness, and benefits of our paper. It includes requirements and technologies for 5G and B5G, channel modeling, the role of NOMA in 5G and B5G, types of NOMA, NOMA’s network architecture, mobility management (MM) in NOMA, asynchronous and synchronous operations in NOMA, energy and green aspects of NOMA in 5G and B5G, NOMA’s challenges, solutions to these challenges, NOMA’s performance indicators. This paper also includes the problems of resource allocation in 5G and B5G, role of NOMA in improving resource allocation and future research directions for next-generation wireless communication networks. This paper also discusses cloud virtualization (CV), fog networking (FN), and edge networking (EN), enterprise and industry 4.0 perspectives, the importance of standardization of NOMA, third generation partnership project (3GPP) standards for NOMA. This paper also highlights the security, blockchain aspects, practicality, and industry acceptance for NOMA in 5G and B5G.
Keywords: NOMA; Recent communication technologies; Resource allocation; 5G; B5G

PanJun Sun,
Security and privacy protection in cloud computing: Discussions and challenges,
Journal of Network and Computer Applications,
Volume 160,
2020,
102642,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102642.
(https://www.sciencedirect.com/science/article/pii/S1084804520301168)
Abstract: With the development of cloud computing, privacy security issues have become increasingly prominent, which is of concern to industry and academia. We review the research progress on privacy security issues from the perspective of several privacy security protection technologies in cloud computing. First, we introduce some privacy security risks of cloud computing and propose a comprehensive privacy security protection framework. Second, we show and discuss the research progress of several technologies, such as access control; ciphertext policy attribute-based encryption (CP-ABE); key policy attribute-based encryption (KP-ABE); the fine-grain, multi-authority, revocation mechanism; the trace mechanism; proxy re-encryption (PRE); hierarchical encryption; searchable encryption (SE); and multi-tenant, trust, and a combination of multiple technologies, and then compare and analyze the characteristics and application scope of typical schemes. Last, we discuss current challenges and highlight possible future research directions.
Keywords: Cloud computing; Privacy security; Access control; Attribute-based encryption; Trust

Huanzhou Zhu, Ligang He, Matthew Leeke, Rui Mao,
WolfGraph: The edge-centric graph processing on GPU,
Future Generation Computer Systems,
Volume 111,
2020,
Pages 552-569,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.052.
(https://www.sciencedirect.com/science/article/pii/S0167739X18325251)
Abstract: There is the significant interest nowadays in developing the frameworks for parallelizing the processing of large graphs such as social networks, web graphs, etc. The work has been proposed to parallelize the graph processing on clusters (distributed memory), multicore machines (shared memory) and GPU devices. Most existing research on GPU-based graph processing employs the vertex-centric processing model and the Compressed Sparse Row (CSR) form to store and process a graph. However, they suffer from irregular memory access and load imbalance in GPU, which hampers the full exploitation of GPU performance. In this paper, we present WolfGraph, a GPU-based graph processing framework that addresses the above problems. WolfGraph adopts the edge-centric processing, which iterates over the edges rather than vertices. The data structure and graph partition in WolfGraph are carefully crafted so as to minimize the graph pre-processing and allow the coalesced memory access. WolfGraph fully utilizes the GPU power by processing all edges in parallel. We also develop a new method, called Concatenated Edge List (CEL), to process a graph that is bigger than the global memory of GPU. WolfGraph allows the users to define their own graph-processing methods and plug them into the WolfGraph framework. Our experiments show that WolfGraph achieves 7-8x speedup over GraphChi and X-Stream when processing large graphs, and it also offers 65% performance improvement over the existing GPU-based, vertex-centric graph processing frameworks, such as Gunrock.
Keywords: GPGPU; Graph processing; CUDA; Parallel processing

Mobasshir Mahbub,
Progressive researches on IoT security: An exhaustive analysis from the perspective of protocols, vulnerabilities, and preemptive architectonics,
Journal of Network and Computer Applications,
Volume 168,
2020,
102761,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102761.
(https://www.sciencedirect.com/science/article/pii/S1084804520302356)
Abstract: The IoT is the upcoming one of the major networking technologies. Using the IoT, different items or devices can be allowed to continuously generate, obtain, and exchange information. Different IoT applications nowadays are centered on computerizing various errands and are attempting to engage the inanimate physical items to act without direct supervision of a human. The current and forthcoming IoT services are exceptionally encouraging to build the degree of solace, proficiency, and automation for the clients. To obtain the option to actualize such a world in a continuously developing manner requires high security, protection, verification, and recuperation from assaults. Right now, incorporating the requisite changes in IoT systems engineering to achieve end-to-end, stable IoT infrastructure is paramount. In this research, a comprehensive analysis is incorporated into the security-relevant problems and threat wellsprings in IoT resources or applications. Specific that and current advancements based on maintaining a high degree of confidence in IoT apps are addressed while looking at the security issues. Four distinct developments are investigated, including cryptography, fog computing, edge computing, and ML (Machine Learning), to extend the degree of IoT security.
Keywords: IoT; Vulnerability; Security; Cryptography; Fog computing; Edge computing; Machine learning

Iacovos Ioannou, Prabagarane Nagaradjane, Pelin Angin, Palaniappan Balasubramanian, Karthick Jeyagopal Kavitha, Palani Murugan, Vasos Vassiliou,
GEMLIDS-MIOT: A Green Effective Machine Learning Intrusion Detection System based on Federated Learning for Medical IoT network security hardening,
Computer Communications,
Volume 218,
2024,
Pages 209-239,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2024.02.023.
(https://www.sciencedirect.com/science/article/pii/S0140366424000793)
Abstract: The increasing use of Internet of Things (IoT) gadgets in a daily rate has heightened security apprehension, particularly within the healthcare sector. In order to prevent the unauthorized disclosure of sensitive data, it is imperative for Internet of Things (IoT) systems to promptly and effectively respond to harmful activities. Nevertheless, the act of transferring data to distant cloud servers for the purpose of analysis gives rise to both temporal delays and apprehensions regarding privacy. To ensure the security of medical Internet of Things (MIoT) networks, a power-efficient Intrusion Detection System (IDS) is employed for three primary objectives that it will result in three stages of execution: (i) The objective is to categorize different types of attacks, such as Man-in-the-Middle (MitM) and Distributed Denial of Service (DDoS), by utilizing well-established machine learning (ML) techniques. This classification stage will serve to enhance the Intrusion Detection System (IDS) and the reporting system. (ii) Anomaly detection (unknown attack identification), or detection of unknown attacks, will be employed to identify previously unknown attacks. This identification stage involves retraining the ML model to enable future recognition and classification of these unknown attacks when the anomaly attack detector identifies that an unknown attack is recognized. Then, a retraining of the first stage classification model is executed due to the anomaly detection. (iii) To ensure that a remote cloud server remains current with the latest classification model changes, Federated Learning (FL) will be utilized. FL allows for collaborative model training while preserving data privacy and security. The experimental findings indicate that the Enhanced Random Forest (also called ensemble random forest) algorithm achieves a remarkable accuracy rate of 99.98% in classifying attacks. Thus, it will be our first stage classifier. Continuing, the One-Class Support Vector Machine (SVM) algorithm demonstrates a high level of accuracy, reaching 99.7% in detecting anomalies so that it will be our second stage identifier. Finally, the third-stage approach, which has as a target the overall system model updater, will be our introduced Federated Learning approach that works with the Enhanced Random Forests and identifies the ERF differences from the old model in an optimal way. The efficacy of our technique is confirmed through the implementation of experiments involving an Internet of Things (IoT) system and a Raspberry Pi MIoT gateway and with simulations that simulate the FL updating process. These experiments successfully identify known and unknown attacks with a high reliability level while limiting resource utilization and energy consumption. Future studies of this work will focus on enhancing the scalability and efficiency of our Intrusion Detection System in MIoT networks.
Keywords: Brute force authentication; DDoS; Green computing; Intrusion detection; MIoT; IoMT; Machine learning; MQTT; MitM; Raspberry Pi; Federated learning

Arzoo Miglani, Neeraj Kumar, Vinay Chamola, Sherali Zeadally,
Blockchain for Internet of Energy management: Review, solutions, and challenges,
Computer Communications,
Volume 151,
2020,
Pages 395-418,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.014.
(https://www.sciencedirect.com/science/article/pii/S0140366419314951)
Abstract: After smart grid, Internet of Energy (IoE) has emerged as a popular technology in the energy sector by integrating different forms of energy. IoE uses Internet to collect, organize, optimize and manage the networks energy information from different edge devices in order to develop a distributed smart energy infrastructure. Sensors and communication technologies are used to collect data and to predict demand and supply by consumers and suppliers respectively. However, with the development of renewable energy resources, Electric Vehicles (EVs), smart grid and Vehicle-to-grid (V2G) technology, the existing energy sector started shifting towards distributed and decentralized solutions. Moreover, the security and privacy issues because of centralization is another major concern for IoE technology. In this context, Blockchain technology with the features of automation, immutability, public ledger facility, irreversibility, decentralization, consensus and security has been adopted in the literature for solving the prevailing problems of centralized IoE architecture. By leveraging smart contracts, blockchain technology enables automated data exchange, complex energy transactions, demand response management and Peer-to-Peer (P2P) energy trading etc. Blockchain will play vital role in the evolution of the IoE market as distributed renewable resources and smart grid network are being deployed and used. We discuss the potential and applications of blockchain in the IoE field. This article is build on the literature research and it provides insight to the end-user regarding the future IoE scenario in the context of blockchain technology. Lastly this article discusses the different consensus algorithm for IoE technology.
Keywords: Consensus algorithm; Blockchain; Internet of Energy; Smart grid; Vehicle-to-grid

Amirhossein Shahbazinia, Saber Salehkaleybar, Matin Hashemi,
ParaLiNGAM: Parallel causal structure learning for linear non-Gaussian acyclic models,
Journal of Parallel and Distributed Computing,
Volume 176,
2023,
Pages 114-127,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2023.01.007.
(https://www.sciencedirect.com/science/article/pii/S0743731523000138)
Abstract: One of the key objectives in many fields in machine learning is to discover causal relationships among a set of variables from observational data. In linear non-Gaussian acyclic models (LiNGAM), it can be shown that the true underlying causal structure can be identified uniquely from merely observational data. The DirectLiNGAM algorithm is a well-known solution to learn the true causal structure in a high dimensional setting. DirectLiNGAM algorithm executes in a sequence of iterations and it performs a set of comparisons between pairs of variables in each iteration. Unfortunately, the runtime of this algorithm grows significantly as the number of variables increases. In this paper, we propose a parallel algorithm, called ParaLiNGAM, to learn casual structures based on DirectLiNGAM algorithm. We propose a threshold mechanism that can reduce the number of comparisons remarkably compared with the sequential solution. Moreover, in order to further reduce runtime, we employ a messaging mechanism between workers. We also present an implementation of ParaLiNGAM on GPU, considering hardware constraints. Experimental results on synthetic and real data show that our proposed solution outperforms DirectLiNGAM by a factor up to 4788X, and by a median of 2344X.
Keywords: Causal discovery; GPU acceleration; Machine learning; Parallel processing; DirectLiNGAM algorithm

Binghao Yan, Qinrang Liu, JianLiang Shen, Dong Liang, Bo Zhao, Ling Ouyang,
A survey of low-latency transmission strategies in software defined networking,
Computer Science Review,
Volume 40,
2021,
100386,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100386.
(https://www.sciencedirect.com/science/article/pii/S1574013721000265)
Abstract: Software-defined networking (SDN), as a revolutionary networking paradigm, provides a new solution for future network development and equipment manufacturing by separating the control plane from the data plane to make the management model more simple and efficient. However, in the era of Industry 4.0 and Big Data, data transmission latency is beginning to replace traditional performance requirements, such as stability and flexibility, as the primary design consideration for network applications and users with more stringent end-to-end latency. Especially the high latency in the delivery of key information not only makes it difficult to provide high-quality services, but also restricts the development of high-tech industries such as the Internet of Vehicles (IoV) and the Internet of Things (IoT) severely. In this paper, with the goal of low-latency communication under the SDN architecture, we review and analyze relevant technologies and research findings in terms of traffic management, congestion control, load balancing, and flow table management in SDN. In addition, the contributions of some of the currently popular emerging network technologies to SDN low latency communications are listed for discussion. Finally, attention is given to the challenges that remain to be addressed and the next research trends for a promising way forward. This paper aims to provide interested researchers with insights to promote their ongoing research in low-latency communications, so as to obtain more achievements.
Keywords: Software defined network; Low-latency transmission; Traffic management; Congestion control; Load balancing; Flow table management

Ning Wang, Zhigang Wang, Yu Gu, Yubin Bao, Ge Yu,
TSH: Easy-to-be distributed partitioning for large-scale graphs,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 804-818,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.06.033.
(https://www.sciencedirect.com/science/article/pii/S0167739X17327723)
Abstract: The big graph era is coming with strong and ever-growing demands on parallel iterative analysis. But, before that, balanced graph partitioning is a fundamental problem and is NP-complete. Till now, there have been several streaming heuristic solutions with a single full scan over the input graph. However, some of them cannot be easily parallelized to further accelerate partitioning for large-scale graphs due to complicated heuristics; while others can be run in parallel but incur expensive communication costs during iterative computation. This paper presents Target-vertex Sensitive Hash (TSH), an easy-to-be distributed partitioning method. We first analyze the locality property naturally provided by the original input graph, which has not yet been considered by existing work. We then exploit such locality to simplify the heuristic rule. The simplified rule is implemented by a two-step framework where target vertices of edges are first logically pre-divided without accessing any graph data and then, based on the distribution of target vertices, streaming partitioning is physically performed in parallel. TSH provides the capability of quickly dividing large-scale graphs because of parallelization, as well as optimizes communication overheads due to the utilization of locality. Using a broad spectrum of real-world graphs, we conduct extensive performance studies to confirm the effectiveness of TSH over up-to-date competitors.
Keywords: Distributed iterative computation; Large-scale graph; Streaming partitioning; Locality

M. Gharbaoui, C. Contoli, G. Davoli, D. Borsatti, G. Cuffaro, F. Paganelli, W. Cerroni, P. Cappanera, B. Martini,
An experimental study on latency-aware and self-adaptive service chaining orchestration in distributed NFV and SDN infrastructures,
Computer Networks,
Volume 208,
2022,
108880,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108880.
(https://www.sciencedirect.com/science/article/pii/S1389128622000822)
Abstract: Network Function Virtualization (NFV) and Software Defined Networking (SDN) changed radically the way 5G networks will be deployed and services will be delivered to vertical applications (i.e., through dynamic chaining of virtualized functions deployed in distributed clouds to best address latency requirements). In this work, we present a service chaining orchestration system, namely LASH-5G, running on top of an experimental set-up that reproduces a typical 5G network deployment with virtualized functions in geographically distributed edge clouds. LASH-5G is built upon a joint integration effort among different orchestration solutions and cloud deployments and aims at providing latency-aware, adaptive and reliable service chaining orchestration across clouds and network resource domains interconnected through SDN. In this paper, we provide details on how this orchestration system has been deployed and it is operated on top of the experimentation infrastructure provided within the Fed4FIRE+ facility and we present performance results assessing the effectiveness of the proposed orchestration approach.
Keywords: SDN; NFV; Orchestration; Federated testbed; Service chaining

Dinh C. Nguyen, Pubudu N. Pathirana, Ming Ding, Aruna Seneviratne,
Blockchain for 5G and beyond networks: A state of the art survey,
Journal of Network and Computer Applications,
Volume 166,
2020,
102693,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102693.
(https://www.sciencedirect.com/science/article/pii/S1084804520301673)
Abstract: The fifth generation (5G) wireless networks are on the way to be deployed around the world. The 5G technologies target to support diverse vertical applications by connecting heterogeneous devices and machines with drastic improvements in terms of high quality of service, increased network capacity and enhanced system throughput. However, 5G systems still remain a number of security challenges that have been mentioned by researchers and organizations, including decentralization, transparency, risks of data interoperability, and network privacy vulnerabilities. Furthermore, the conventional techniques may not be sufficient to deal with the security requirements of 5G. As 5G is generally deployed in heterogeneous networks with massive ubiquitous devices, it is quite necessary to provide secure and decentralized solutions. Motivated from these facts, in this paper we provide a state-of-the-art survey on the integration of blockchain with 5G networks and beyond. In this detailed survey, our primary focus is on the extensive discussions on the potential of blockchain for enabling key 5G technologies, including cloud computing, edge computing, Network Function Virtualization, Network Slicing, and D2D communications. We then explore and analyse the opportunities that blockchain potentially empowers important 5G services, ranging from spectrum management, data sharing, network virtualization, resource management to interference management, federated learning, privacy and security provision. The recent advances in the applications of blockchain in 5G Internet of Things are also surveyed in a wide range of popular use-case domains, such as smart healthcare, smart city, smart transportation, smart grid and UAVs. The main findings derived from the comprehensive survey on the cooperated blockchain-5G networks and services are then summarized, and possible research challenges with open issues are also identified. Lastly, we complete this survey by shedding new light on future directions of research on this newly emerging area.
Keywords: 5G networks; Blockchain; 5G Internet of Things; 5G services; Machine learning; Security and privacy

Muhammad Asim Shahid, Noman Islam, Muhammad Mansoor Alam, M.S. Mazliham, Shahrulniza Musa,
Towards Resilient Method: An exhaustive survey of fault tolerance methods in the cloud computing environment,
Computer Science Review,
Volume 40,
2021,
100398,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100398.
(https://www.sciencedirect.com/science/article/pii/S1574013721000381)
Abstract: Fault Tolerance (FT) is one of the cloud’s very critical problems for providing security assistance. Due to the diverse service architecture, detailed architectures & multiple interrelationships that occur in the cloud, implementation is complicated. A few other previous studies attempt to integrate the different fault tolerance frameworks and solutions suggested for the cloud environment, but some accounts occur to be constrained. This research article provides a detailed survey of the state-of-the-art research on emerging methods of fault tolerance for Cloud Computing & categorizes techniques of fault tolerance into three categories: Reactive Methods, Proactive Methods & Resilient Methods. Reactive Methods allow the system to reach a defect condition but instead attempt to get the device back up. Proactive Methods help to prevent the device from reaching a defective condition by introducing actions to minimize defects before impacting the device. On either side, newly developed Resilient Methods strive to reduce the amount of time a device takes to find from a malfunction. In the Resilient Methods context, Machine Learning and Artificial Intelligence played an important role in mapping the recovery period to a task to be configured. This survey offers a comprehensive and detailed description of the various faults kinds, factors, & different methods to fault tolerance used in the cloud. In light of their Basic Methods & some other specific characteristics, & also offers a Clear study of different tolerance mechanisms for failures & provides a comparative study of the structures under the article. It is noted that approaches of fault tolerance directed to checkpoint restart and replication are mainly included to address the crash faults in the cloud.
Keywords: Cloud Computing; Fault & failures; Fault tolerance frameworks; Methods; Schemes & techniques

U.S.P. Srinivas Aditya, Roshan Singh, Pranav Kumar Singh, Anshuman Kalla,
A Survey on Blockchain in Robotics: Issues, Opportunities, Challenges and Future Directions,
Journal of Network and Computer Applications,
Volume 196,
2021,
103245,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103245.
(https://www.sciencedirect.com/science/article/pii/S1084804521002435)
Abstract: Robotics is the multi-disciplinary domain that is booming in today’s world, and expanding its roots deep into various fields of research, manufacturing industries, healthcare, and even in our day-to-day lives. Nevertheless, as with any other evolving technology, robotics face numerous challenges. In this context, lately, blockchain technology has been identified as a promising technology to resolve many of these issues such as identification of malicious/rogue nodes, malfunctioning/faults in automated processes, non-compliance to the agreed norms and privacy rules, security attacks on robotic systems, and non-transparency in performance monitoring and audits. In particular, blockchain with its features like decentrality, immutability, provenance, low operational cost, tight access control, and trustworthy operations, can offer significant improvements to new applications and use cases driven by robotics. Thus, the paper begins with exploring the key requirements and technical challenges encountered by robots in general. Next, it provides detailed overview of blockchain technology in a tutorial style. Then, the role of blockchain for different uses cases of robotics are surveyed. Furthermore, various technical challenges that need to be mitigated to harness full potential of blockchain for robotics are highlighted. Finally, the future research directions are presented that can pave the way ahead for advancements and profitable integration of blockchain in the realm of robotics.
Keywords: Blockchain; Robotics; Smart contracts

Michel J.F. Rosa, Célia Ghedini Ralha, Maristela Holanda, Aleteia P.F. Araujo,
Computational resource and cost prediction service for scientific workflows in federated clouds,
Future Generation Computer Systems,
Volume 125,
2021,
Pages 844-858,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.07.030.
(https://www.sciencedirect.com/science/article/pii/S0167739X21002922)
Abstract: The use of cloud platforms has been widely encouraged in applications that require a lot of processing power and storage such as scientific workflows. However, users who operate such workflows are faced with a very large variety and quantity of available resources, specially considering cloud federation platforms, making difficult to choose efficiently. Thus, the design issue to operate such workflows is far from trivial resulting in a problem to scientific workflows’ users. In order to address this problem, we propose a provisioning service called CRCPs - Computational Resource and Cost Prediction service that measure user resources and report the runtime financial cost before starting the workflow execution. In addition, CRCPs allows users to choose between high-performance, low-budget executions, or set how much to pay and how long to finish the workflow in an automatic and transparent way. The results show the CRCPs adequacy to estimate resources, time and execution cost of two different bioinformatic workflows executed using BioNimbuZ federated cloud platform.
Keywords: Cloud computing; Federated clouds; Multiple linear regression; Public cloud platforms; Metaheuristic

Ahmad Hammoud, Hadi Otrok, Azzam Mourad, Zbigniew Dziong,
Stable federated fog formation: An evolutionary game theoretical approach,
Future Generation Computer Systems,
Volume 124,
2021,
Pages 21-32,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.05.021.
(https://www.sciencedirect.com/science/article/pii/S0167739X21001710)
Abstract: Instability within fog federations is considered as a serious problem that degrades the performance of the provided services. The latter may affect the service availability due to fog providers withdrawing their resources. It may either lead to failures for some users invocations, or to an increase in the number of tasks inside the servers’ processing-queue. Such a critical problem strips the fog paradigm from its main characteristic, the low latency factor. As far as we are aware, no work in the literature has addressed the problem of encountering unstable fog federations. Their main concerns were increasing the providers’ payoff regardless of their behavior. To address the aforementioned limitation, this work studies the stability of the federations through modeling the problem as an evolutionary game-theoretical model. Moreover, it devises a decentralized algorithm that implants the Replicator Dynamics model within which expresses the evolutionary dynamics. Experiments are conducted using EUA Datasets to simulate our algorithm and to show that it leads to an evolutionarily stable strategy over time, which stabilizes the federations and improves the Quality-of-Service for the users.
Keywords: Evolutionary game theory; Federated fog; Fog computing; Stability; IoT

Iacopo Colonnelli, Marco Aldinucci, Barbara Cantalupo, Luca Padovani, Sergio Rabellino, Concetto Spampinato, Roberto Morelli, Rosario Di Carlo, Nicolò Magini, Carlo Cavazzoni,
Distributed workflows with Jupyter,
Future Generation Computer Systems,
Volume 128,
2022,
Pages 282-298,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.10.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003976)
Abstract: The designers of a new coordination interface enacting complex workflows have to tackle a dichotomy: choosing a language-independent or language-dependent approach. Language-independent approaches decouple workflow models from the host code’s business logic and advocate portability. Language-dependent approaches foster flexibility and performance by adopting the same host language for business and coordination code. Jupyter Notebooks, with their capability to describe both imperative and declarative code in a unique format, allow taking the best of the two approaches, maintaining a clear separation between application and coordination layers but still providing a unified interface to both aspects. We advocate the Jupyter Notebooks’ potential to express complex distributed workflows, identifying the general requirements for a Jupyter-based Workflow Management System (WMS) and introducing a proof-of-concept portable implementation working on hybrid Cloud-HPC infrastructures. As a byproduct, we extended the vanilla IPython kernel with workflow-based parallel and distributed execution capabilities. The proposed Jupyter-workflow (Jw) system is evaluated on common scenarios for High Performance Computing (HPC) and Cloud, showing its potential in lowering the barriers between prototypical Notebooks and production-ready implementations.
Keywords: Distributed computing; Jupyter notebooks; Streamflow; Workflow management systems

Marcello Cinque, Domenico Cotroneo, Luigi De Simone, Stefano Rosiello,
Virtualizing mixed-criticality systems: A survey on industrial trends and issues,
Future Generation Computer Systems,
Volume 129,
2022,
Pages 315-330,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.12.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X21004787)
Abstract: Virtualization is gaining attraction in the industry as it promises a flexible way to integrate, manage, and re-use heterogeneous software components with mixed-criticality levels, on a shared hardware platform, while obtaining isolation guarantees. This work surveys the state-of-the-practice of real-time virtualization technologies by discussing common issues in the industry. In particular, we analyze how different virtualization approaches and solutions can impact isolation guarantees and testing/certification activities, and how they deal with dependability challenges. The aim is to highlight current industry trends and support industrial practitioners to choose the most suitable solution according to their application domains.
Keywords: Virtualization; Real-time applications; Mixed-criticality systems; Resource isolation; Safety certification; Dependability

Luis M. Vaquero, Felix Cuadrado, Yehia Elkhatib, Jorge Bernal-Bernabe, Satish N. Srirama, Mohamed Faten Zhani,
Research challenges in nextgen service orchestration,
Future Generation Computer Systems,
Volume 90,
2019,
Pages 20-38,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.07.039.
(https://www.sciencedirect.com/science/article/pii/S0167739X18303157)
Abstract: Fog/edge computing, function as a service, and programmable infrastructures, like software-defined networking or network function virtualisation, are becoming ubiquitously used in modern Information Technology infrastructures. These technologies change the characteristics and capabilities of the underlying computational substrate where services run (e.g. higher volatility, scarcer computational power, or programmability). As a consequence, the nature of the services that can be run on them changes too (smaller codebases, more fragmented state, etc.). These changes bring new requirements for service orchestrators, which need to evolve so as to support new scenarios where a close interaction between service and infrastructure becomes essential to deliver a seamless user experience. Here, we present the challenges brought forward by this new breed of technologies and where current orchestration techniques stand with regards to the new challenges. We also present a set of promising technologies that can help tame this brave new world.
Keywords: NVM; SDN; NFV; Orchestration; Large scale; Serverless; FaaS; Churn; Edge; Fog

Md. Mahmudul Islam, Muhammad Toaha Raza Khan, Malik Muhammad Saad, Dongkyun Kim,
Software-defined vehicular network (SDVN): A survey on architecture and routing,
Journal of Systems Architecture,
Volume 114,
2021,
101961,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101961.
(https://www.sciencedirect.com/science/article/pii/S1383762120302113)
Abstract: With the advances in automotive industry and telecommunication technologies, more and more vehicles are connected which paves the pathway to future intelligent transportation system (ITS). The safety of both the passengers and pedestrians has been a major concern in ITS. Vehicular ad hoc network (VANET) is a promising technology that can provide a wide range of services, such as safety, convenience, and infotainment for both the passengers and drivers. However, due to the dynamic nature of the vehicular environment, VANET has to deal with several challenges such as intermittent connectivity, provisioning of quality of service (QoS), and heterogeneity of applications. Moreover, the deployment of new services/protocols on a large-scale is a daunting task due to the inflexible architecture of VANET. The distinctive features of software-defined vehicular network (SDVN) such as programmability and flexibility leverages the vehicular network to satisfy the performance and management requirements of VANET. In this paper, we first present the taxonomy of SDVN architecture based on its modes of operation. Then we survey the state-of-the-art SDVN routing protocols and classify them based on different criteria. Finally, we highlight the challenges of current SDVNs.
Keywords: Software-defined networking; VANET; Software-defined vehicular ad hoc networks; Architecture; Routing protocols

Pegah Gazori, Dadmehr Rahbari, Mohsen Nickray,
Saving time and cost on the scheduling of fog-based IoT applications using deep reinforcement learning approach,
Future Generation Computer Systems,
Volume 110,
2020,
Pages 1098-1115,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.060.
(https://www.sciencedirect.com/science/article/pii/S0167739X19308702)
Abstract: Due to the rapid growth of intelligent devices and the Internet of Things (IoT) applications in recent years, the volume of data that is generated by these devices is increasing ceaselessly. Hence, moving all of these data to cloud datacenters would be impossible and would lead to more bandwidth usage, latency, cost, and energy consumption. In such cases, the fog layer would be the best place for data processing. In the fog layer, the computing equipment dedicates parts of its limited resources to process the IoT application tasks. Therefore, efficient utilization of computing resources is of great importance and requires an optimal and intelligent strategy for task scheduling. In this paper, we have focused on the task scheduling of fog-based IoT applications with the aim of minimizing long-term service delay and computation cost under the resource and deadline constraints. To address this problem, we have used the reinforcement learning approach and have proposed a Double Deep Q-Learning (DDQL)-based scheduling algorithm using the target network and experience replay techniques. The evaluation results reveal that our proposed algorithm outperforms some baseline algorithms in terms of service delay, computation cost, energy consumption and task accomplishment and also handles the Single Point of Failure (SPoF) and load balancing challenges.
Keywords: Fog computing; Task scheduling; Deep reinforcement learning; Double Q-Learning; Service delay; Computation cost

Stefano Di Mascio, Alessandra Menicucci, Eberhard Gill, Gianluca Furano, Claudio Monteleone,
Open-source IP cores for space: A processor-level perspective on soft errors in the RISC-V era,
Computer Science Review,
Volume 39,
2021,
100349,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2020.100349.
(https://www.sciencedirect.com/science/article/pii/S1574013720304494)
Abstract: This paper discusses principles and techniques to evaluate processors for dependable computing in space applications. The focus is on soft errors, which dominate the failure rate of processors in space. Error, failure and propagation models from literature are selected and employed to estimate the failure rate due to soft errors in typical processor designs. A similar approach can be followed for applications with different radiation environments (e.g. automotive, servers, experimental instrumentation exposed to radiation on ground), by adapting the error models. This detailed white-box analysis is possible only for open-source Intellectual Property (IP) cores and in this work it will be applied to several open-source IP cores based on the RISC-V Instruction Set Architecture (ISA). For these case studies, several types of redundancy described in literature for space processors will be evaluated in terms of their cost-effectiveness and expected final in-orbit behavior. This work provides a comprehensive framework to assess efficacy and cost-effectiveness of redundancy, instead of listing and categorizing the techniques described in literature without assessing their relevance to state-of-the-art designs in space applications.
Keywords: Processors; Fault tolerance; Space

Debanjan Roy Chowdhury, Sukumar Nandi, Diganta Goswami,
Video streaming over IoV using IP multicast,
Journal of Network and Computer Applications,
Volume 197,
2022,
103259,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103259.
(https://www.sciencedirect.com/science/article/pii/S1084804521002563)
Abstract: Video traffic over Internet of vehicles is increasing day by day to provide passengers on-board infotainment services. A video streaming service over Internet of vehicles involves design challenges of vehicular gateway selection, and vehicle to vehicle collaboration. Vehicular gateway selection is challenging due to high vehicle mobility and time-varying link conditions. Existing works on vehicular gateway selection focus on either service cost minimization, or quality of experience. To achieve a balance between these two aspects, we propose a novel vehicular gateway selection protocol that minimizes streaming service cost, keeping end-to-end delivery delay as a constraint. In our work, the vehicular gateway selection problem is formulated centrally as the minimum set-covering problem, and is solved by the greedy approximation method. In existing works, vehicle to vehicle collaboration happens in the application layer ignoring the presence of network layer nodes which do not run any proprietary application. To make the vehicle to vehicle collaboration in the network layer, we propose a novel multicast protocol SS-CAST specifically designed for streaming service over vehicular networks. Compared to existing protocols, the simulation results show VSIM as the clear winner for minimizing service cost while giving a competitive performance in the quality of experience metrics.
Keywords: Internet of vehicles; Video streaming; Gateway selection; IP multicast

Bharat Dwivedi, Debarati Sen, Sandip Chakraborty,
A survey of longitudinal changes in cellular network architecture: The good, the bad, and the ugly,
Journal of Network and Computer Applications,
Volume 207,
2022,
103496,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103496.
(https://www.sciencedirect.com/science/article/pii/S1084804522001382)
Abstract: This survey explores the architectural changes in the cellular network architecture over the past few years and highlights the advantages and additional challenges associated with these architectural changes. The continuation of essential innovations transforms the cellular architecture and directs it towards a more flexible, scalable, distributed, and programmable platform. Restructuring and disassociating different architectural components is required to make the cellular network architecture scalable and flexible for next-generation. The new cellular architecture has separated the radio unit from the baseband unit. It has also included virtualized infrastructure, strengthening the cellular network, and separating the control and data planes. The goal is to increase cellular network capacity, reliability, end-to-end network connectivity, Quality of Service (QoS), and Quality of Experience (QoE) in the new cellular network architecture. Since it is projected that cellular traffic will be 1000 times greater in the next generation of cellular communication, hence, it is required to upgrade the current cellular network architecture. Another vital factor that needs to be considered is to reduce the latency for achieving high data rates with growing traffic and it can be achieved by segregating the radio unit from the baseband unit and further dividing it into distributed and centralized units. This survey also points out the limits of previous studies and highlights several open issues that requires further investigation to build next-generation cellular communication technologies.
Keywords: Network architecture; Radio access networks (RAN); Fog-RAN (F-RAN); Evolved Packet Core (EPC); Software defined networking (SDN)

Rongji Liao, Yuan Zhang, Jinyao Yan, Yang Cai, Narisu Tao,
STOP: Joint send buffer and transmission control for user-perceived deadline guarantee via curriculum guided-deep reinforcement learning,
Journal of Network and Computer Applications,
Volume 221,
2024,
103787,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103787.
(https://www.sciencedirect.com/science/article/pii/S1084804523002060)
Abstract: Real-time applications with strict user-perceived deadline requirements, such as cloud virtual reality (VR) gaming and high-frequency trading, have experienced substantial growth in recent years. To ensure the user-perceived deadline at the transport layer, i.e., the network transmission delay and the waiting delay in the send buffer, transmission control scheme plays an important role. While several delay-sensitive transmission control schemes have been proposed, none of them consider the impact of waiting delay in the send buffer. To address this issue, we propose STOP, a joint send buffer and congestion window (CWND) control approach for guaranteeing user-perceived deadline via curriculum-guided deep reinforcement learning. To obtain optimal joint control policy, STOP utilizes a reinforcement learning-based algorithm to jointly adjust the CWND and send buffer with respect to the send buffer state, the network condition and the deadline requirement. Furthermore, to enhance the generalization capability of reinforcement learning-based algorithm, STOP incorporates curriculum learning during the training phase with a difficulty measurer and training scheduler. Extensive experiments in NS-3 indicate that the STOP scheme achieves an overwhelmingly higher average arrival ratio, i.e., around 95% vs. around 0.2%, compared with current prevalent transmission control scheme such as BBR, Cubic and Vegas.
Keywords: Deadline-aware; Transmission control; Deep reinforcement learning; Send buffer; Curriculum learning; Joint control

Raja Majid Ali Ujjan, Zeeshan Pervez, Keshav Dahal, Ali Kashif Bashir, Rao Mumtaz, J. González,
Towards sFlow and adaptive polling sampling for deep learning based DDoS detection in SDN,
Future Generation Computer Systems,
Volume 111,
2020,
Pages 763-779,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.10.015.
(https://www.sciencedirect.com/science/article/pii/S0167739X19318333)
Abstract: Distributed Denial of Service (DDoS) is one of the most rampant attacks in the modern Internet of Things (IoT) network infrastructures. Security plays a very vital role for an ever-growing heterogeneous network of IoT nodes, which are directly connected to each other. Due to the preliminary stage of Software Defined Networking (SDN), in the IoT network, sampling based measurement approaches currently results in low-accuracy, higher memory consumption, higher-overhead in processing and network, and low attack-detection. To deal with these aforementioned issues, this paper proposes sFlow and adaptive polling based sampling with Snort Intrusion Detection System (IDS) and deep learning based model, which helps to lower down the various types of prevalent DDoS attacks inside the IoT network. The flexible decoupling property of SDN enables us to program network devices for required parameters without utilizing third-party propriety based hardware or software. Firstly, in data-plane, to lower down processing and network overhead of switches, we deployed sFlow and adaptive polling based sampling individually. Secondly, in control-plane, to optimize detection accuracy, we deployed Snort IDS collaboratively with Stacked Autoencoders (SAE) deep learning model. Furthermore, after applying performance metrics on collected traffic streams, we quantitatively investigate trade off among attack detection accuracy and resources overhead. The evaluation of the proposed system demonstrates higher detection accuracy with 95% of True Positive rate with less than 4% of False Positive rate within sFlow based implementation compared to adaptive polling.
Keywords: DDoS; IoT; SDN; Snort; Sampling

Gines Garcia-Aviles, Carlos Donato, Marco Gramaglia, Pablo Serrano, Albert Banchs,
ACHO: A framework for flexible re-orchestration of virtual network functions,
Computer Networks,
Volume 180,
2020,
107382,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107382.
(https://www.sciencedirect.com/science/article/pii/S1389128619317165)
Abstract: Network Function Virtualization enables network slicing as a novel paradigm for service provisioning. With network slicing, Virtual Network Functions (VNFs) can be instantiated at different locations of the infrastructure, choosing their optimal placement based on parameters such as the requirements of the service or the resources available. One limitation of state-of-the-art technology for network slicing is the inability to re-evaluate orchestration decisions once the slice has been deployed, in case of changing service demands or network conditions. In this paper, we present ACHO, a novel software framework that enables seamless re-orchestration of VNFs of any kind, including RAN and Core. With ACHO, VNFs and resources can be easily re-assigned to match, e.g., varying user demands or changes in the nodes’ load. ACHO uses lightweight mechanisms, such as splitting the engine of a VNF from the data it requires to perform its operation, in such a way that, when re-allocating a VNF, only the data is moved (a new engine is instantiated in the new location). We demonstrate the use of ACHO in a small scale testbed, showing that (i) the proposed re-orchestration is feasible, (ii) it results much faster than existing alternatives (especially for relocation), and (iii) the framework can be readily applied to existing VNFs after minimal changes to their implementation.
Keywords: 5G Mobile communication; Computer network management; Network architecture; Network function virtualization

Nivedita Shrivastava, Muhammad Abdullah Hanif, Sparsh Mittal, Smruti Ranjan Sarangi, Muhammad Shafique,
A survey of hardware architectures for generative adversarial networks,
Journal of Systems Architecture,
Volume 118,
2021,
102227,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102227.
(https://www.sciencedirect.com/science/article/pii/S1383762121001582)
Abstract: Recent years have witnessed a significant interest in the “generative adversarial networks” (GANs) due to their ability to generate high-fidelity data. Many models of GANs have been proposed for a diverse range of domains ranging from natural language processing to image processing. GANs have a high compute and memory requirements. Also, since they involve both convolution and deconvolution operation, they do not map well to the conventional accelerators designed for convolution operations. Evidently, there is a need of customized accelerators for achieving high efficiency with GANs. In this work, we present a survey of techniques and architectures for accelerating GANs. We organize the works on key parameters to bring out their differences and similarities. Finally, we present research challenges that are worthy of attention in near future. More than summarizing the state-of-art, this survey seeks to spark further research in the field of GAN accelerators.
Keywords: Review; Deep neural networks; Generative adversarial network; Transposed convolution; Dilated convolution; GPU; FPGA

Saibharath S., Sudeepta Mishra, Chittaranjan Hota,
Joint QoS and energy-efficient resource allocation and scheduling in 5G Network Slicing,
Computer Communications,
Volume 202,
2023,
Pages 110-123,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.02.009.
(https://www.sciencedirect.com/science/article/pii/S0140366423000464)
Abstract: Network Slicing (NS) is fast evolving as a prominent enabler for providing tailored services in the Fifth Generation of cellular networks (5G). Network Slices are virtualized network entities formed over physical substrates, deployed for the customized application use cases. A Network Slice needs to exhibit end to end capabilities and meet Quality of Service (QoS) specifications and Service Level Agreements (SLAs). To provide end-to-end traffic management capabilities in the network slice, firstly, traffic flows are categorized into different priority traffic classes, and their severity levels are assessed. Priorities can be applied across cellular and IP based systems. Machine Learning (ML) algorithms are employed on QoS profile attributes in establishing traffic priorities in slices. Secondly, we propose a novel algorithm for NS Resource Partitioning and User Allocation. We put forward an online virtual backbone based solution for resource allocation and priority class-based packet scheduling. This joint QoS and energy efficiency driven approach is built on top of established traffic classes and dynamic power savings techniques. Finally, through Cognitive Cycles (CC), we devise better network re-configuration to obtain more energy savings. Traffic classifier modules are implemented using Jupyter notebook and Python API. Scheduling and resource allocation modules in networks slices are emulated in Mininet, Flowvisor, and Beacon and POX controllers. The simulation results reveal the reduced node consumption is achieved through the evolutionary CC algorithm, and it outperforms other standard approaches by at least 23%. Similarly, for the traffic priority prediction, from the results, we could infer Gradient Boosting and Random Forest Regressors exhibit superior accuracy with the root mean square deviation of 2.2% and 1.2% respectively when compared to other standard ML algorithms.
Keywords: Quality of Service; Traffic classification; Resource allocation; Network Slicing; Energy efficiency

David M. Chan, Roshan Rao, Forrest Huang, John F. Canny,
GPU accelerated t-distributed stochastic neighbor embedding,
Journal of Parallel and Distributed Computing,
Volume 131,
2019,
Pages 1-13,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.04.008.
(https://www.sciencedirect.com/science/article/pii/S074373151830875X)
Abstract: Modern datasets and models are notoriously difficult to explore and analyze due to their inherent high dimensionality and massive numbers of samples. Existing visualization methods which employ dimensionality reduction to two or three dimensions are often inefficient and/or ineffective for these datasets. This paper introduces t-SNE-CUDA, a GPU-accelerated implementation of t-Distributed Symmetric Neighbor Embedding (t-SNE) for visualizing datasets and models. t-SNE-CUDA significantly outperforms current implementations with 15-700x speedups on the CIFAR-10 and MNIST datasets. These speedups enable, for the first time, large scale visualizations of modern computer vision datasets such as ImageNet, as well as larger NLP datasets such as GloVe. From these new visualizations, we can draw a number of interesting conclusions. In addition, the performance on machine learning datasets allows us to compute t-SNE embeddings in close to real time, and we explore the applications of such fast embeddings in the domain of importance sampling for neural network training.
Keywords: T-SNE; Embedding; CUDA; Parallel computing; GPU computing; Applications

Lucas M. Ponce, Daniele Lezzi, Rosa M. Badia, Dorgival Guedes,
DDF Library: Enabling functional programming in a task-based model,
Journal of Parallel and Distributed Computing,
Volume 151,
2021,
Pages 112-124,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.02.009.
(https://www.sciencedirect.com/science/article/pii/S0743731521000307)
Abstract: In recent years, the areas of High-Performance Computing (HPC) and massive data processing (also know as Big Data) have been in a convergence course, since they tend to be deployed on similar hardware. HPC systems have historically performed well in regular, matrix-based computations; on the other hand, Big Data problems have often excelled in fine-grained, data parallel workloads. While HPC programming is mostly task-based, like COMPSs, popular Big Data environments, like Spark, adopt the functional programming paradigm. A careful analysis shows that there are pros and cons to both approaches, and integrating them may yield interesting results. With that reasoning in mind, we have developed DDF, an API and library for COMPSs that allows developers to use Big Data techniques while using that HPC environment. DDF has a functional-based interface, similar to many Data Science tools, that allows us to use dynamic evaluation to adapt the task execution in run time. It brings some of the qualities of Big Data programming, making it easier for application domain experts to write Data Analysis jobs. In this article we discuss the API and evaluate the impact of the techniques used in its implementation that allow a more efficient COMPSs execution. In addition, we present a performance comparison with Spark in several application patterns. The results show that each technique significantly impacts the performance, allowing COMPSs to outperform Spark in many use cases.
Keywords: COMPSs; Big Data; Performance evaluation; Data-flow programming

Rui Silva, David Santos, Flávio Meneses, Daniel Corujo, Rui L. Aguiar,
A hybrid SDN solution for mobile networks,
Computer Networks,
Volume 190,
2021,
107958,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107958.
(https://www.sciencedirect.com/science/article/pii/S1389128621000931)
Abstract: The adoption of Software Defined Networking (SDN) has encountered resistance mainly due to its disruptive nature for current deployed networks. In order to allow a smoother transition of SDN into production networks and take advantage of SDN’s characteristics, a mix of SDN with traditional networks, or Hybrid SDN Networks, can be deployed, paving the path to fully SDN-capable networks. The introduction of SDN allows for new and more efficient network optimization methods. This paper presents a Hybrid SDN Mobile Core Network for 4G and Non-Standalone 5G, which integrates support for Wi-Fi access, traffic offloading capabilities from 3GPP to non-3GPP Access and dynamic non-3GPP network slice instantiation, while still being flexible enough to be used in Standalone 5G. Results showed that the virtualization of the Mobile Core Network and the introduction of SDN in its architecture (making it a hybrid SDN solution) presented no significant impact in latency, attachment time and throughput in the mobile network when compared with a traditional deployment, coupling it with all the flexibility that SDN provides. Furthermore, benefits of traffic offloading for both the end user and the network operator are also shown.
Keywords: Hybrid SDN; 4G; EPC; 5G; Mobile Core Network; Traffic offloading

Minsoo Kim, Ilhyun Suh, Yon Dohn Chung,
MARS: A multi-level array representation for simulation data,
Future Generation Computer Systems,
Volume 111,
2020,
Pages 419-434,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.11.010.
(https://www.sciencedirect.com/science/article/pii/S0167739X19311306)
Abstract: In the numerical simulation domain, owing to the huge size of data and the complexity of implementing the domain specific applications, a database-centric approach for handling multidimensional simulation data is gaining considerable attention. Array databases provide an optimized set of features to support administrating multidimensional data; representing simulation data with an array can be an optimal choice. Generally, query performance on sparsely filled arrays, especially when empty cells are placed between adjacent elements, can be poor. In this context, previous studies focused on the compact representation of simulation data by reducing the number of empty cells between adjacent elements as possible. However, these methods inevitably lose the original spatial structure of elements (i.e., the relative distance and direction among elements), making it impossible to utilize the built-in multidimensional operators provided by array databases. In this paper, we propose MARS, a multi-level array representation for simulation data. MARS utilizes multiple level arrays with various resolutions to cope with the two addressed problems. In the MARS representation, elements tend to be concentrated into dense array regions, where each region is selectively stored in one of the level arrays that most reduces the empty cells between adjacent elements. Unlike existing methods, MARS retains the spatial structure of elements, and thus no additional efforts to reorganize the original spatial structure for query processing is required. We built MARS on top of SciDB and implemented a specialized command line tool for MARS. We present methods and optimized operators for query processing over MARS. We evaluate the performance of MARS using two real-world numerical simulation datasets.
Keywords: Array databases; Scientific data; Query processing

Sajad Khorsandroo, Adrián Gallego Sánchez, Ali Saman Tosun, JM Arco, Roberto Doriguzzi-Corin,
Hybrid SDN evolution: A comprehensive survey of the state-of-the-art,
Computer Networks,
Volume 192,
2021,
107981,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107981.
(https://www.sciencedirect.com/science/article/pii/S1389128621001109)
Abstract: Software-Defined Networking (SDN) is an evolutionary networking paradigm which has been adopted by large network and cloud providers, among which are Tech Giants. However, embracing a new and futuristic paradigm as an alternative to well-established and mature legacy networking paradigm requires a lot of time along with considerable financial resources and technical expertise. Consequently, many enterprises cannot afford it. A compromise solution then is a hybrid networking environment (a.k.a. Hybrid SDN (hSDN)) in which SDN functionalities are leveraged while existing traditional network infrastructures are acknowledged. Recently, hSDN has been seen as a viable networking solution for a diverse range of businesses and organizations. Accordingly, the body of literature on hSDN research has improved remarkably. On this account, we present this paper as a comprehensive state-of-the-art survey which expands upon hSDN from many different perspectives.
Keywords: Software-Defined Networking (SDN); Hybrid SDN; Network architecture; Network security; Network management; Traffic engineering; Implementation and deployment

Wafa'a Kassab, Khalid A. Darabkh,
A–Z survey of Internet of Things: Architectures, protocols, applications, recent advances, future directions and recommendations,
Journal of Network and Computer Applications,
Volume 163,
2020,
102663,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102663.
(https://www.sciencedirect.com/science/article/pii/S1084804520301375)
Abstract: Ubiquitous sensing, provided via wireless sensor networks technologies, disseminates across many domains of contemporary day living. This provides the ability to sense, process, analyze and infer environmental parameters from natural resources and delicate ecologies to urban environments. The explosion in the number of devices that are connected to the internet has led to the emergence of the Internet of Things (IoT) technological revolution. In these technologies, actuators and sensors incorporate smoothly with the IoT environment. Furthermore, the sensed data is shared through platforms to innovate a common operating picture. This cutting-edge technology is fueled by a diversity of IoT devices that enables technologies such as near field communication, embedded actuator, sensor nodes, radio frequency identification tags, and readers. IoT has emerged from its infancy and has established a fully integrated future internet. Different visions of IoT technologies have been reviewed, however, what emerges currently in this field should be faced and displayed via the research community. In this paper, we are keen to discuss the recent worldwide implementation of IoT, where the prime enabling technologies, recent and future communication protocols and application areas that drive IoT research in the near future are explored. Furthermore, the original, recent, future enhancements of all IoT stack's protocols are extensively discussed. Middleware's definition, usages, types and open research challenges are further illustrated. Not only to this extent but rather, this survey details the simulation tools of IoT networks, IoT sensors along with their recent application areas, broad IoT research challenges, as well as in-depth analysis of IoT research history and recommendations that attract current IoT researchers' attention.
Keywords: IoT architectures; IoT protocols; IoT applications; IoT middleware; IoT simulators; IoT challenges; Future directions; Recommendations

Tharaka Hewa, Mika Ylianttila, Madhusanka Liyanage,
Survey on blockchain based smart contracts: Applications, opportunities and challenges,
Journal of Network and Computer Applications,
Volume 177,
2021,
102857,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102857.
(https://www.sciencedirect.com/science/article/pii/S1084804520303234)
Abstract: Blockchain is one of the disruptive technical innovation in the recent computing paradigm. Many applications already notoriously hard and complex are fortunate to ameliorate the service with the blessings of blockchain and smart contracts. The decentralized and autonomous execution with in-built transparency of blockchain based smart contracts revolutionize most of the applications with optimum and effective functionality. The paper explores the significant applications which already benefited from the smart contracts. We also highlight the future potential of the blockchain based smart contracts in these applications perspective.
Keywords: Blockchain; Smart contracts; Applications; DLT; Hyperledger Fabric; Ethereum; Corda; Stellar

Yang Xu, Guojun Wang, Ju Ren, Yaoxue Zhang,
An adaptive and configurable protection framework against android privilege escalation threats,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 210-224,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.09.042.
(https://www.sciencedirect.com/science/article/pii/S0167739X18307775)
Abstract: Android is a successful mobile platform with a thriving application ecosystem. However, despite its security precautions like permission mechanism, it is still vulnerable to privilege escalation threats and particularly confused deputy attacks that exploit the permission leak vulnerabilities of Android applications. Worse, most existing detection and protection techniques have become costly and unresponsive in current Android dynamic permission environments. In this paper, we propose a configurable Android security framework to prevent the exploitation of permission leak vulnerabilities of third-party applications via confused deputy attacks. Our framework collects the runtime states of applications and enforces policy and capability-based access control to restrain riskful inter-application communications, so as to provide more responsive, adaptive, and flexible application protection. Besides, our framework provides users with a flexible runtime policy configuration together with a complementary security mechanism to mitigate risks induced by inappropriate policies. Additionally, we present a sophisticated access decision cache system with a proactive maintenance method that ensures the efficiency and dependability of decision services. Theoretical analysis and experimental evaluation demonstrate that our approach provides configurable and effective protections for third-party applications against permission leak vulnerabilities at small performance and usability costs.
Keywords: Android; Privilege escalation threat; Dynamic permission mechanism; Capability-based access control; Configurability

Muhammad Hassan, Mauro Conti, Chhagan Lal,
NC based DAS-NDN: Network Coding for robust Dynamic Adaptive Streaming over NDN,
Computer Networks,
Volume 174,
2020,
107222,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107222.
(https://www.sciencedirect.com/science/article/pii/S1389128619312551)
Abstract: Over the last few years, the dramatic growth in video demand has inspired the service providers (e.g., Netflix and YouTube) to swing towards HTTP based Dynamic Adaptive Streaming (DASH). However, sustaining the adequate bandwidth claims over this rapid growth in multimedia content becomes a significant challenge for network operators. Considering the effectiveness of the next generation future Internet architecture, i.e., Name Data Networking (NDN), recently DASH over NDN is implemented. The fundamental characteristics of NDN, such as efficient content distribution and low bandwidth requirements, significantly increase the bandwidth utilization, which ensures the smooth delivery of multimedia content. However, we discovered that the above characteristics of NDN also opens the door for new vulnerabilities. In this paper, first we propose a new attack termed as “Bitrate Oscillation Attack” (BOA), which disrupt the functionality of DASH protocol over NDN by exploiting its two key features called in-network caching and interest aggregation. In particular, BOA forces the DASH streaming system running at the honest client to oscillate in various video resolutions with high frequency and amplitude, within a single video session. Second, to mitigate the BOA, we design and implement a proactive countermeasure called “NC based DAS-NDN”. Our solution efficiently enables the network coding to DAS multimedia content and within NDN architecture. Thus, without any coordination between the network nodes reduces bitrate oscillations in the presence of BOA and NDN’s inherent content source variations. The performance evaluation performed on different target scenarios proves the effectiveness of our proposed attack, and the results also show the correctness of our proposed corresponding countermeasure. In particular, the result analysis shows that BOA increases the annoyance factor in spatial dimension of end-user, and our countermeasure greatly reduces the adverse effects of BOA and also make DAS friendly to NDN’s inherent features.
Keywords: Name Data Networking; Adaptive multimedia streaming; Bit rate oscillations; DASH; Network Coding

Hisham A. Kholidy,
Autonomous mitigation of cyber risks in the Cyber–Physical Systems,
Future Generation Computer Systems,
Volume 115,
2021,
Pages 171-187,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.09.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X19320680)
Abstract: The Cyber–Physical Systems (CPS) attacks and vulnerabilities are increasing and the consequences of such attacks can be catastrophic. The CPS needs to be self-resilient to cyber-attacks through a precise autonomous and timely risk mitigation model that can analyze and assess the risk of the CPS and apply a proper response strategy against the ongoing attacks. There is a limited amount of work on the self-protection of the cyber risks in the CPS. This paper contributes toward the need of advanced security approaches to respond against the attacks across the CPS in an autonomous way, with or without including a system administrator in the loop for troubleshooting based on the criticality of the CPS asset that can be protected, once the alert about a possible intrusion has been raised. To this end, this paper augments our existing security framework with an Autonomous Response Controller (ARC). ARC uses our quantitative Hierarchical Risk Correlation Tree (HRCT) that models the paths an attacker can traverse to reach certain goals and measures the financial risk that the CPS assets face from cyber-attacks. ARC also uses a Competitive Markov Decision Process (CMDP) to model the security reciprocal interaction between the protection system and the attacker/adversary as a multi-step, sequential, two-player stochastic game in which each player tries to maximize his/her benefit. The experiments’ results depict that the accuracy of ARC outperforms the traditional Static Intrusion Response System (S-IRS) by 43.61%. To experimentally test and validate ARC in real-time large-scale data, we run the Aurora attack to open the generator breaker in our testbed to create a cascading failure and voltage collapse. ARC was able to recover the CPS system and provide a timely response in less than 6 s. We compared the output of ARC against the current state of the art, the Suricata intrusion response system. ARC was able to mitigate the single line to ground (SLG) attacks and recover the CPS to its normal state in 122 s before Suricata does.
Keywords: Cyberattacks; CPS security; Risk mitigation; Self-protection; Autonomous intrusion response

Muluneh Mekonnen Tulu, Mbazingwa E. Mkiramweni, Ronghui Hou, Sultan Feisso, Talha Younas,
Influential nodes selection to enhance data dissemination in mobile social networks: A survey,
Journal of Network and Computer Applications,
Volume 169,
2020,
102768,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102768.
(https://www.sciencedirect.com/science/article/pii/S1084804520302423)
Abstract: Downloading of contents on mobile devices has been increasing rapidly since the introduction of mobile communication technologies. The huge traffic load presents a significant challenge to mobile network operators. Therefore, mobile social network (MSN) has been proposed to leverage cellular links by offloading mobile traffic via device-to-device communications. To do so, applying effective algorithm to identify influential spreader in MSN is of critical importance. Recently, various techniques have been proposed, each with its particular points of interest and impediments. In this paper, we provide a comprehensive survey of different techniques used to identify influential nodes in MSNs. In this regard, we discuss the advantages and disadvantages of the methods used to select initial seeds. We also review MSNs with regard to characteristics, platforms, classification, benefits, and challenges. In addition, we review data dissemination algorithms in MSNs. We then analyze and indicate the node selection complication in future networks. Finally, we outline possible future research directions and summarize the major challenges for on-going node selection in MSNs research.
Keywords: Influential nodes; Mobile social networks; Data dissemination; Wireless communications

Yijie Wang, Ziping Yu, Zhongliang Zhao, Xianbin Cao,
AOR: Adaptive opportunistic routing based on reinforcement learning for planetary surface exploration,
Computer Communications,
Volume 211,
2023,
Pages 134-146,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.09.008.
(https://www.sciencedirect.com/science/article/pii/S0140366423003183)
Abstract: Planetary surface exploration mobile ad-hoc networks (PSEMANET) show the characteristics of routing void caused by craters and electromagnetic interference, relatively high dynamics caused by node heterogeneity, and small node capacity caused by load constraints, which is the reason for the increase of communication delay and packet loss. Adaptive opportunistic routing based on reinforcement learning (AOR) is an opportunistic routing protocol that determines the forwarding node based on the current coordinates, queue length, and the number of neighbors. The full forwarding areas are used to avoid routing void, and the dual competition mechanism is designed to avoid the node hidden problem. The Q-value obtained by reinforcement learning is used to design the dynamic delay cost (DDC) so that nodes in the forwarding areas can compete to achieve adaptive path switching to the environment. Compared with representative proactive routing OLSR, reactive routing AODV, geographic routing GPSR and opportunistic routing BLR, AOR and AOR with memory(AOR-M) have high packet delivery ratio (PDR) and low end-to-end delay in planetary surface exploration scenarios implemented by our simulation system. And the AOR-M protocol shows the lowest expected end-to-end delay and highest channel utilization in both high and low speed scenarios. The result shows that the AOR-M provides efficient and robust routing in planetary surface exploration mobile ad-hoc networks with a complex environment, highly dynamic nodes, and performance constraints.
Keywords: Planetary surface exploration; Mobile ad hoc networks; Reinforcement learning; Opportunistic routing

Alcardo Alex Barakabitze, Arslan Ahmad, Rashid Mijumbi, Andrew Hines,
5G network slicing using SDN and NFV: A survey of taxonomy, architectures and future challenges,
Computer Networks,
Volume 167,
2020,
106984,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106984.
(https://www.sciencedirect.com/science/article/pii/S1389128619304773)
Abstract: The increasing consumption of multimedia services and the demand of high-quality services from customers has triggered a fundamental change in how we administer networks in terms of abstraction, separation, and mapping of forwarding, control and management aspects of services. The industry and the academia are embracing 5G as the future network capable to support next generation vertical applications with different service requirements. To realize this vision in 5G network, the physical network has to be sliced into multiple isolated logical networks of varying sizes and structures which are dedicated to different types of services based on their requirements with different characteristics and requirements (e.g., a slice for massive IoT devices, smartphones or autonomous cars, etc.). Softwarization using Software-Defined Networking (SDN) and Network Function Virtualization (NFV)in 5G networks are expected to fill the void of programmable control and management of network resources. In this paper, we provide a comprehensive review and updated solutions related to 5G network slicing using SDN and NFV. Firstly, we present 5G service quality and business requirements followed by a description of 5G network softwarization and slicing paradigms including essential concepts, history and different use cases. Secondly, we provide a tutorial of 5G network slicing technology enablers including SDN, NFV, MEC, cloud/Fog computing, network hypervisors, virtual machines & containers. Thidly, we comprehensively survey different industrial initiatives and projects that are pushing forward the adoption of SDN and NFV in accelerating 5G network slicing. A comparison of various 5G architectural approaches in terms of practical implementations, technology adoptions and deployment strategies is presented. Moreover, we provide a discussion on various open source orchestrators and proof of concepts representing industrial contribution. The work also investigates the standardization efforts in 5G networks regarding network slicing and softwarization. Additionally, the article presents the management and orchestration of network slices in a single domain followed by a comprehensive survey of management and orchestration approaches in 5G network slicing across multiple domains while supporting multiple tenants. Furthermore, we highlight the future challenges and research directions regarding network softwarization and slicing using SDN and NFV in 5G networks.
Keywords: 5G; SDN; NFV; Network slicing; Cloud/edge computing; Network softwarization

Jyoti Verma, Abhinav Bhandari, Gurpreet Singh,
iNIDS: SWOT Analysis and TOWS Inferences of State-of-the-Art NIDS solutions for the development of Intelligent Network Intrusion Detection System,
Computer Communications,
Volume 195,
2022,
Pages 227-247,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.08.022.
(https://www.sciencedirect.com/science/article/pii/S0140366422003371)
Abstract: Introduction:
The growth of ubiquitous networked devices and the proliferation of geographically dispersed ‘Internet of Thing’ devices have exponentially increased network traffic. The socio-economical society is highly dependent on modern devices, and unavailability may lead to catastrophic results for even a short time. The less secure and heterogeneous devices in the public domain have shaped a cyber-attack surface in the cloud environment. Traditional approaches for Network Intrusion Detection Systems have proven ineffective and insufficient in defending against zero-day attacks.
Methods:
This article visited the advancements in the intrusion detection realm in the last five years and conducted a comprehensive retrospection of modern network intrusion detection systems. The authors have performed a comprehensive SWOT (Strength, Weakness, Opportunities, Threats) analysis of contemporary Network Intrusion Detection Systems in multiple technology dimensions, including big-data processing of high volume network traffic, machine learning, deep learning for self-learning machines, readiness for zero-day attacks, distributed processing, cost-effective solution, and ability to perform autonomous operations.
Results:
The paper turns SWOT analysis into TOWS inferences from the retrospective study for strategy formulation and features the attributes of a futuristic NIDS solution.
Discussion:
The article concludes with the discussion and future scope as the pinnacle of security solution development against zero-day attacks.
Keywords: Big data; Cloud computing; Deep learning; Intrusion Detection System; IDS; IoT; Machine learning; Network Intrusion Detection; NIDS; SWOT; TOWS

Muhammad Nadeem, Zhenmin Li, Avinash Malik, Morteza Biglari-Abhari, Zoran Salcic,
Allocation and scheduling of SystemJ programs on chip multiprocessors with weighted TDMA scheduling,
Journal of Systems Architecture,
Volume 98,
2019,
Pages 63-78,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2019.06.009.
(https://www.sciencedirect.com/science/article/pii/S138376211930013X)
Abstract: The event-driven model of computation provides a good platform for capturing complex real-time applications with low latency requirements since this model is free of problems like deadlocks, livelocks and performance bottlenecks such as locking overheads. Furthermore, there is great potential for extracting parallelism from an event-driven model on a chip multiprocessor (CMP). But, the execution and scheduling of this model on a CMP faces two primary problems that need to be addressed: (a) allocation of asynchronous event handlers (AEHs) on a minimal number of processing elements of a CMP such that the time guarantees are met and (b) scheduling the AEHs taking into consideration the shared resource access contentions. We propose a two-stage Integer Linear Programming approach for static allocation and scheduling of AEHs executing on a time-analyzable multiprocessor architecture with shared resources. The first stage allocates AEHs on a minimal number of processing elements in a CMP with weighted time-division multiplex access (TDMA) to the shared memory while meeting the timing requirements. The second stage then tunes the TDMA schedule such that the worst case reaction time of AEHs can be further shortened. This approach shows 43.75% better utilization of resources compared to traditional fair TDMA scheduling on CMPs.
Keywords: Multiprocessor systems; TDMA; Allocation; Scheduling; Real-time

Ghada Alsuhli, Ahmed Fahim, Yasser Gadallah,
A survey on the role of UAVs in the communication process: A technological perspective,
Computer Communications,
Volume 194,
2022,
Pages 86-123,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.07.021.
(https://www.sciencedirect.com/science/article/pii/S0140366422002687)
Abstract: Recently, the use of unmanned aerial vehicles (UAVs) has extended to include many aspects of life such as the commercial, military, search and rescue, monitoring and communications aspects, to name a few. It is unthinkable nowadays to imagine life without robust and far-reaching communication systems. This has become quite evident with the recent COVID-19 pandemic that has struck humanity with profound effects like no other. Many of the human activities had to be switched from being face-to-face activities to the online mode. Therefore, the use of UAVs to extend the reach of communication systems will prove invaluable to many citizens of the world. We see that UAVs can assume different functional roles in the communications process. In this study, we present a novel classification of UAV-related communications along two axes. First, this classification examines the role of the UAV in the communications process where it classifies the UAV as a provider of communications, a relay (service extender) of communications or a consumer of the communications. Second, it examines the communication technologies most used in conjunction with UAV utilization. We provide a critical discussion of the studies that deal with different applications as well as the issues encountered in using a certain technology with UAVs as they assume a specific role. Finally, we present the lessons learned as well as the future directions for using the UAVs in their different roles in connection to the different communication technologies.
Keywords: UAV communications; Provider; Consumer; Relay; Communication technologies; LTE; mmWave; FSO; WiFi; LPWAN; ZigBee

Qi Shen, Craig Sharp, Richard Davison, Gary Ushaw, Rajiv Ranjan, Albert Y. Zomaya, Graham Morgan,
A general purpose contention manager for software transactions on the GPU,
Journal of Parallel and Distributed Computing,
Volume 139,
2020,
Pages 1-17,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.12.018.
(https://www.sciencedirect.com/science/article/pii/S0743731519301376)
Abstract: The Graphics Processing Unit (GPU) is now used extensively for general purpose GPU programming (GPGPU), allowing for greater exploitation of the multi-core model across many application domains. This is particularly true in cloud/edge/fog computing, where multiple GPU enabled servers support many different end user services. This move away from the naturally parallel domain of graphics can incur significant performance issues. Unlike the CPU, code that is hindered from execution due to blocking/waiting on the GPU can affect thousands of threads, rendering the advantages of a GPU irrelevant and reducing a highly parallel environment down to a serial one in the worst case. In this paper we present a solution that minimises blocking/waiting in GPGPU computing using a contention manager that offsets memory conflicts across threads through thread re-ordering. We consider conflicts of memory not only to avoid corruption (standard for transactional memory) but also in the semantic layer of application logic (e.g., enforcing ordering to ensure money drawn from bank account occurs after all deposits). We demonstrate how our approach is successful across a number of industry benchmarks and compare our approach to the only other related solution. We also demonstrate that our approach is scalable in terms of thread numbers (a key requirement on the GPU). We believe this is the first work of its kind demonstrating a generalised conflict and semantic contention manager suitable for the scale of parallel execution found on a GPU.
Keywords: GPU; Parallel processing; High performance computing

Md. Farhad Hossain, Ayman Uddin Mahin, Topojit Debnath, Farjana Binte Mosharrof, Khondoker Ziaul Islam,
Recent research in cloud radio access network (C-RAN) for 5G cellular systems - A survey,
Journal of Network and Computer Applications,
Volume 139,
2019,
Pages 31-48,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.04.019.
(https://www.sciencedirect.com/science/article/pii/S1084804519301432)
Abstract: Traditional architectures of cellular networks are facing tremendous challenges due to unprecedented increase in mobile data trafﬁc, limited spectrum availability and high power consumption. In light of this, industries as well as research communities are in constant search for fundamental breakthroughs in developing novel network architectures for supporting the exploding user demand, while reducing capital and operational expenditures for network operators. Cloud radio access network (C-RAN) architecture is such a paradigm shifting concept for cellular networks, which is also being actively considered as a major candidate for future 5G cellular systems. This paper presents a comprehensive survey on the most recent advances in C-RAN research focusing on the analysis and enhancement of its various major aspects. In particular, after reviewing the works on C-RAN architectures, we then focus on the papers published speciﬁcally on the throughput enhancement, interference management, energy efﬁciency, latency, security and system cost reduction of C-RAN based cellular networks. A summary of the open issues and future research directions in these areas of C-RAN based cellular networks is also presented.
Keywords: C-RAN architectures; Throughput; Interference management; Energy efﬁciency; Latency; Security; System cost; 5G cellular networks

Lerina Aversano, Mario Luca Bernardi, Marta Cimitile, Riccardo Pecori,
A systematic review on Deep Learning approaches for IoT security,
Computer Science Review,
Volume 40,
2021,
100389,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100389.
(https://www.sciencedirect.com/science/article/pii/S1574013721000290)
Abstract: The constant spread of smart devices in many aspects of our daily life goes hand in hand with the ever-increasing demand for appropriate mechanisms to ensure they are resistant against various types of threats and attacks in the Internet of Things (IoT) environment. In this context, Deep Learning (DL) is emerging as one of the most successful and suitable techniques to be applied to different IoT security aspects. This work aims at systematically reviewing and analyzing the research landscape about DL approaches applied to different IoT security scenarios. The contributions we reviewed are classified according to different points of view into a coherent and structured taxonomy in order to identify the gap in this pivotal research area. The research focused on articles related to the keywords ’deep learning’, ’security’ and ’Internet of Things’ or ’IoT’ in four major databases, namely IEEEXplore, ScienceDirect, SpringerLink, and the ACM Digital Library. We selected and reviewed 69 articles in the end. We have characterized these studies according to three main research questions, namely, the involved security aspects, the used DL network architectures, and the engaged datasets. A final discussion highlights the research gaps still to be investigated as well as the drawbacks and vulnerabilities of the DL approaches in the IoT security scenario.
Keywords: Internet of Things; Security; Systematic review; Deep Learning

Deepsubhra Guha Roy, Puja Das, Debashis De, Rajkumar Buyya,
QoS-aware secure transaction framework for internet of things using blockchain mechanism,
Journal of Network and Computer Applications,
Volume 144,
2019,
Pages 59-78,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.06.014.
(https://www.sciencedirect.com/science/article/pii/S108480451930219X)
Abstract: The Internet of Things (IoT) paradigm enables an enormous network with millions of connected smart things to control almost all aspect of services. In an IoT scenario, self-configured things are dynamically connected in a universal network. The small things are broadly distributed in a real world model with some essential processing capacity and finite storage. Security, privacy and reliability are the most important pillars of the IoT infrastructure. It is challenging to share personal data, highly confidential professional data over a centralized system. The infrastructure itself is capable to provide the guarantee to transmit data safely. However, a special attention for the data along with data consistency is required. Cyber attackers and hackers mostly target the centralized systems. To triumph over these types of situations we propose a security based blockchain service along with some encryption approaches for a secure communication. The encryption and decryption is performed by a hash based secret key. Risk modelling and relative risk reduction also done to prevent attacks under a virtual private network in IoT domain. Before and after attack users also wish to know quality of service (QoS). We have developed a mobile application framework which provides desire output of a user from different aspect of QoS for better understanding. The MQTT broker performs as a negotiator to convey data between cloud consumer and cloud provider. In QoS monitoring section user get chance to reconsider availability, reliability and responsibility like QoS factors while recovering resources and actual resources before attacking system based on CPU utilization, cache status and storage. As per experiment result we able to recover 100% RAM, 97.64% CPU utilization and 78.7% storage using proposed encryption and algorithm on blockchain infrastructure.
Keywords: Internet of things; Man in the middle attack; Risk assessment; Risk recovery blockchain; Congestions control; QoS

Rodothea Myrsini Tsoupidi, Elena Troubitsyna, Panagiotis Papadimitratos,
Thwarting code-reuse and side-channel attacks in embedded systems,
Computers & Security,
Volume 133,
2023,
103405,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103405.
(https://www.sciencedirect.com/science/article/pii/S0167404823003152)
Abstract: Embedded devices are increasingly present in our everyday life. They often process critical information, and hence, rely on cryptographic protocols to achieve security. However, embedded devices remain particularly vulnerable to attackers seeking to hijack their operation and extract sensitive information by exploiting side channels and code reuse. Code-Reuse Attack (CRAs) can steer the execution of a program to malicious outcomes, altering existing on-board code without direct access to the device memory. Moreover, Side-Channel Attacks (SCAs) may reveal secret information to the attacker based on mere observation of the device. Thwarting CRAs and SCAs against embedded devices is especially challenging because embedded devices are usually resource constrained. Fine-grained code diversification can hinder CRAs by introducing uncertainty to the binary code; while software mechanisms can thwart timing or power SCAs. The resilience to either attack may come at the price of the overall efficiency. Moreover, a unified approach that preserves these mitigations against both CRAs and SCAs is not available. In this paper, we propose a novel SecDivCon approach that tackles this challenge. SecDivCon is a combinatorial compiler-based approach that combines software diversification against CRAs with software mitigations against SCAs. SecDivCon restricts the performance overhead introduced by the generated code that thwarts the attacks and hence, offers a secure-by-design approach enabling control over the performance-security trade-off. Our experiments, using 16 benchmark programs, show that SCA-aware diversification is effective against CRAs, while preserving SCA mitigation properties at a low, controllable overhead. Given the combinatorial nature of our approach, SecDivCon is suitable for small, performance-critical functions that are sensitive to SCAs. SecDivCon may be used as a building block to whole-program code diversification or in a re-randomization scheme of cryptographic code.
Keywords: Compiler-based mitigation; Automatic software diversification; Software masking; Constant-resource programming; Secure compilation

Jiawei Li, Hongbin Luo, Shan Zhang, Hongyi Li, Fei Yan,
Design and implementation of efficient control for incoming inter-domain traffic with Information-Centric Networking,
Journal of Network and Computer Applications,
Volume 133,
2019,
Pages 109-125,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.02.013.
(https://www.sciencedirect.com/science/article/pii/S1084804519300578)
Abstract: Efficient and accurate control of incoming inter-domain traffic is critically important for Autonomous Systems (ASes). However, it is extremely difficult for two neighboring ASes to efficiently control incoming traffic over multiple inter-domain links in the current Internet. By contrast, Information-Centric Networking (ICN) is a novel network paradigm and has many inherent features, which can support finer granularity of routing policies and provide more opportunities to realize Traffic Engineering (TE). In this paper, we explore the potentials of ICN in controlling incoming inter-domain traffic between neighboring ASes who have multiple inter-domain links. We present a heuristic algorithm and extend it with different information on network status and content metadata. We implement them in a real prototype and compare them with a classic inter-domain TE approach in the current Internet (i.e., the IP-prefixes negotiation approach, IPN) in three scenarios. Three interesting conclusions are drawn from extensive experiments on the prototype. First, ICN makes it easier to control inter-domain traffic compared with IPN. In particular, ICN approaches have shorter negotiation delay and lower negotiation overhead. Second, content metadata (e.g., content size) in ICN is quite beneficial to control traffic since it can help estimate traffic volumes so as to achieve more accurate control of traffic. Third, ICN can efficiently handle the bursty traffic and link failures compared with IPN.
Keywords: Information-Centric Networking(ICN); Traffic Engineering

Xin Li, Ruitao Liu, Zhenmin Qiao,
Privacy information verification of homomorphic algorithm for aggregated data based on fog layer structure,
Computer Communications,
Volume 181,
2022,
Pages 309-319,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.08.015.
(https://www.sciencedirect.com/science/article/pii/S0140366421003091)
Abstract: Becoming a vital position in the interconnection industry of the Internet of Things, IIoT has promoted the conversion of traditional industries to intelligent industries. However, it is necessary further to solve the security and privacy threats in IIoT while reducing communication bandwidth and computing resource consumption to carry out research. In order to solve the problem of user privacy leakage caused by the access structure, a fog computing-oriented access control structure hiding scheme is proposed in the paper. The Paillier homomorphic filter algorithm is introduced in the fog calculation. The Paillier homomorphic algorithm hides the mapping function in the access structure during the data upload process, achieving the effect of completely hiding the access structure. Moreover, the ciphertext is stored separately from the access structure, and the Paillier homomorphic algorithm is run through the fog node during the decryption process to detect whether the attributes of the data user exist in the hidden access structure. If it exists, reconstruct the mapping function and send it to the data user, who then downloads and decrypts the ciphertext. If it does not exist, it means that the data user does not meet the access conditions of the data, and there is no need to download and decrypt the ciphertext. Meanwhile, a simulation experiment platform is constructed in the paper to distinguish the performance of the method proposed in the paper from other similar methods, proving the efficiency and practicability of the scheme.
Keywords: Data privacy; Fog computing; Data aggregation; Paillier homomorphic algorithm; LDA-EPP model

Ruiyun Yu, Ann Move Oguti, Mohammad S. Obaidat, Shuchen Li, Pengfei Wang, Kuei-Fang Hsiao,
Blockchain-based solutions for mobile crowdsensing: A comprehensive survey,
Computer Science Review,
Volume 50,
2023,
100589,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100589.
(https://www.sciencedirect.com/science/article/pii/S1574013723000564)
Abstract: Mobile crowdsensing (MCS) is an emerging data-driven paradigm that leverages the collective intelligence of the crowd, their mobility, and the crowd-companioned smart mobile devices embedded with powerful sensors to acquire information from the physical environment for crowd intelligence extraction and human-centric service delivery. However, existing MCS systems operate in a centralized manner, giving rise to several challenges, including privacy, security, incentives, and dependence on a central service provider. Blockchain is a novel application paradigm that incorporates point-to-point transmission, consensus mechanisms, cryptography, intelligent contracts, distributed data storage, and other computing technologies, creating a shift from the current centralized paradigm to a decentralized paradigm. Nonetheless, the convergence of MCS and blockchains necessitates addressing numerous fundamental challenges arising from their merger. This paper examines the major issues facing MCS systems and blockchain’s potential role in addressing them. We present the MCS-blockchain integrated deployment strategies, architectural designs, and core blockchain technology principles that contribute significantly to the performance of blockchain-based MCS applications. Additionally, the advancement of blockchain technology and its impact on MCS system security and performance requirements are investigated. Finally, we highlight current research gaps and future research opportunities that may inspire the deployment of novel blockchain-based MCS systems.
Keywords: Mobile crowdsensing; Blockchain; Privacy; Incentive mechanisms; Distributed data storage

Hang Thanh Bui, Hamed Aboutorab, Arash Mahboubi, Yansong Gao, Nazatul Haque Sultan, Aufeef Chauhan, Mohammad Zavid Parvez, Michael Bewong, Rafiqul Islam, Zahid Islam, Seyit A. Camtepe, Praveen Gauravaram, Dineshkumar Singh, M. Ali Babar, Shihao Yan,
Agriculture 4.0 and beyond: Evaluating cyber threat intelligence sources and techniques in smart farming ecosystems,
Computers & Security,
Volume 140,
2024,
103754,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2024.103754.
(https://www.sciencedirect.com/science/article/pii/S0167404824000555)
Abstract: The digitisation of agriculture, integral to Agriculture 4.0, has brought significant benefits while simultaneously escalating cybersecurity risks. With the rapid adoption of smart farming technologies and infrastructure, the agricultural sector has become an attractive target for cyberattacks. This paper presents a systematic literature review that assesses the applicability of existing cyber threat intelligence (CTI) techniques within smart farming infrastructures (SFIs). We develop a comprehensive taxonomy of CTI techniques and sources, specifically tailored to the SFI context, addressing the unique cyber threat challenges in this domain. A crucial finding of our review is the identified need for a virtual Chief Information Security Officer (vCISO) in smart agriculture. While the concept of a vCISO is not yet established in the agricultural sector, our study highlights its potential significance. The implementation of a vCISO could play a pivotal role in enhancing cybersecurity measures by offering strategic guidance, developing robust security protocols, and facilitating real-time threat analysis and response strategies. This approach is critical for safeguarding the food supply chain against the evolving landscape of cyber threats. Our research underscores the importance of integrating a vCISO framework into smart farming practices as a vital step towards strengthening cybersecurity. This is essential for protecting the agriculture sector in the era of digital transformation, ensuring the resilience and sustainability of the food supply chain against emerging cyber risks.
Keywords: Cyber threat intelligence (CTI); Systematic literature review; virtual Chief Information Security Officer (vCISO); Agriculture 4.0; Agriculture 5.0; Smart farming infrastructures (SFIs); Digital twin technology

Long Luo, Hongfang Yu, Shouxi Luo, Zilong Ye, Xiaojiang Du, Mohsen Guizani,
Scalable explicit path control in software-defined networks,
Journal of Network and Computer Applications,
Volume 141,
2019,
Pages 86-103,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.05.014.
(https://www.sciencedirect.com/science/article/pii/S108480451930181X)
Abstract: In this paper, we present a scalable PAth COntrol (PACO) approach to meet the increasing demands of fine-grained and explicit paths in Software-Defined Networks. PACO builds upon Segment Routing and adopts the pathlet segment as the building blocks for constructing network paths. By proactively generating and pre-installing a small collection of segments in networks, PACO can explicitly and quickly set up a massive number of reactive paths by simply concatenating these segments. We theoretically investigate the proactive segment generation as well as the optimal concatenation for reactive paths and propose efficient algorithms based on the optimal solutions for each of them. Extensive simulations show that PACO outperforms the state-of-the-art SR-based approach by supporting more than 40% explicit paths and achieves a substantial reduction in switch memory (up to 94%) compared with the OpenFlow-based approach.
Keywords: Software-defined networks; Scalability; Explicit path control; Segment routing

Qizhi Zhang, Yale He, Ruilin Lai, Zhihao Hou, Gansen Zhao,
A survey on the efficiency, reliability, and security of data query in blockchain systems,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 303-320,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.03.044.
(https://www.sciencedirect.com/science/article/pii/S0167739X23001309)
Abstract: With the emergence of digital currencies, blockchain systems have been committed to the efficient and trusted storage of data, building decentralized, tamper-proof, persistent, and anonymous digital ledger. Researchers have devoted considerable attention to query methods on blockchain platforms, leading to numerous developments and advancements. There is an imperative need for a comprehensive investigation of all these efforts, as well as their most recent progress and results. This study strives to provide a comprehensive and thorough survey of notable works and recent progress related to query technologies and theories. In general, blockchain query methods rely on utilizing distributed databases, data indexing structures, and cryptographic algorithms to achieve efficient, verifiable, and secure queries. Beyond the above, we examine existing issues of blockchain query technologies and theories on query efficiency, reliability, and security. In the end, this work concludes with a summarization on typical scenarios of blockchain query schemes, as well as a discussion of future challenges to be addressed in future research.
Keywords: Blockchain query; Query reliability; Query security; Data structures; Blockchain applications; Privacy protection

Takanori Hara, Masahiro Sasabe,
Practicality of in-kernel/user-space packet processing empowered by lightweight neural network and decision tree,
Computer Networks,
Volume 240,
2024,
110188,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110188.
(https://www.sciencedirect.com/science/article/pii/S1389128624000203)
Abstract: Integrating machine learning (ML) into kernel packet processing, such as extended Berkeley Packet Filter (eBPF) and eXpress Data Path (XDP), represents a promising strategy for achieving fast and intelligent networking on generic hardware. This includes tasks like automating network operations and discerning traffic classification, exemplified by intrusion detection systems (IDS) combining Decision Tree (DT) and eBPF. However, the potential of ML-empowered packet processing remains to be fully explored. To ensure the integrity and security of kernel operations, eBPF/XDP programs must adhere to stringent constraints such as the maximum number of jump instructions, maximum stack space, and exclusion of floating-point arithmetic. These constraints pose challenges for implementing more intricate ML techniques (e.g., neural networks (NNs)) within eBPF/XDP programs. In such scenarios, AF_XDP provides an alternative solution by allowing XDP programs to redirect packets to user-space applications, bypassing the network stack. This paper initiates an exploration into fast packet classification through two distinct approaches: (1) an in-kernel approach employing eBPF/XDP and (2) a user-space approach assisted by AF_XDP. Specifically, to tackle the eBPF constraints, the in-kernel NN classifier adopts (1) quantization of trained model in the user space, (2) executing the integer-arithmetic-only NN within the kernel space, and (3) sequential layer operations through tail calls. These approaches are evaluated based on factors including packet processing speed, resource efficiency, and detection performance. Notably, our experimental findings demonstrate that (1) Classifiers relying solely on integer arithmetic, such as NN and DT, significantly reduce inference time while maintaining binary classification performance; (2) The lightweight NN classifier can improve the detection performance for most of attacks in case of the multi-class classification compared to the lightweight DT classifier; (3) In single-core scenarios, the DT-empowered in-kernel method can almost achieve the maximum packets per second (pps), i.e., about 800,000pps, whereas the NN-empowered one exhibits lower pps (i.e., about 450,000pps); (4) In multi-core scenarios, the NN-empowered packet processing can almost achieve the maximum pps with two or more cores in the AF_XDP approach and four or more cores in the in-kernel approaches.
Keywords: extended Berkeley Packet Filter (eBPF); eXpress Data Path (XDP); AF_XDP; Intrusion detection system (IDS); Machine learning (ML); Quantization; Quantized neural network (NN); Decision tree (DT)

Piotr Boryło, Edyta Biernacka, Jerzy Domżał, Bartosz Ka̧dziołka, Mirosław Kantor, Krzysztof Rusek, Maciej Skała, Krzysztof Wajda, Robert Wójcik, Wojciech Za̧bek,
A tutorial on reinforcement learning in selected aspects of communications and networking,
Computer Communications,
Volume 208,
2023,
Pages 89-110,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.05.019.
(https://www.sciencedirect.com/science/article/pii/S0140366423001858)
Abstract: Telecommunication systems are increasingly complex, dynamic, and heterogeneous. Tools are needed to efficiently support and automate complex control and management processes. Reinforcement learning becomes one of the most attractive and popular solutions applicable to a wide variety of different aspects of communications and networking. Its development is further boosted by the favorable conditions created by both the existing IT infrastructure and evolving network architectures. The aim of this tutorial is twofold. Firstly, to provide fundamentals regarding the Reinforcement Learning (RL) method. Secondly, to comprehensively study the examples of applying RL-based solutions to solve problems in different aspects of communications and networking. Studies are supplemented with additional explanations, figures and critical considerations, including pros and cons of using selected methods for particular purposes. This part uniquely complements the tutorial one and facilitates in-depth understanding of RL. Based on the conducted studies, we draw a comparative analysis, summaries and expected future research topics and challenges of using RL in communications and networking.
Keywords: Reinforcement learning; Computer networks; Example-based tutorial

Sparsh Mittal, Himanshi Gupta, Srishti Srivastava,
A survey on hardware security of DNN models and accelerators,
Journal of Systems Architecture,
Volume 117,
2021,
102163,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2021.102163.
(https://www.sciencedirect.com/science/article/pii/S1383762121001168)
Abstract: As “deep neural networks” (DNNs) achieve increasing accuracy, they are getting employed in increasingly diverse applications, including security-critical applications such as medical and defense. This immense use of DNNs has motivated the researchers to scrutinizingly study their security vulnerability and propose countermeasures, especially in the context of hardware security. In this paper, we present a survey of techniques for the hardware security of DNNs. For the research works, we highlight the threat-model, key idea for launching attack and defense strategies. We organize the works on salient categories to highlight their strengths and limitations. This paper aims to equip researchers with the knowledge of recent advances in DNN security and motivate them to think of security as the first principle.
Keywords: Hardware security; Trojan; Fault-injection attack; Side-channel attack; Encryption; Deep neural network

Sudarshan Nandy, Mainak Adhikari, Supriya Chakraborty, Ahmed Alkhayyat, Neeraj Kumar,
IBoNN: Intelligent Agent-based Internet of Medical Things framework for detecting brain response from Electroencephalography signal using Bag-of-Neural Network,
Future Generation Computer Systems,
Volume 130,
2022,
Pages 241-252,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.12.019.
(https://www.sciencedirect.com/science/article/pii/S0167739X21005008)
Abstract: The inability of a patient to talk, hear, or both for any specific reason can create a worrisome scenario because any form of reaction can deem the brain activity. In such a scenario, Electroencephalography (EEG) is used to measure a patient’s responsiveness by observing recorded electrical signals on the scalp. However, reading and interpreting EEG signals remotely is a difficult task due to the lack of an intelligent framework in the healthcare domain. Thus to monitor and analyze the EEG signals remotely with an efficient Internet of Medical Things (IoMT) framework and analyze these signals, various machine learning (ML) and deep learning (DL) models have been incorporated. However, the existing ML/DL models do not allow end-users to understand the entire logic behind analyzing those signals that make the IoMT framework decision not transparent. Motivated by the above-mentioned problems, in this paper, an empirical Intelligent Agent-based Bag-of-Neural Networks (IBoNN) model is incorporated in the IoMT framework to make real-time decisions with high accuracy. The IBoNN model is intended to categorize the incoming brain signals based on a collection of neural networks and determine the correct response from EEG signals of the patients. The outperformance of the proposed IBoNN model over the standard ML models is evaluated with a set of performance matrices over a benchmark dataset. From the experimental analysis, it has been observed that the IBoNN model yields an average accuracy of 91%–99% during the categorization of brain responses from EEG signals.
Keywords: Internet of Medical Things; Brain signal classification; Electroencephalography; Neural Network; Classification model; Edge networks

Theodoros Toliopoulos, Nikodimos Nikolaidis, Anna-Valentini Michailidou, Andreas Seitaridis, Theodoros Nestoridis, Chrysa Oikonomou, Anastasios Temperekidis, Fotios Gioulekas, Anastasios Gounaris, Nick Bassiliades, Panagiotis Katsaros, Apostolos Georgiadis, Fotis K. Liotopoulos,
Sboing4Real: A real-time crowdsensing-based traffic management system,
Journal of Parallel and Distributed Computing,
Volume 162,
2022,
Pages 59-75,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.01.017.
(https://www.sciencedirect.com/science/article/pii/S0743731522000193)
Abstract: This work describes the architecture of the back-end engine of a real-time traffic data processing and satellite navigation system. The role of the engine is to process real-time feedback, such as speed and travel time, provided by in-vehicle devices and derive real-time reports and traffic predictions through leveraging historical data as well. We present the main building blocks and the versatile set of data sources and processing platforms that need to be combined together to form a fully-functional and scalable solution. We also present performance results focusing on meeting system requirements while keeping the need for computing resources low. The lessons and results presented are of value to additional real-time applications that rely on both recent and historical data. Finally, we discuss the application of the aforementioned solution to a successful pilot study, where the full system was deployed and processed data from 800 taxis for a period of 3 months.
Keywords: Vehicle traffic monitoring; IoT; Stream processing; Massive parallelism; OLAP

Sparsh Mittal, Subhrajit Nag,
A survey of encoding techniques for reducing data-movement energy,
Journal of Systems Architecture,
Volume 97,
2019,
Pages 373-396,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2018.11.001.
(https://www.sciencedirect.com/science/article/pii/S1383762118304521)
Abstract: In modern processors, data-movement consumes two orders of magnitude higher energy than a floating-point operation and hence, data-movement is becoming the primary bottleneck in scaling the performance of modern processors within the fixed power budget. Intelligent data-encoding techniques hold the promise of reducing the data-movement energy. In this paper, we present a survey of encoding techniques for reducing data-movement energy. By classifying the works on key metrics, we bring out their similarities and differences. This paper is expected to be useful for computer architects, processor designers and researchers in the area of interconnect and memory system design.
Keywords: Data movement; Encoding technique; Energy saving; Sparse code; Limited weight coding; Value locality; Value prediction

Sibi Chakkaravarthy Sethuraman, Tharshith Goud Jadapalli, Devi Priya Vimala Sudhakaran, Saraju P. Mohanty,
Flow based containerized honeypot approach for network traffic analysis: An empirical study,
Computer Science Review,
Volume 50,
2023,
100600,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100600.
(https://www.sciencedirect.com/science/article/pii/S1574013723000679)
Abstract: The world of connected devices has been attributed to applications that relied upon multitude of devices to acquire and distribute data over extremely diverse networks. This caused a plethora of potential threats. In the field of IT security, the concept of digital baits, or honeypots, which are typically network components (computer systems, access points, or switches) launched to be interrogated, savaged, and impacted, is currently popular as it allows scientists to comprehend further on assault patterns and behavior. Combining the inherent modularity with the administration enabled by the container makes security management simple and permits dispersed deployments, resulting in a very dynamic system. This study delivers several contributions in this regard. First, it comprehends the patterns, methods, and malware types that container honeypots deal with thus examining new developments in existing honeypot research to fill gaps in knowledge about the honeypot technology. A broad range of independently initiated and jointly conducted container honeypot strategies and studies that encompass various methodologies is surveyed. Second, using numerous use cases that aid scientific research, we address and investigate a number of challenges pertaining to container honeypots, such as identification problems, honeypot security issues, and dependability issues. Furthermore, based on our extensive honeypot research, we developed VIKRANT, a containerized research honeypot which assists researchers as well as enthusiasts in generating real-time flow data for threat intelligence. The configured approach was monitored resulting in several data points that allowed relevant conclusions about the malevolent users’ activities.
Keywords: Cyber security; IDS; Deception; Virtualization; Containers; Honeypots; Flow data

Sofiane Zaidi, Mohammed Atiquzzaman, Carlos T. Calafate,
Internet of Flying Things (IoFT): A Survey,
Computer Communications,
Volume 165,
2021,
Pages 53-74,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.10.023.
(https://www.sciencedirect.com/science/article/pii/S014036642031971X)
Abstract: Unmanned Aerial Vehicles (UAVs) have recently received significant attention by the civilian and military community, mostly due to the fast growth of UAV technologies supported by wireless communications and networking. UAVs can be used to improve the efficiency and performance of the Internet of Things (IoT) in terms of connectivity, coverage, reliability, stability, etc. In particular, to support IoT applications in an efficient manner, UAVs should be organized as a Flying Ad-hoc NETwork (FANET). FANET is a subclass of Mobile Ad-hoc Network (MANET) where nodes are Unmanned Artifact Systems (UAS). However, the deployment of UAVs in IoT is limited by several constraints, such as limited resource capacity of UAVs and ground devices, signal collision and interference, intermittent availability of the IoT infrastructure, etc. In the Internet of Flying Things (IoFT) literature, there are no survey or study that exhaustively covers and discusses all key concepts and recent works on IoFT. In this paper a comprehensive survey on the IoFT is presented, covering the state of the art in flying things with a focus on IoFT. A taxonomy of related literature on IoFT is proposed, including a classification, description and comparative study of different work on IoFT. Furthermore, the paper presents IoFT applications, IoFT challenges and future perspectives. This survey aims to provide the basic concepts and a complete overview of the recent studies on IoFT for the scientific researchers.
Keywords: Internet of Flying Things; Unmanned Aerial Vehicle; Unmanned Artifact System; Internet of Things; Flying Ad-hoc NETwork

Diksha Moolchandani, Anshul Kumar, Smruti R. Sarangi,
Accelerating CNN Inference on ASICs: A Survey,
Journal of Systems Architecture,
Volume 113,
2021,
101887,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101887.
(https://www.sciencedirect.com/science/article/pii/S1383762120301612)
Abstract: Convolutional neural networks (CNNs) have proven to be a disruptive technology in most vision, speech and image processing tasks. Given their ubiquitous acceptance, the research community is investing a lot of time and resources on deep neural networks. Custom hardware such as ASICs are proving to be extremely worthy platforms for running such programs. However, the ever-increasing complexity of these algorithms poses challenges in achieving real-time performance. Specifically, CNNs have prohibitive costs in terms of computation time, throughput, latency, storage space, memory bandwidth, and power consumption. Hence, in the last 5 years, a lot of work has been done by the scientific community to mitigate these costs. Researchers have primarily focused on reducing the computation time, the number of computations, the memory access time, and the size of the memory footprint. In this survey paper, we propose a novel taxonomy to classify prior work, and describe some of the key contributions in these areas in detail.
Keywords: CNN; Inferencing; ASICs; Accelerators

B. Martini, M. Gharbaoui, P. Castoldi,
Intent-based network slicing for SDN vertical services with assurance: Context, design and preliminary experiments,
Future Generation Computer Systems,
Volume 142,
2023,
Pages 101-116,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.12.033.
(https://www.sciencedirect.com/science/article/pii/S0167739X2200437X)
Abstract: Network slicing is announced to be one of the key features for 5G infrastructures enabling network operators to provide network services with the flexibility and dynamicity necessary for the vertical services, while relying on Network Function Virtualization (NFV) and Software-defined Networking (SDN). On the other hand, vertical industries are attracted by flexibility and customization offered by operators through network slicing, especially if slices come with in-built SDN capabilities to programmatically connect their application components and if they are relieved of dealing with detailed technicalities of the underlying (virtual) infrastructure. In this paper, we present an Intent-based deployment of a NFV orchestration stack that allows for the setup of Qos-aware and SDN-enabled network slices toward effective service chaining in the vertical domain. The main aim of the work is to simplify and automate the deployment of tenant-managed SDN-enabled network slices through a declarative approach while abstracting the underlying implementation details and unburdening verticals to deal with technology-specific low-level networking directives. In our approach, the intent-based framework we propose is based on an ETSI NFV MANO platform and is assessed through a set of experimental results demonstrating its feasibility and effectiveness.
Keywords: SDN; NFV; Intent-based networking; Network slicing; MANagement and Orchestration; ETSI NFV MANO

Tong Shu, Chase Q. Wu,
Energy-efficient mapping of large-scale workflows under deadline constraints in big data computing systems,
Future Generation Computer Systems,
Volume 110,
2020,
Pages 515-530,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2017.07.050.
(https://www.sciencedirect.com/science/article/pii/S0167739X17300468)
Abstract: Large-scale workflows for big data analytics have become a main consumer of energy in data centers where moldable parallel computing models such as MapReduce are widely applied to meet high computational demands with time-varying computing resources. The granularity of task partitioning in each moldable job of such big data workflows has a significant impact on energy efficiency, which remains largely unexplored. In this paper, we analyze the properties of moldable jobs and formulate a workflow mapping problem to minimize the dynamic energy consumption of a given workflow request under a deadline constraint in big data systems. Since this problem is strongly NP-hard, we design a fully polynomial-time approximation scheme (FPTAS) for a special case with a pipeline-structured workflow on a homogeneous cluster and a heuristic for the generalized problem with an arbitrary workflow on a heterogeneous cluster. The performance superiority of the proposed solution in terms of dynamic energy saving and deadline missing rate is illustrated by extensive simulation results in comparison with existing algorithms, and further validated by real-life workflow implementation and experimental results in Hadoop/YARN systems.
Keywords: Big data; Workflow mapping; Green computing

Juan Carlos Salinas-Hilburg, Marina Zapater, José M. Moya, José L. Ayala,
Fast energy estimation framework for long-running applications,
Future Generation Computer Systems,
Volume 115,
2021,
Pages 20-33,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.08.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X20305380)
Abstract: The computation power in data center facilities is increasing significantly. This brings with it an increase of power consumption in data centers. Techniques such as power budgeting or resource management are used in data centers to increase energy efficiency. These techniques require to know beforehand the energy consumption throughout a full profiling of the applications. This is not feasible in scenarios with long-running applications that have long execution times. To tackle this problem we present a fast energy estimation framework for long-running applications. The framework is able to estimate the dynamic CPU and memory energy of the application without the need to perform a complete execution. For that purpose, we leverage the concept of application signature. The application signature is a reduced version, in terms of execution time, of the original application. Our fast energy estimation framework is validated with a set of long-running applications and obtains RMS values of 11.4% and 12.8% for the CPU and memory energy estimation errors, respectively. We define the concept of Compression Ratio as an indicator of the acceleration of the energy estimation process. Our framework is able to obtain Compression Ratio values in the range of 10.1 to 191.2.
Keywords: Energy efficiency; Data centers; Application signature; Energy estimation

Daniel Castro, Paolo Romano, João Barreto,
Hardware Transactional Memory meets memory persistency,
Journal of Parallel and Distributed Computing,
Volume 130,
2019,
Pages 63-79,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.03.009.
(https://www.sciencedirect.com/science/article/pii/S0743731518303952)
Abstract: Persistent Memory (PM) and Hardware Transactional Memory (HTM) are two recent architectural developments whose joint usage promises to drastically accelerate the performance of concurrent, data-intensive applications. Unfortunately, combining these two mechanisms using existing architectural supports is far from being trivial. This paper presents NV-HTM, a system that allows the execution of transactions over PM using unmodified commodity HTM implementations. NV-HTM exploits a hardware–software co-design technique, which is based on three key ideas: (i) relying on software to persist transactional modifications after they have been committed via HTM; (ii) postponingthe externalization of commit events to applications until it is ensured, via software, that any data version produced and observed by committed transactions is first logged in PM; (ii) pruning the commit logs via checkpointing schemes that not only bound the log space and recovery time, but also implement wear leveling techniques to enhance PM’s endurance. By means of an extensive experimental evaluation, we show that NV-HTM can achieve up to 10× speed-ups and up to 11.6× reduced flush operations with respect to state of the art solutions, which, unlike NV-HTM, require custom modifications to existing HTM systems.
Keywords: Transaction; Memory; Persistent; Hardware; System

Matteo Repetto,
Adaptive monitoring, detection, and response for agile digital service chains,
Computers & Security,
Volume 132,
2023,
103343,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103343.
(https://www.sciencedirect.com/science/article/pii/S0167404823002535)
Abstract: Modern business is increasingly adopting fully-digital workflows composed of complementary services (in terms of infrastructures, software, networks, data and devices) from different domains, hence giving rise to complex and heterogeneous digital chains. The substantial fragmentation in service operation and ownership between these domains impacts cybersecurity operations, by hindering a coherent and cooperative defense strategy for the entire chain. As a result, this situation gives attackers more opportunity to move laterally within the chain once they have found and compromised the weakest link. A ground-breaking evolution of legacy cybersecurity processes is necessary towards collaborative and adaptive models that fit the dynamic, agile, and heterogeneous nature of federated environments. In this paper, we elaborate on the necessary convergence between complementary workflows for response, analysis, and intelligence, by considering the peculiarity of these operations and the relevant threat scenario. Our analysis points out the main research challenges to fill the existing gap between management and protection practice for digital service chains. Moreover, we outline a reference architecture that combines such workflows. The objective is to foster researchers to broaden the scope of their work, in order to address open security issues for modern business and computing paradigms.
Keywords: Security orchestration and autonomous response; Digital service chain; Cyber-threat intelligence; Proactive forensics; Predictive analytics

Fariba Ghaffari, Emmanuel Bertin, Noel Crespi, Julien Hatin,
Distributed ledger technologies for authentication and access control in networking applications: A comprehensive survey,
Computer Science Review,
Volume 50,
2023,
100590,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2023.100590.
(https://www.sciencedirect.com/science/article/pii/S1574013723000576)
Abstract: The accelerated growth of networking technologies highlights the importance of Authentication and Access Control (AAC) as protection against associated attacks. Controlling access to resources, facilitating resource sharing, and managing user mobility are some of the notable capabilities provided by AAC methods. Centralized methods are the most common deployment architectures, that can be threatened by several attacks at their central points. Emerging Distributed Ledger Technology (DLT) has attracted significant interest in the AAA community. The distributed nature of DLT and its immutability can bring unprecedented opportunities to resolve many of the challenges of conventional systems. We survey the state-of-the-art in deploying authentication and access control approaches via DLT for several networking use cases. More precisely, we explore DLT applications in (1) Authentication; (2) Access Control; and (3) Comprehensive AAC solutions. First, we present the challenges of centralized solutions and discuss the capability of DLT for their resolution. Then, we propose a taxonomy to categorize the existing methods. Analysis, comparison, and discussion on the advantages and disadvantages of these methods have been provided regarding different parameters such as DLT types, AAC approaches, security, reliability, scalability, etc. While DLT provides various benefits, several challenges remain for the migration to DLT-based AAC. In light of these general limitations, we propose some future directions, targeting the current lacunae and future needs.
Keywords: Authentication; Access control; Networking applications; Distributed ledger technology; Blockchain; Smart contract; Security; Privacy; Taxonomy

Meriem Achir, Abdelkrim Abdelli, Lynda Mokdad, Jalel Benothman,
Service discovery and selection in IoT: A survey and a taxonomy,
Journal of Network and Computer Applications,
Volume 200,
2022,
103331,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103331.
(https://www.sciencedirect.com/science/article/pii/S1084804521003167)
Abstract: Recently, Internet has evolved into a new generation, called Internet of Things, thus enabling the connection between the physical and the digital worlds by creating an ubiquitous and self-organizing network. A huge number of smart objects are becoming now identifiable and addressable while being able to communicate with each other. Moreover, the integration of cloud infrastructures in the design of IoT, has moved this new trademark technology into a new dimension, enabling virtualisation and service provisioning. Billions of cloud services with different performance levels, requirements and functionalities are thus being offered in IoT, raising however the issues of their management, discovery and selection. In the literature, a considerable effort has been invested to address service discovery and selection in the context of IoT, despite the lack of standardization that meets the IoT requirements. In this paper, we propose an exhaustive taxonomy to classify service discovery approaches in the context of IoT, that we subsequently evaluate according to different aspects and criteria. Then, we discuss the gaps and advantages of each class of our taxonomy and locate the context and the requirements under which each can operate. Finally, we identify the challenges and future research directions in this domain.
Keywords: Taxonomy; Service discovery; Service selection; IoT; QoS; QoE; Classification; Architecture; Object discovery

Rui Xu, Edwin Hsing-Mean Sha, Qingfeng Zhuge, Yuhong Song, Han Wang,
Loop interchange and tiling for multi-dimensional loops to minimize write operations on NVMs,
Journal of Systems Architecture,
Volume 135,
2023,
102799,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102799.
(https://www.sciencedirect.com/science/article/pii/S1383762122002843)
Abstract: Non-volatile memory (NVM) is expected to be the second tier of memory in two-tier memory systems. However, because of the limited write endurance, it is vital to reduce the number of writes on NVM. Large-scale nested loops are the performance bottleneck in programs since the data cannot be held on the first tier of memory and then causes many write operations on NVM. Loop tiling groups iterations and loop interchange changes the execution order of the loop to improve data locality and thus reduce communication with NVM. However, research that combines loop interchange and loop tiling for minimizing writes on NVM is uncommon. In this paper, we propose a new loop tiling scheme and combine the loop interchange to solve these issues. Specifically, we propose a strategy to generate the legal tile shape, which is a parallelogram, first. Then, we propose an optimal tile size selection technique to minimize the write operations on NVM. In addition, we adopt the loop interchange technique to help loop tiling generate an optimal tile size for multi-dimensional loops. Finally, we schedule the accessing operations and computations in a pipeline fashion to cover the NVM latency. Experiments show that the proposed scheme can reduce the write on NVM efficiently. In addition, for 2-dimensional loops, NVM latency can be completely hidden.
Keywords: Loop tiling; Write operations; Loop interchange; Pipeline; Non-volatile memory

Gabriele D'Angelo, Stefano Ferretti,
Adaptive parallel and distributed simulation of complex networks,
Journal of Parallel and Distributed Computing,
Volume 163,
2022,
Pages 30-44,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2022.01.022.
(https://www.sciencedirect.com/science/article/pii/S0743731522000284)
Abstract: Complex networks are an important methodology to model several (if not all) aspects of the real world, in which multiple entities interact, in some way. While many aspects related to such interactions can be investigated by looking at the general mathematical metrics of the networks, an alternative approach lies in the simulation of some application protocol on top of (large scale) complex networks. In this paper, we present a study on this intricate problem. The complexity of the simulation is due to the need to model all the interactions among network nodes. We focus on discrete-event simulation, a simulation methodology that enables both sequential (i.e. monolithic) and Parallel And Distributed Simulation (i.e. PADS) approaches. We discuss the performance and scalability requirements that the simulator should have. We also introduce a case study based on the agent-based simulation of gossip dissemination on top of a complex network. To demonstrate the viability of this simulation technique, we focus on a tool we built to simulate complex networks. The tool exploits adaptive partitioning mechanisms, which are essential to reduce the communication overhead in the PADS. An experimental evaluation has been conducted using different network topologies and simulator setups. Results demonstrate the feasibility of the approach to simulate complex networks.
Keywords: Simulation; Complex networks; Parallel and distributed simulation

Jessie Hui Wang, Jilong Wang, Changqing An, Qianli Zhang,
A survey on resource scheduling for data transfers in inter-datacenter WANs,
Computer Networks,
Volume 161,
2019,
Pages 115-137,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.06.011.
(https://www.sciencedirect.com/science/article/pii/S1389128618303931)
Abstract: Today, datacenter providers spend a lot of money on inter-datacenter WANs to transmit traffic flows between geographically distributed datacenter sites. It is very important for datacenter providers to design resource scheduling schemes to schedule inter-datacenter networking resources to satisfy data transfer requests and achieve maximum performance or economic benefits. In this article, we conduct a review study on research efforts being conducted on the inter-datacenter networking resource scheduling problem. Our survey study is presented in three parts. The first part focuses on formulating the problem theoretically, including the definition and features of a data transfer, network models of inter-DC WANs, scheduling objectives, and scheduling dimensions. We also summarize a conceptual implementation architecture of scheduling systems. Second, we classify existing schemes according to their objectives and scheduling dimensions and then explain how they formulate and solve their own problems. In the third part, we examine practical challenges in developing scheduling systems and also point out some future directions. Since we do not see any survey focusing on the scheduling problem of inter-DC WANs, we believe this article can provide some help for research developments in this important field by organizing logically the recent research efforts from industry and academia.
Keywords: Inter-DC WAN; Scheduling; Resource allocation; Traffic engineering; Datacenter

Paola G. Vinueza Naranjo, Zahra Pooranian, Mohammad Shojafar, Mauro Conti, Rajkumar Buyya,
FOCAN: A Fog-supported smart city network architecture for management of applications in the Internet of Everything environments,
Journal of Parallel and Distributed Computing,
Volume 132,
2019,
Pages 274-283,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.07.003.
(https://www.sciencedirect.com/science/article/pii/S0743731518304775)
Abstract: Smart city vision brings emerging heterogeneous communication technologies such as Fog Computing (FC) together to substantially reduce the latency and energy consumption of Internet of Everything (IoE) devices running various applications. The key feature that distinguishes the FC paradigm for smart cities is that it spreads communication and computing resources over the wired/wireless access network (e.g., proximate access points and base stations) to provide resource augmentation (e.g., cyberforaging) for resource- and energy-limited wired/wireless (possibly mobile) things. Motivated by these considerations, this paper presents a Fog-supported smart city network architecture called Fog Computing Architecture Network (FOCAN), a multi-tier structure in which the applications are running on things thatjointly compute, route, and communicate with one another through the smart city environment. FOCAN decreases latency and improves energy provisioning and the efficiency of services among things with different capabilities. In particular, three types of communications are defined between FOCAN devices – interprimary, primary, and secondary communication –to manage applications in a way that meets the quality of service standards for the Internet of Everything. One of the main advantages of the proposed architecture is that the devices can provide the services with low energy usage and in an efficient manner. Simulation results for a selected case study demonstrate the tremendous impact of the FOCAN energy-efficient solution on the communication performance of various types of things in smart cities.
Keywords: Smart city; Internet of Everything (IoE); Fog computing; Routing algorithm; Computing and communication

Gongshun Min, Liang Liu, Wenbin Zhai, Zijie Wang, Wanying Lu,
An efficient data collection algorithm for partitioned wireless sensor networks,
Future Generation Computer Systems,
Volume 140,
2023,
Pages 53-66,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.09.006.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002898)
Abstract: Data collection with mobile agent (MA) can balance the energy consumption of nodes in partitioned wireless sensor networks. However, the existing data collection algorithms for partitioned WSNs do not formalize the problem systematically. These algorithms are not efficient and difficult to meet the timeliness requirement. In order to solve the above shortcomings, we first formalize the Data Collection Problem (DCP). Then, it is transformed into a Generalized Traveling Salesman Problem (GTSP), and we propose a GTSP-Based data collection Algorithm (GBA) to calculate the rendezvous points (RPs) and the moving path of MA. GBA selects RPs by solving the GTSP problem. Furthermore, based on GBA, we take advantage of constructing convex hull to plan a more efficient moving path for MA. In addition, in order to reduce the energy burden on the RPs, we design an Improved Shortest Path tree (ISP tree) to aggregate data from nodes to RPs in partitioned WSNs. Finally, extensive simulations demonstrate the effectiveness and advantages of our algorithm in terms of the length of MA’s moving path and the amount of data collection per unit time.
Keywords: Convex hull; Data collection; GTSP(Generalized Traveling Salesman Problem); ISP tree(Improved Shortest Path tree); MA (Mobile Agent); Partitioned WSNs(partitioned Wireless Sensor Networks)

Jean Nestor M. Dahj, Kingsley A. Ogudo, Leandro Boonzaaier,
A hybrid analytical concept to QoE index evaluation: Enhancing eMBB service detection in 5G SA networks,
Journal of Network and Computer Applications,
Volume 221,
2024,
103765,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103765.
(https://www.sciencedirect.com/science/article/pii/S1084804523001844)
Abstract: The launch of commercial 5G networks has unlocked numerous opportunities for heavy data users and high-speed applications. The expected requirements for enhanced mobile broadband (eMBB) are pushing end-users to adopt 5G optimistically. Though already deployed 5G networks have shown high data rates and very low latency, the service-based experience and application behavior have been challenging to monitor. The legacy quality of experience (QoE) and service (QoS) monitoring and evaluation techniques have shown limitations in 5G standalone networks. The current 5G deployment large amount of user plane traffic generated by end-users makes the legacy-monitoring task very costly for mobile network operators (MNOs). And the complexity of the projected future 5G architecture, including advanced technologies such as network functions virtualization (NFV), software-defined networking (SDN), and network slicing, makes traditional service detection and QoE assessment ineffective. In this paper, we discuss a cost-effective hybrid analytical approach to eMBB service detection, analysis, and perceived user QoE measurement from raw traffic in a live 5G standalone (SA) network. We first use flow-level-based packet inspection and machine learning to detect and classify eMBB services from raw traffic. We then use a statistical approach to compute the user quality index (UQI). The concept is tested on traffic captured on a fixed 5G SA network. And the output enabled the MNO to have a 5G QoE assessment structure and awareness to adjust network traffic policies.
Keywords: 5G standalone network; Enhanced mobile broadband (eMBB); Machine learning; Network functions virtualization (NFV); Service awareness; Quality of service (QoS); Quality of experience (QoE); Software-defined networks (SDN)

Qiliang Li, Min Lyu, Liangliang Xu, Yinlong Xu,
Fast recovery for large disk enclosures based on RAID2.0: Algorithms and evaluation,
Journal of Parallel and Distributed Computing,
Volume 188,
2024,
104854,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2024.104854.
(https://www.sciencedirect.com/science/article/pii/S0743731524000182)
Abstract: The RAID2.0 architecture, which uses dozens or even hundreds of disks, is widely adopted for large-capacity data storage. However, limited resources like memory and CPU cause RAID2.0 to execute batch recovery for disk failures. The traditional random data placement and recovery schemes result in highly skewed I/O access within a batch, which slows down the recovery speed. To address this issue, we propose DR-RAID, an efficient reconstruction scheme that balances local rebuilding workloads across all surviving disks within a batch. We dynamically select a batch of tasks with almost balanced read loads and make intra-batch adjustments for tasks with multiple solutions of reading source chunks. Furthermore, we use a bipartite graph model to achieve a uniform distribution of write loads. DR-RAID can be applied with homogeneous or heterogeneous disk rebuilding bandwidth. Experimental results demonstrate that in offline rebuilding, DR-RAID enhances the rebuilding throughput by up to 61.90% compared to the random data placement scheme. With varied rebuilding bandwidth, the improvement can reach up to 65.00%.
Keywords: RAID; RAID2.0; Disk failure; Recovery

Nikolay Chervyakov, Mikhail Babenko, Andrei Tchernykh, Nikolay Kucherov, Vanessa Miranda-López, Jorge M. Cortés-Mendoza,
AR-RRNS: Configurable reliable distributed data storage systems for Internet of Things to ensure security,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 1080-1092,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2017.09.061.
(https://www.sciencedirect.com/science/article/pii/S0167739X17306015)
Abstract: Benefits of Internet of Things and cloud–fog-edge computing are associated with the risks of confidentiality, integrity, and availability related with the loss of information, denial of access for a long time, information leakage, conspiracy and technical failures. In this article, we propose a configurable, reliable, and confidential distributed data storage scheme with the ability to process encrypted data and control results of computations. Our system utilizes Redundant Residue Number System (RRNS) with new method of error correction codes and secret sharing schemes. We introduce the concept of an approximate value of a rank of a number (AR), which allows us to reduce the computational complexity of the decoding from RNS to binary representation, and size of the coefficients. Based on the properties of the approximate value and arithmetic properties of RNS, we introduce AR-RRNS method for error detection, correction, and controlling computational results. We provide a theoretical basis to configure probability of information loss, data redundancy, speed of encoding and decoding to cope with different objective preferences, workloads, and storage properties. Theoretical analysis shows that by appropriate selection of RRNS parameters, the proposed scheme allows not only increasing safety, reliability, and reducing an overhead of data storage, but also processing of encrypted data.
Keywords: Big data storage; Multi-cloud; Internet of Things; Resource management; Security; Safety; Reliability; Residue number system

Dimitrios J. Vergados, Katina Kralevska, Yuming Jiang, Angelos Michalas,
Local voting: A new distributed bandwidth reservation algorithm for 6TiSCH networks,
Computer Networks,
Volume 180,
2020,
107384,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107384.
(https://www.sciencedirect.com/science/article/pii/S1389128619314628)
Abstract: The IETF 6TiSCH working group fosters the adaptation of IPv6-based protocols into Internet of Things by introducing the 6TiSCH Operation Sublayer (6top). The 6TiSCH architecture integrates the high reliability and low-energy consumption of IEEE 802.15.4e Time Slotted Channel Hopping (TSCH) with IPv6. IEEE 802.15.4e TSCH defines only the communication between nodes through a schedule but it does not specify how the resources are allocated for communication between the nodes in 6TiSCH networks. We propose a distributed algorithm for bandwidth allocation, called Local Voting, that adapts the schedule to the network conditions. The algorithm tries to equalize the link load (defined as the ratio of the queue length plus the new packet arrivals, over the number of allocated cells) through cell reallocation by calculating the number of cells to be added or released by 6top. Simulation results show that equalizing the load throughout 6TiSCH network provides better fairness in terms of load, reduces the queue sizes and packets reach the root faster compared to representative algorithms from the literature. Local Voting combines good delay performance and energy efficiency that are crucial features for Industrial Internet-of-Things applications.
Keywords: IoT; IEEE 802.15.4e; 6TiSCH; networks; TSCH; 6top; Load balancing; Resource allocation

Jie Sun, Tianyu Wo, Xudong Liu, Tianjiao Ma, Xudong Mou, Jinghong Lan, Nan Zhang, Jianwei Niu,
Scalable inter-domain network virtualization,
Journal of Network and Computer Applications,
Volume 218,
2023,
103701,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103701.
(https://www.sciencedirect.com/science/article/pii/S1084804523001200)
Abstract: To realize inter-domain network virtualization for hybrid cloud, the following challenges must be resolved. (1) Scalability. The network virtualization system should allow tenant virtual networks to use on-demand addressing schema and span multiple domains. This requirement, combined with the expansion of the network scale, can lead to a rapid increase in flow rule consumption, posing a significant challenge for network scalability. (2) Inter-domain network flow scheduling. To allocate bandwidth for inter-domain network flows with low cost and guaranteed bandwidth is complex (NP-hard). Existing researches fail to address this problem in a flexible yet efficient manner. This paper proposes a network virtualization solution that jointly considers the two challenges. Towards the scalability challenge, we use MAC translation to encode location hierarchy and tenant information into MAC addresses, thereby enabling L2 forwarding rule aggregation on each switch. Besides, we offload the L3 virtual network rules onto the first hop software switches and tackle L3 routing through VMAC-based forwarding. We formally prove that our approach can bound the worst-case rule consumption by combining these two techniques. Towards the second challenge, we formulate the inter-domain network flow scheduling (INFOS) problem into an integer linear programming (ILP) problem and prove its NP-hardness. We also propose a heuristic algorithm that can guarantee bandwidth lower-bound while avoiding causing significant profit loss or introducing too many network updates. Evaluation shows that our approach can reduce the average flow rule consumption significantly, compared with state-of-the-art approaches. Besides, towards the same bandwidth guarantee target, we can avoid up to 94.4% of the network update operations.
Keywords: Network virtualization; Inter domain; Tenant isolation; Scalability; Bandwidth allocation; Flow rule

TianZhang He, Adel N. Toosi, Rajkumar Buyya,
Performance evaluation of live virtual machine migration in SDN-enabled cloud data centers,
Journal of Parallel and Distributed Computing,
Volume 131,
2019,
Pages 55-68,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.04.014.
(https://www.sciencedirect.com/science/article/pii/S074373151830474X)
Abstract: In Software-Defined Networking (SDN) enabled cloud data centers, live VM migration is a key technology to facilitate the resource management and fault tolerance. Despite many research focus on the network-aware live migration of VMs in cloud computing, some parameters that affect live migration performance are neglected to a large extent. Furthermore, while SDN provides more traffic routing flexibility, the latencies within the SDN directly affect the live migration performance. In this paper, we pinpoint the parameters from both system and network aspects affecting the performance of live migration in the environment with OpenStack platform, such as the static adjustment algorithm of live migration, the performance comparison between the parallel and the sequential migration, and the impact of SDN dynamic flow scheduling update rate on TCP/IP protocol. From the QoS view, we evaluate the pattern of client and server response time during the pre-copy, hybrid post-copy, and auto-convergence based migration.
Keywords: Live VM migration; Software-Defined Networking; Cloud computing; Virtual machine; Performance measures; OpenStack; OpenDaylight

Mirosław Kantor, Edyta Biernacka, Piotr Boryło, Jerzy Domżał, Piotr Jurkiewicz, Miłosz Stypiński, Robert Wójcik,
A survey on multi-layer IP and optical Software-Defined Networks,
Computer Networks,
Volume 162,
2019,
106844,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.06.022.
(https://www.sciencedirect.com/science/article/pii/S1389128618314312)
Abstract: As Software-Defined Networks become more and more popular, they start to appear in various architectures. The most common utilization of SDN is to control the network in the electric layers, including OSI/ISO layers 2 and above. However, SDN can be used much further. Professionals and scientists noticed that combining management and control of many layers at the same time can provide benefits. At one point, SDN started to be envisioned for multi-layer networks. In this survey we present, compare and contrast solutions that utilize SDN in multi-layer network architectures. The main objective is to analyze evolving multi-layer network architectures and show how these solutions coupled with SDN contribute to make future networks simple, flexible and cost-effective.
Keywords: SDN; Software-Defined Networking; Multi-layer; Optical networks

Yashwant Singh Patel, Jatin Bedi,
MAG-D: A multivariate attention network based approach for cloud workload forecasting,
Future Generation Computer Systems,
Volume 142,
2023,
Pages 376-392,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.01.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X2300002X)
Abstract: The Coronavirus pandemic and the work-from-home have drastically changed the working style and forced us to rapidly shift towards cloud-based platforms & services for seamless functioning. The pandemic has accelerated a permanent shift in cloud migration. It is estimated that over 95% of digital workloads will reside in cloud-native platforms. Real-time workload forecasting and efficient resource management are two critical challenges for cloud service providers. As cloud workloads are highly volatile and chaotic due to their time-varying nature; thus classical machine learning-based prediction models failed to acquire accurate forecasting. Recent advances in deep learning have gained massive popularity in forecasting highly nonlinear cloud workloads; however, they failed to achieve excellent forecasting outcomes. Consequently, demands for designing more accurate forecasting algorithms exist. Therefore, in this work, we propose ’MAG-D’, a Multivariate Attention and Gated recurrent unit based Deep learning approach for Cloud workload forecasting in data centers. We performed an extensive set of experiments on the Google cluster traces, and we confirm that MAG-DL exploits the long-range nonlinear dependencies of cloud workload and improves the prediction accuracy on average compared to the recent techniques applying hybrid methods using Long Short Term Memory Network (LSTM), Convolutional Neural Network (CNN), Gated Recurrent Units (GRU), and Bidirectional Long Short Term Memory Network (BiLSTM).
Keywords: Resources’ utilization; Deep learning; Prediction approaches; Cloud data centers; Energy-efficiency; Time-series

Jian Wang, Yongxin Liu, Shuteng Niu, Houbing Song, Weipeng Jing, Jiawei Yuan,
Blockchain enabled verification for cellular-connected unmanned aircraft system networking,
Future Generation Computer Systems,
Volume 123,
2021,
Pages 233-244,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.05.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X21001461)
Abstract: The emerging 5G New Radio (5G NR) stimulates the evolution of applications in many fields. Compared with 4G, 5G NR can provide much more reliable, efficient, flexible networking access services to mobile devices which can escalate the quality of services (QoS) for the users remarkably. The advantages of affordability, easy assembling and quick driving, allow the deployment of Unmanned Aircraft System (UAS) on a large scale, and provide the elevation for the industrial and the civilian implementations ubiquitously. With the enhancement of cellular networking, the control range of UAS can extend significantly which allows the controller finish their mission far away from workplace and prevents the threats derived from the hazards in the uncertain environment. The Base Stations (BSs) are vital to the cellular-connected UAS because the BSs can provide the Internet access for the UAS and the remote controllers that sends instructions to the UAS and backhauls the packets to the controllers precisely and timely. However, the malicious BSs are great threats to the cellular-connected UAS. Specifically, the malicious BSs are scattered in the legislative BSs and cannot provide formal networking access services to the UAS. What is worse, the attackers can leverage the malicious BSs to disclose the controllers’ privacy and set threats to the public property. In this paper, we propose a novel blockchain-based approach to mitigate the threats from accessing the malicious BSs. We implement the consensus construction on the authentication of networking access to enhance the efficiency of verification between BSs. To achieve the security of cellular connected UAS, we deploy blockchain to maintain the high online rate for accessing networking. Apart from the prevention of accessing malicious BSs, our approach can detect the malicious BSs and eliminate the proportion of the malicious BSs in the cellular networking. The evaluation shows that the proposed approach outperforms the conventional point-to-point authentication method. Concurrently, we enhanced the verification between BSs and the cellular-connected UAS. Our proposed blockchain enabled verification for cellular UAS networking has high potentials to rise the efficiency and the security of the UAS deployment on a large scale.
Keywords: Blockchain; Access authentication; UAS; Cellular networking; Verification

Alex Barcelo, Anna Queralt, Toni Cortes,
Revisiting active object stores: Bringing data locality to the limit with NVM,
Future Generation Computer Systems,
Volume 129,
2022,
Pages 425-439,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.10.025.
(https://www.sciencedirect.com/science/article/pii/S0167739X21004210)
Abstract: Object stores are widely used software stacks that achieve excellent scale-out with a well-defined interface and robust performance. However, their traditional get/put interface is unable to exploit data locality at its fullest, and limits reaching its peak performance. In particular, there is one way to improve data locality that has not yet achieved mainstream adoption: the active object store. Although there are some projects that have implemented the main idea of the active object store such as Swift’s Storlets or Ceph Object Classes, the scope of these implementations is limited. We believe that there is a huge potential for active object stores in the current status quo. Hyper-converged nodes are bringing more computing capabilities to storage nodes — and vice versa. The proliferation of non-volatile memory (NVM) technology is blurring the line between system memory (fast and scarce) and block devices (slow and abundant). More and more applications need to manage a sheer amount of data (data analytics, Big Data, Machine Learning & AI, etc.), demanding bigger clusters and more complex computations. All these elements are potential game changers that need to be evaluated in the scope of active object stores. More specifically, having NVM devices presents additional opportunities, such as in-place execution. Being able to use the NVM from within the storage system while taking advantage of in-place execution (thanks to the byte-addressable nature of the NVM), in conjunction with the computing capabilities of hyper-converged nodes, can lead to active object stores that greatly outperform their non-active counterparts. In this article we propose an active object store software stack and evaluate it on an NVM-populated node. We will show how this setup is able to reduce execution times from 10% up to more than 90% in a variety of representative application scenarios. Our discussion will focus on the active aspect of the system as well as on the implications of the memory configuration.
Keywords: Object store; Active storage system; Non-volatile memory; High-performance computing; Data analytics; Big data

Ya-Ju Yu, Shan-Yao Lo,
Energy-efficient non-anchor channel allocation in NB-IoT cellular networks,
Computer Networks,
Volume 239,
2024,
110145,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110145.
(https://www.sciencedirect.com/science/article/pii/S138912862300590X)
Abstract: In Narrowband Internet of Things (NB-IoT), a base station can provide multiple non-anchor channels to release the burden of anchor channels. NB-IoT devices can use non-anchor channels to execute data transmission to save energy. However, we creatively observe a new important property in the NB-IoT; allocating more non-anchor channels to a base station may even cause an increase in the energy consumption of devices during the data transmission procedure. Unlike traditional channel assignment problems, this new property in the NB-IoT leads to the non-anchor channel allocation being a non-convex problem. This article studies the energy-efficient non-anchor channel allocation in the NB-IoT to minimize the energy consumption of devices to finish their data requests. Because of the non-convex property, this article proposes a dynamic programming algorithm without considering frequency reuse to determine how many non-anchor channels each base station should use. We prove that the dynamic programming algorithm can get an optimal solution. Then, we propose an energy-efficient non-anchor channel reuse algorithm to minimize the energy consumption of devices. Compared with two baselines, the simulation results show that the proposed algorithms reduce the energy consumption by 66%, and blind decoding consumes the most energy of a device.
Keywords: NB-IoT; Energy efficiency; Non-anchor channels; Channel allocation; Cellular networks

N. Deepa, Quoc-Viet Pham, Dinh C. Nguyen, Sweta Bhattacharya, B. Prabadevi, Thippa Reddy Gadekallu, Praveen Kumar Reddy Maddikunta, Fang Fang, Pubudu N. Pathirana,
A survey on blockchain for big data: Approaches, opportunities, and future directions,
Future Generation Computer Systems,
Volume 131,
2022,
Pages 209-226,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.01.017.
(https://www.sciencedirect.com/science/article/pii/S0167739X22000243)
Abstract: Big data has generated strong interest in various scientific and engineering domains over the last few years. Despite many advantages and applications, there are many challenges in big data to be tackled for better quality of service, e.g., big data analytics, big data management, and big data privacy and security. Blockchain with its decentralization and security nature has the great potential to improve big data services and applications. In this article, we provide a comprehensive survey on blockchain for big data, focusing on up-to-date approaches, opportunities, and future directions. First, we present a brief overview of blockchain and big data as well as the motivation behind their integration. Next, we survey various blockchain services for big data, including blockchain for secure big data acquisition, data storage, data analytics, and data privacy preservation. Then, we review the state-of-the-art studies on the use of blockchain for big data applications in different domains such as smart city, smart healthcare, smart transportation, and smart grid. For a better understanding, some representative blockchain-big data projects are also presented and analyzed. Finally, challenges and future directions are discussed to further drive research in this promising area.
Keywords: Blockchain; Big data; Vertical applications; Smart city; Smart healthcare; Smart transportation; Security

Ertem Esiner, Anwitaman Datta,
Two-factor authentication for trusted third party free dispersed storage,
Future Generation Computer Systems,
Volume 90,
2019,
Pages 291-306,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.08.001.
(https://www.sciencedirect.com/science/article/pii/S0167739X17322859)
Abstract: We propose a trusted third party free protocol for secure (in terms of content access, manipulation, and confidentiality) data storage and multi-user collaboration over an infrastructure of untrusted storage servers. It is achieved by the application of data dispersal, encryption as well as two-factor (knowledge and possession) based authentication and access control techniques so that unauthorized parties (attackers) or a small set of colluding servers cannot gain access to the stored data. The protocol design takes into account usability issues as opposed to the closest prior work Esiner and Datta (2016). We explore the security implications of the proposed model with event tree analysis and report on experiment results to demonstrate the practicality of the approach concerning computational overheads. Given that the protocol does not rely on any trusted third party, and most operations including actual collaboration do not require users to be online simultaneously, it is suitable not only for traditional multi-cloud setups but also for edge/fog computing environments.
Keywords: Layered security; Two-factor access control; Data out-sourcing; Edge computing; User controlled encryption; Erasure codes

Auday Al-Dulaimy, Wassim Itani, Javid Taheri, Maha Shamseddine,
bwSlicer: A bandwidth slicing framework for cloud data centers,
Future Generation Computer Systems,
Volume 112,
2020,
Pages 767-784,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.06.021.
(https://www.sciencedirect.com/science/article/pii/S0167739X19328055)
Abstract: Bandwidth allocation is an important and influential factor in enhancing the performance of the data centers’ nodes. In this paper we propose bwSlicer, a framework for bandwidth slicing in cloud data centers, that sheds light on the virtues of effective dynamic bandwidth allocation on improving the system performance and energy efficiency. Three algorithms are investigated to deal with this issue. In the first algorithm, called Fair Bandwidth Reallocation (FBR), two virtual machines co-hosted on the same node conditionally exchange bandwidth slices based on their requirements. The second algorithm, called Required Bandwidth Allocation (RBA), periodically monitors the co-hosted virtual machines and adds/removes bandwidth slices for each of them based on their bandwidth utilization. The third algorithm, called Divide Bandwidth Reallocation (DBR), divides the bandwidth of the virtual machine into slices once it finishes its execution, and distributes the slices among the co-hosted running virtual machines according to a specific policy. The proposed bandwidth slicing algorithms are emulated in a virtualized networking environment using the Mininet network emulator. The emulation results demonstrated a promising improvement ratio in execution time and energy consumption reaching up to 30%. These results present a call for action for further research into bandwidth slicing and reallocation as a viable complement to other energy-saving techniques for enhancing the energy consumption in cloud data centers.
Keywords: Bandwidth allocation; Bandwidth Slicing; Cloud Computing; CloudLet; Fog Computing; VM management

Nilson L. Damasceno, Marcos Lage, Antônio A. de A. Rocha,
Tinycubes: A modular technology for interactive visual analysis of historical and continuously updated spatiotemporal data,
Future Generation Computer Systems,
Volume 143,
2023,
Pages 378-391,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.01.007.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000146)
Abstract: Technological advances allow sending data collected, like IoT data and Social Network data, immediately to computational centers. However, state-of-the-art solutions for interactive visual exploration and analysis of spatiotemporal multidimensional using a single computer are focused on historical datasets. Also, most of these solutions utilize a monolithic architecture, with a restricted set of information extraction techniques and rigid internal data structures, which leads to the creation of forks to handle different usage needs. Tinycubes is a technology that supports the interactive exploration and visual analysis of spatiotemporal multidimensional continuously collected data in real-time combined with data from historical datasets, consistently producing up-to-date information using a single computer. The technology’s core is a novel and efficient data structure based on data cubes that allow data insertion and removal on-the-fly. The Tinycubes Technology is modular, allowing the usage of different information extraction algorithms and efficient data storage methods. Our experiments indicate that Tinycubes will enable the exploration of newly generated data combined with historical data while presenting performance similar or superior to the state-of-the-art solutions that use a single computer.
Keywords: Visual data analysis; Visual analytics; Exploratory visualization; Spatiotemporal; Datacube; Decision support system

Mostafa Haghi Kashani, Mona Madanipour, Mohammad Nikravan, Parvaneh Asghari, Ebrahim Mahdipour,
A systematic review of IoT in healthcare: Applications, techniques, and trends,
Journal of Network and Computer Applications,
Volume 192,
2021,
103164,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103164.
(https://www.sciencedirect.com/science/article/pii/S1084804521001764)
Abstract: Internet of Things (IoT) is an ever-expanding ecosystem that integrates software, hardware, physical objects, and computing devices to communicate, collect, and exchange data. The IoT provides a seamless platform to facilitate interactions between humans and a variety of physical and virtual things, including personalized healthcare domains. Lack of access to medical resources, growth of the elderly population with chronic diseases and their needs for remote monitoring, an increase in medical costs, and the desire for telemedicine in developing countries, make the IoT an interesting subject in healthcare systems. The IoT has a potential to decrease the strain on sanitary systems besides providing tailored health services to improve the quality of life. Therefore, this paper aims to identify, compare systematically, and classify existing investigations taxonomically in the Healthcare IoT (HIoT) systems by reviewing 146 articles between 2015 and 2020. Additionally, we present a comprehensive taxonomy in the HIoT, analyze the articles technically, and classify them into five categories, including sensor-based, resource-based, communication-based, application-based, and security-based approaches. Furthermore, the benefits and limitations of the selected methods, with a comprehensive comparison in terms of evaluation techniques, evaluation tools, and evaluation metrics, are included. Finally, based on the reviewed studies, power management, trust and privacy, fog computing, and resource management as leading open issues; tactile Internet, social networks, big data analytics, SDN/NFV, Internet of nano things, and blockchain as important future trends; and interoperability, real-testbed implementation, scalability, and mobility as challenges are worth more studying and researching in HIoT systems.
Keywords: Internet of things (IoT); Healthcare; e-health; Systematic review

Maiass Zaher, Aymen Hasan Alawadi, Sándor Molnár,
Sieve: A flow scheduling framework in SDN based data center networks,
Computer Communications,
Volume 171,
2021,
Pages 99-111,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.02.013.
(https://www.sciencedirect.com/science/article/pii/S0140366421000761)
Abstract: Today’s data centers act as the primary infrastructure for emerging technologies. QoS imposes requirements for more attentive techniques that can deal with different characteristics of traffic classes and patterns. In this context, network flows can be classified into large and long-lived flows called elephant flows and mice flows, which are small and short-lived flows. According to the characteristics of the emerging technologies, e.g., IoT and Big Data, mice flows are dominant; Hence, it is crucial to improve Flow Completion Time (FCT) for such delay-sensitive flows. This paper presents Sieve, a new distributed Software Defined Networks (SDN) based framework. Sieve initially schedules a portion of the flows based on the available bandwidth despite their classes. We propose a distributed sampling technique which sends a portion of the packets to the controller. Furthermore, Sieve polls the edge switches periodically to get the network information rather than polls all switches in the network, and it reschedules elephant flows only. Mininet emulator and mathematical analysis have been employed to validate the proposed solution in 4-ary Fat-Tree DCN. Sieve provides less FCT up to around 58% for mice flows and maintains throughput of elephant flows compared to Equal Cost MultiPath (ECMP) and Hedera.
Keywords: Data Center Network (DCN); Flow scheduling; Software Defined Networks (SDN); Mice flow; Elephant flow

Shubhankar Chaudhary, Pramod Kumar Mishra,
DDoS attacks in Industrial IoT: A survey,
Computer Networks,
Volume 236,
2023,
110015,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.110015.
(https://www.sciencedirect.com/science/article/pii/S1389128623004607)
Abstract: As the IoT expands its influence, its effect is becoming macroscopic and pervasive. One of the most discernible effects is in the industries where it is known as Industrial IoT (IIoT). IIoT provides automated, comprehensive, regressive and easy-to-use methods to look over its components. Along with the benefits, it also brings concerns that spawn from the IoT itself. Moreover, the challenges in the industries also add up because they have their own set of requirements and procedures to perform. Among those challenges, one of the prominent is DDoS attacks. So, through this paper, the DDoS attacks in IIoT is studied. This paper has culminated the work done in the domain involving IoT and IIoT. With this different forms of attacks involved in DDoS, the tools involved in generating the attacks and the overall traffic generators is also discussed. To elucidate, IIoT architecture and various layers involved in communication is discussed to correlate the threat of DDoS attacks in IIoT. Further, the studies made in various categories such as machine learning, deep learning, federated learning and transfer learning is elaborated. Finally, the challenges present in IIoT and the security requirements needed to overcome challenges in IIoT is explained.
Keywords: IoT; IIoT; SDN; DDoS; IIoT architecture; Security

Xiaoang Zheng, Aris Leivadeas, Matthias Falkner,
Intent Based Networking management with conflict detection and policy resolution in an enterprise network,
Computer Networks,
Volume 219,
2022,
109457,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109457.
(https://www.sciencedirect.com/science/article/pii/S1389128622004911)
Abstract: Intent-Based Networking (IBN) is a novel networking paradigm that allows networks to be autonomously configured, continuously assured, and to be highly adaptable to high-level intentions of network users and operators. In IBN systems, conflict detection and policy resolution modules are crucial to intent activation to enforce correct network configurations. To this end, in this study, we propose an extensible intent model, complemented with a conflict detection algorithm. We also propose a policy resolution algorithm that is based on a two-dimensional analysis of intent endpoints and time spans. To better showcase the efficiency of our algorithm, we also developed an enterprise-based IBN intent management web application for network users and administrators. Our evaluation experiments reveal the effectiveness of our algorithms in terms of reliable resolutions and fast response time.
Keywords: Intent-Based Networking; Conflict detection; Policy resolution; Intent management; Intent model

Chunlin Li, Mingyang Song, Qingchuan Zhang, Youlong Luo,
Cluster load based content distribution and speculative execution for geographically distributed cloud environment,
Computer Networks,
Volume 186,
2021,
107807,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107807.
(https://www.sciencedirect.com/science/article/pii/S1389128621000025)
Abstract: The scale of big data has shown an explosive growth, which makes the processing of big data put forward higher requirements on data centers, and a single data center can no longer meet the needs of big data processing. To deal with this situation, a geographically distributed cloud system needs to be built. However, in the geographically distributed cloud system, each data center is distributed in different geographic locations, which makes the data placement operations in the geographically distributed cloud system lead to greater overhead. To solve this problem, this paper proposes a data placement strategy. This strategy comprehensively considers the data transmission latency, bandwidth cost, cloud server storage capacity, and load capacity during the data placement process, and formulates a data placement problem that minimizes the energy consumption of data transmission. Then the minimum set cover method based on Lagrangian relaxation is used to solve this problem and obtain the optimal data placement scheme. On the other hand, in a geographically distributed cloud data center, the execution progress of the job submitted by the user will be affected by the straggler task. To solve this problem, this paper proposes a speculative execution strategy for the geographically distributed cloud system. This strategy performs different speculative execution operations according to the state of the cluster load, and then calculates the load capacity of the nodes in the cluster. The node with the strongest load capacity in the cluster is used to perform speculative execution operations. Experimental results show that the proposed data placement strategy can effectively improve the performance of the energy consumption, the data storage cost, the network transmission cost and the data transmission time. The proposed speculative execution strategy can effectively improve the performance of the job completion time, cluster throughput and QoS satisfaction rate.
Keywords: Geographically distributed cloud; Data placement; Speculative execution; Lagrange relaxation

Fahimeh Yazdanpanah, Raheel Afsharmazayejani,
A systematic analysis of power saving techniques for wireless network-on-chip architectures,
Journal of Systems Architecture,
Volume 126,
2022,
102485,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2022.102485.
(https://www.sciencedirect.com/science/article/pii/S1383762122000649)
Abstract: Wireless network-on-chip (WNoC, a.k.a. WiNoC) architectures, as an emerging and viable alternative approach, overcome the communication constraints and drawbacks of network-on-chip (NoC) architectures. High scalability, high bandwidth, and low latency are the advantages of WNoCs that considerably alleviate the shortage of wired on-chip networks. Despite the great merits of WNoC, on the other hand, power-hungry wireless components cause problems regarding power consumption and temperature issues. Therefore, a significant challenge and an integral part of designing an efficient WNoC architecture would be resolving power management difficulties. Hence, the researchers have investigated various approaches to reduce and manage energy problems of WNoC structures while keeping up a trade-off between power efficiency and improving other performance parameters. This article presents a comprehensive systematic analysis on energy saving techniques in WNoCs. The goal is to classify state-of-the-art WNoCs architectures employing different approaches for reducing power consumption such as power gating, dynamic frequency/voltage scaling, enhancing transceiver structure, modifying topology and routing strategies. Some researchers investigate the impact of a wave-guiding medium type or wireless devices (e.g., transceivers) on power consumption. Besides, the voltage and frequency scaling, structure, routing and communication strategies are principal targets of energy efficiency.
Keywords: Wireless NoC; Power saving; Energy efficiency; Power gating; DVFS

Yongxuan Lai, Lu Zhang, Fan Yang, Lv Zheng, Tian Wang, Kuan-Ching Li,
CASQ: Adaptive and cloud-assisted query processing in vehicular sensor networks,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 237-249,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.11.034.
(https://www.sciencedirect.com/science/article/pii/S0167739X18302164)
Abstract: Vehicles in urban cities are equipped with increasing more sensing units. Large amount of data are continuously generated and they bring great potentials to the intelligent and green city traffic management. However, data gathering and query processing remain key and challenging issues due to the huge amount of sensing data, changeable road conditions, rapid network topology and density changes caused by the movement of vehicles. There is great necessity for the cloud and the vehicular sensor networks to integrate and enhance each other on the cooperative urban sensing applications. In this paper we propose an adaptive and cloud-assisted query processing scheme for VANETs, that adopts the concept of edge nodes and integrates the cloud and vehicular networks to facilitate data storage and indexing, so queries could be processed and forwarded along different communication channels according to the cost and time bounds of the queries. Moreover, the cloud calculates result forwarding strategy by solving a Linear Programming problem, where the query results select the best path either through the 4G channel or through the DSRC (Dedicated Short Range Communication). This research is one of the first steps towards the integration of the cloud and the vehicular networks, as well as edge nodes and the 4G channel, to improve the effectiveness and efficiency of the query processing in VANETs. Extensive experiments demonstrate that up to 94% of the queries could be successfully processed in the proposed scheme, much higher than existing query schemes, while at the same time with a relatively low querying cost.
Keywords: Cloud-assisted; Query result forwarding; Data storage; Query processing; VANETs

Saleh Mohamed AlHidaifi, Muhammad Rizwan Asghar, Imran Shafique Ansari,
Towards a Cyber Resilience Quantification Framework (CRQF) for IT infrastructure,
Computer Networks,
Volume 247,
2024,
110446,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110446.
(https://www.sciencedirect.com/science/article/pii/S1389128624002780)
Abstract: Cyber resilience quantification is the process of evaluating and measuring an organisation’s ability to withstand, adapt to, and recover from cyber-attacks. It involves estimating IT systems, networks, and response strategies to ensure robust defence and effective recovery mechanisms in the event of a cyber-attack. Quantifying cyber resilience can be difficult due to the constantly changing components of IT infrastructure. Traditional methods like vulnerability assessments and penetration testing may not be effective. Measuring cyber resilience is essential to evaluate and strengthen an organisation’s preparedness against evolving cyber-attacks. It helps identify weaknesses, allocate resources, and ensure the uninterrupted operation of critical systems and information. There are various methods for measuring cyber resilience, such as evaluating, teaming and testing, and creating simulated models. This article proposes a cyber resilience quantification framework for IT infrastructure that utilises a simulation approach. This approach enables organisations to simulate different attack scenarios, identify vulnerabilities, and improve their cyber resilience. The comparative analysis of cyber resilience factors highlights pre-configuration’s robust planning and adaptation (61.44%), buffering supported’s initial readiness (44.53%), and network topologies’ robust planning but weak recovery and adaptation (60.04% to 77.86%), underscoring the need for comprehensive enhancements across all phases. The utilisation of the proposed factors is crucial in conducting a comprehensive evaluation of IT infrastructure in the event of a cyber-attack.
Keywords: Cyber resilience; Cybersecurity; Cyber-attacks; Framework; Quantification; OMNeT＋＋ simulator

Andrés García Mangas, Francisco José Suárez Alonso,
WOTPY: A framework for web of things applications,
Computer Communications,
Volume 147,
2019,
Pages 235-251,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.09.004.
(https://www.sciencedirect.com/science/article/pii/S0140366419304633)
Abstract: The interoperability problems that originate from the heterogeneity in protocols and platforms is one of the main challenges currently faced by the Internet of Things (IoT). The Web of Things (WoT) is an architectural solution to this issue based on leveraging the Web as a means to ensure interoperability. The World Wide Web Consortium (W3C) is currently behind one of the most relevant WoT initiatives—a group of building blocks to serve as a possible foundation for the WoT. This work describes an experimental framework based on the W3C WoT, including a set of concrete and original protocol binding implementations (HTTP, Websockets, MQTT and CoAP). One of the main novelties is that all protocol binding implementations have support for all interaction verbs from the WoT interaction model. The framework is especially adequate to build WoT applications for devices on all layers of the fog computing model; this multi-layer integration is achieved by leveraging the W3C WoT architecture and interaction model. A functional implementation in Python is also described, including low-level designs and implementation details for the binding templates. The behavior of the framework and the protocol bindings is studied by implementing a benchmark application under multiple conditions and hardware platforms. Finally, recommendations are extracted from the obtained results for the most adequate protocols for each scenario and interaction verb.
Keywords: Web of Things; Internet of Things; Fog computing

Vinicius Fulber-Garcia, Elias P. Duarte, Alexandre Huff, Carlos R.P. dos Santos,
Network service topology: Formalization, taxonomy and the CUSTOM specification model,
Computer Networks,
Volume 178,
2020,
107337,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107337.
(https://www.sciencedirect.com/science/article/pii/S1389128619312460)
Abstract: Network Function Virtualization (NFV) relies on virtualization technologies to allow the implementation of middleboxes in software that is executed on commercial off-the-shelf hardware. Multiple Virtual Network Functions (VNFs) can be combined to form arbitrary network services. The term service topology has been freely employed by both major NFV recommendation and standardization bodies (the Internet Engineering Task Force (IETF) and European Telecommunications Standards Institute (ETSI)) and in the literature. The objective of this work is to present a formal specification that unifies these different views of service topologies. A taxonomy is proposed which allows the classification of topologies according to multiple criteria, including structure, size, heterogeneity, function sharing, among others. We also propose the CUstom Service TOpology Model (CUSTOM), a specification model that allows the design of network service topologies that feature the different categories proposed in the taxonomy. Finally, we demonstrate the specification capabilities of CUSTOM through a series of case studies.
Keywords: NFV; Service; Topology; Definitions; Taxonomy; Specification; Model

Khalid A. Darabkh, Muna Al-Akhras, Jumana N. Zomot, Mohammed Atiquzzaman,
RPL routing protocol over IoT: A comprehensive survey, recent advances, insights, bibliometric analysis, recommendations, and future directions,
Journal of Network and Computer Applications,
Volume 207,
2022,
103476,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103476.
(https://www.sciencedirect.com/science/article/pii/S1084804522001242)
Abstract: In the course of time, the Internet of Things (IoT) has attracted significant research interest. However, IoT devices have limited resources available in terms of battery power, processing capacity, memory, bandwidth, etc. In an attempt to provide connectivity and Internet Protocol version 6 (IPv6) support to IoT devices, the IPv6 routing protocol for Low-Power and Lossy Networks (RPL) was officially launched as the standard routing protocol for IoT in 2012. Despite being reputed and used in diverse applications, several recent studies have revealed RPL's drawbacks and limitations. The main objective of this work is to help the IoT research community understand all aspects of RPL. The paper also provides a detailed description of the operation of the RPL protocol. What is more, this work includes novel and thorough examples, thereby gaining practical knowledge of the pros and cons of this protocol. In addition, this paper reviews and summarizes relevant RPL-based protocols and conducts comprehensive comparisons among them from the perspectives of reliability, robustness, energy efficiency, and flexibility. Technically speaking, after studying and reviewing the majority of the proposed RPL solutions, we are ultimately capable, in this work, of highlighting all the challenges faced by IoT researchers while enhancing RPL and providing what is expected to be dealt with professionally. The present work also gives more details about RPL simulation platforms and RPL applications. Not only to this extent, but rather the historical bibliometric analysis of RPL, which shows the trends in the area of research to be focused on, has been professionally analyzed based on RPL challenges over the years 2010 through 2021. To this end, the conclusions and recommendations of this study are presented along with the effective directions for future RPL, and their applicability. As a result, the authors believe that this work will be a valuable reference for all RPL researchers and designers.
Keywords: Internet of things (IoT); Routing protocols; RPL; RPL challenges; enhancements; and recommendations; RPL future directions

Nikolay O. Nikitin, Pavel Vychuzhanin, Mikhail Sarafanov, Iana S. Polonskaia, Ilia Revin, Irina V. Barabanova, Gleb Maximov, Anna V. Kalyuzhnaya, Alexander Boukhanovsky,
Automated evolutionary approach for the design of composite machine learning pipelines,
Future Generation Computer Systems,
Volume 127,
2022,
Pages 109-125,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.08.022.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003307)
Abstract: The effectiveness of the machine learning methods for real-world tasks depends on the proper structure of the modeling pipeline. The proposed approach is aimed to automate the design of composite machine learning pipelines, which is equivalent to computation workflows that consist of models and data operations. The approach combines key ideas of both automated machine learning and workflow management systems. It designs the pipelines with a customizable graph-based structure, analyzes the obtained results, and reproduces them. The evolutionary approach is used for the flexible identification of pipeline structure. The additional algorithms for sensitivity analysis, atomization, and hyperparameter tuning are implemented to improve the effectiveness of the approach. Also, the software implementation on this approach is presented as an open-source framework. The set of experiments is conducted for the different datasets and tasks (classification, regression, time series forecasting). The obtained results confirm the correctness and effectiveness of the proposed approach in the comparison with the state-of-the-art competitors and baseline solutions.
Keywords: AutoML; Workflow; Composite pipeline; Machine learning; Evolutionary algorithms; WMS

José Cabrero-Holgueras, Sergio Pastrana,
A Methodology For Large-Scale Identification of Related Accounts in Underground Forums,
Computers & Security,
Volume 111,
2021,
102489,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2021.102489.
(https://www.sciencedirect.com/science/article/pii/S0167404821003138)
Abstract: Underground forums allow users to interact with communities focused on illicit activities. They serve as an entry point for actors interested in deviant and criminal topics. Due to the pseudo-anonymity provided, they have become improvised marketplaces for trading illegal products and services, including those used to conduct cyberattacks. Thus, these forums are an important data source for threat intelligence analysts and law enforcement. The use of multiple accounts is forbidden in most forums since these are mostly used for malicious purposes. Still, this is a common practice. Being able to identify an actor or gang behind multiple accounts allows for proper attribution in online investigations, and also to design intervention mechanisms for illegal activities. Existing solutions for multi-account detection either require ground truth data to conduct supervised classification or use manual approaches. In this work, we propose a methodology for the large-scale identification of related accounts in underground forums. These accounts are similar according to the distinctive content posted, and thus are likely to belong to the same actor or group. The methodology applies to various domains and leverages distinctive artefacts and personal information left online by the users. We provide experimental results on a large dataset comprising more than 1.1M user accounts from 15 different forums. We show how this methodology, combined with existing approaches commonly used in social media forensics, can assist with and improve online investigations.
Keywords: Social media forensics; Underground forums; Large-Scale measurement; Related accounts; Cybercrime

Zainab H. Ali, Noha A. Sakr, Nora El-Rashidy, Hesham A. Ali,
A reliable position-based routing scheme for controlling excessive data dissemination in vehicular ad-hoc networks,
Computer Networks,
Volume 229,
2023,
109785,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109785.
(https://www.sciencedirect.com/science/article/pii/S138912862300230X)
Abstract: In the past two decades, the automotive industry has undergone tremendous changes, as this field has become one of the fastest developing and growing fields, especially with the progress of global digitization and massive research related to networks. So, it comes as no surprise that self-driving vehicles are ubiquitous mainly relying on Vehicular ad-hoc networks (VANETs). This transformation ensures improved road navigation and traffic congestion avoidance by relying on Rapid Data Deployment (DD). On the other hand, although DD achieves high connection reliability, it may affect network bandwidth and performance. In addition, excessive DD causes frequent link outages resulting in reduced data delivery, massive packet loss, and premature end of network life. This paper presents an integrated architecture proposal for a vehicle dynamic assistance architecture, based on reliable methods to ensure steering accuracy while minimizing the energy expended for controlling the DD rate via VANETs. The proposed architecture integrates between Software-Defined Networks (SDN) and fog computing based on dealing with the mobility factors that exploit vehicle communication behaviors. Such integration will aid in improving network performance in terms of packet delivery and DD. The study also discusses how to take into consideration the Euclidean distance, geographical routing information, residual power ratio, and latency time to maximize network stability and avoid possible link disruption. The simulation results prove that there is a 62% to 70% enhancement of the whole power consumption and network throughput, depending on the implementation of the proposed position-based routing approach. Interestingly, the proposed routing protocol is a dual-phase routing protocol with a 90% of SDN data packet delivery ratio and an 82% of SDN data loss reduction. So, when the SDN fails to deliver packets, the proposed position-based routing handles them as a parallel mechanism of SDN.
Keywords: Computing platform; Position-based routing; Energy conservation; SDN; Data Dissemination; VANETs

Mehmet Fatih Tuysuz, Ramona Trestian,
From serendipity to sustainable green IoT: Technical, industrial and political perspective,
Computer Networks,
Volume 182,
2020,
107469,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107469.
(https://www.sciencedirect.com/science/article/pii/S1389128620311452)
Abstract: Recently, Internet of Things (IoT) has become one of the largest electronics market for hardware production due to its fast evolving application space. However, one of the key challenges for IoT hardware is the energy efficiency as most of IoT devices/objects are expected to run on batteries for months/years without a battery replacement or on harvested energy sources. Widespread use of IoT has also led to a large-scale rise in the carbon footprint. In this regard, academia, industry and policy-makers are constantly working towards new energy-efficient hardware and software solutions paving the way for an emerging area referred to as green-IoT. With the direct integration and the evolution of smart communication between physical world and computer-based systems, IoT devices are also expected to reduce the total amount of energy consumption for the Information and Communication Technologies (ICT) sector. However, in order to increase its chance of success and to help at reducing the overall energy consumption and carbon emissions a comprehensive investigation into how to achieve green-IoT is required. In this context, this paper surveys the green perspective of the IoT paradigm and aims to contribute at establishing a global approach for green-IoT environments. A comprehensive approach is presented that focuses not only on the specific solutions but also on the interaction among them, and highlights the precautions/decisions the policy makers need to take. On one side, the ongoing European projects and standardization efforts as well as industry and academia based solutions are presented and on the other side, the challenges, open issues, lessons learned and the role of policymakers towards green-IoT are discussed. The survey shows that due to many existing open issues (e.g., technical considerations, lack of standardization, security and privacy, governance and legislation, etc.) that still need to be addressed, a realistic implementation of a sustainable green-IoT environment that could be universally accepted and deployed, is still missing.
Keywords: Energy efficiency; Internet of Things; IoT; Green Networking

Yuwen Zhou, Bangbang Ren, Junjie Xie, Lailong Luo, Deke Guo, Xiaobo Zhou,
Enable the proactively load-balanced control plane for SDN via intelligent switch-to-controller selection strategy,
Computer Networks,
Volume 233,
2023,
109867,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109867.
(https://www.sciencedirect.com/science/article/pii/S1389128623003122)
Abstract: The novelty of the software-defined network(SDN) is to separate the control plane and the data plane for easier manipulation of the network. The distributed control plane is designed to achieve more powerful computation capacity and address the single-point failure problem. However, it also poses a new challenge that how to arrange switch-controller associations effectively. The direct static configuration cannot adapt the time-varying requests from switches well, and it would result in imbalance problems on the control plane and cause long-tail latency. Thus, it is necessary to take proper actions to adjust the switch-to-controller association dynamically. Existing controller-based load balancing methods need to communicate with the switches frequently and incur not only high assumptions of the rare control channel of SDN but also high computation costs. In this paper, we provide a switch-based solution that puts Reinforcement Learning agents on all Switches (RLoS). Instead of setting static rules predefined by operators, RLoS makes each switch actively select the best controller. RLoS treats every switch as an independent agent with its own neural network and parameters. With the carefully designed training algorithm, the agents could choose their preferable controllers via their local information. The results show that even with partial observation, the RLoS still can achieve considerable improvement in the load balance among all controllers compared with those controller-based association benchmarks. Our RLoS could decrease the maximum response latency among controllers by about 5%∼15% under different scenarios on average.
Keywords: Distributed controllers; Load balance; Software defined network; Multi-agent reinforcement learning

Sang-Hoon Choi, Ki-Woong Park,
Cloud-BlackBox: Toward practical recording and tracking of VM swarms for multifaceted cloud inspection,
Future Generation Computer Systems,
Volume 137,
2022,
Pages 219-233,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2022.07.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X22002321)
Abstract: Given the widening scope of the utilization and application of cloud computing services from general to mission-critical systems such as strategic military, financial, and the information systems of governmental agencies, the need for the development of improved methods to ensure the stability and security of cloud data and services is being increasingly emphasized. Various approaches have been developed to improve the security and stability of cloud infrastructure. In particular, the continuous inspection of the memory of Virtual Machine (VM) instances in the cloud platform has been an important factor in identifying the causes of security incidents related to zero-day vulnerabilities and critical system faults in cloud infrastructure. However, despite numerous studies in the field of continuous memory inspection, it is difficult to find a practical solution that is deployable in commercial-off-the-shelf cloud platforms. For instance, continuous memory snapshots generally cause various problems such as increased VM downtime occurrences, user-obstructive latency for memory snapshots, VM performance degradation, and massive data generation. To alleviate these limitations, we propose Cloud-BlackBox, which enables the recording of the memory of VM swarms running on cloud platforms that require a very high level of stability and security, and facilitates the flexible analysis of the recorded memory on a large-scale. A VM swarm refers to an environment in which multiple VMs are run in parallel. The proposed Cloud-BlackBox method provides the following benefits. First, by clustering VM swarm kernel memory, the amount of computation required to capture memory snapshots and the size of the generated snapshot images are minimized. Further, we propose a mechanism to merge kernel memory by rapidly identifying the homogeneity of the memory layout through analysis of the underlying base image and introspection of the running VM. The application of the proposed mechanism led to a storage reduction by a factor of 12.85. Second, a cognitive-scale bitmap was designed to track changes in the memory of VM swarms. The cognitive-scale bitmap is a mechanism that can dynamically manage the tracking of memory change information by recognizing the memory usage patterns of component VMs. With the designed cognitive-scale bitmap, the time required for the collection of a memory snapshot was reduced by more than 14.85 times, and the VM input/output (I/O) performance degradation was reduced by 50%. Third, a synchronized accessible memory interchange (SAMI) mechanism is proposed to facilitate the agile in-depth analysis of large-scale memory resources. Cloud-BlackBox tracks and records memory change information. Therefore, a procedure for restoring the recorded memory to a raw-memory analyzable form is required to analyze the recorded memory. The SAMI mechanism assists the analyst in ensuring consistent memory restoration performance when arbitrarily selecting recorded memory. Furthermore, SAMI is useful for reducing the scope of analysis without memory restoration simply by analyzing recorded metadata. Consequently, the revised schemes inside Cloud-BlackBox have several applications in various fields, such as advanced detection of malicious activities, service error recovery, malware analysis, and antivirus functions. In addition, the proposed approach has been implemented on a campus-wide cloud computing service called SysCore-Cloud.
Keywords: Cloud computing; Hypervisor; Virtual machine; VM swarms; System inspection; Memory snapshot

Ting Xu, Ming Zhao, Xin Yao, Yusen Zhu,
An improved communication resource allocation strategy for wireless networks based on deep reinforcement learning,
Computer Communications,
Volume 188,
2022,
Pages 90-98,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.02.018.
(https://www.sciencedirect.com/science/article/pii/S0140366422000615)
Abstract: With the advent of 5G networks, user demand for high-speed, low-latency, and high-reliability services continues to grow. When traditional communication technologies cannot meet the needs, wireless network LoRa technology has emerged. Although LoRa has low power consumption, Long-distance, and other advantages, terminal nodes still face frequent data collection and energy consumption issues how to more efficiently combine the deep reinforcement learning method for LoRa wireless network communication and allocate resources reasonably and effectively. This paper proposes a communication channel resource allocation strategy based on deep reinforcement learning, the CL-LoRa strategy. It uses extended preamble and low-power interception technologies to achieve on-demand synchronization and low-power communication. The basic idea of this strategy is to detect channel quality based on CAD, coordinate node scheduling, and wireless channel allocation. The node will choose different ways to acquire the channel according to the current network load, namely CSMA-CA competition and dynamic duty cycle communication. In this way, the channel utilization rate is improved, and the energy consumption problem of the long-distance communication data volume is perfectly solved. The duty cycle access method is based on the imbalance of energy consumption in the Internet of Things. It uses the remaining energy of the remote central node to dynamically adjust the duty cycle of the node, wake up the working time of the node, and send more beacons to the sleeping node. Reduce the sleep delay of the node. Through theoretical analysis of CL-LoRa protocol performance, compared with DDC-LoRa protocol and ADC-LoRa protocol, CL-LoRa protocol can increase channel utilization by 9%, reduce terminal energy consumption by 1.6%, and increase throughput by 1.5%.
Keywords: Teacon; Lora; Allocating resources; Deep reinforcement learning; Low-power consumption; 5G network

Ashish Kumar, Rahul Saha, Mauro Conti, Gulshan Kumar, William J. Buchanan, Tai Hoon Kim,
A comprehensive survey of authentication methods in Internet-of-Things and its conjunctions,
Journal of Network and Computer Applications,
Volume 204,
2022,
103414,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103414.
(https://www.sciencedirect.com/science/article/pii/S1084804522000716)
Abstract: Internet of Thing (IoT) is one of the most influential technologies in the present time. People, processes, and things are connected with the Internet through IoT. With the increasing demands of user applications, the number of connections is also increasing exponentially. Therefore, security becomes a critical issue in IoTs. Confidentiality, Integrity, and Availability (CIA) services are important for IoT applications. IoTs must also ensure proper authentication mechanisms to ensure CIA in the second stage. Various researches in this direction address the authentication issues in IoTs. In this paper, we survey the authentication aspects in IoTs and their allied domains. We analyze the potentialities of the existing state-of-the art-approaches and also identify their limitations. We discuss the basics of authentication and its related attacks for the ease of interpretability of the readers. We show a taxonomical understanding of the approaches and try to connect the evolution of the solution strategies. These connections, to the best of our knowledge, are novel as compared to the existing authentication surveys. Besides, the multidimensional vision of this survey for IoT extensions is an add-on to the benefits. We also provide a discussion on the future direction of research in this domain. In a nutshell, this survey is a one-stop solution for academia and industry to understand the status quo of IoT authentication schemes/protocols.
Keywords: Internet-of-Things; Authentication; Security; Cryptography; Research problems

Iacovos Ioannou, Christophoros Christophorou, Vasos Vassiliou, Andreas Pitsillides,
A novel Distributed AI framework with ML for D2D communication in 5G/6G networks,
Computer Networks,
Volume 211,
2022,
108987,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108987.
(https://www.sciencedirect.com/science/article/pii/S138912862200158X)
Abstract: Inspired by the adoption of Artificial Intelligence (AI) and Machine Learning (ML) approaches in 5G and 6G networks, in this paper we propose a novel ML based Distributed AI (DAI) framework able to attain the ambitious goals set for emerging 5G/6G networks. The novelty of the DAI framework is that it is implemented in an autonomous, dynamic and flexible fashion, utilising Belief Desire Intention (BDI) agents, extended with ML capabilities, which reside on the mobile devices (User Equipment). We refer to these as BDIx agents. This provides a component-based framework (likened to LEGO-based building blocks), which can build on and utilise execution plans, by composing and arranging ML techniques in flexible ways within the framework, in order to achieve the desired goals. More specifically, we form a modular BDIx agent at a multi-agent system (MAS), integrated with Fuzzy Logic for the perception/cognitive part of the agents. By exploiting the capabilities of the BDIx agents in our DAI framework, we allow mobile devices to intercommunicate and cooperate in an autonomous manner, thus offering a number of attractive features, including improved performance in terms of network control execution time and message exchange, fast response in handling dynamic aspects in the network, self-organising network functionalities, and a framework that can act as the glue platform in employing one or more intelligent approaches to tackle the diverse 5G/6G technical requirements. To demonstrate the potential of the DAI framework we focus on Device to Device (D2D) communication and illustrate its flexibility in addressing diverse D2D challenges. Through example Plan Libraries and enhanced metrics, we outline DAI implementation specifics to achieve a number of identified 5G/6G D2D requirements. To embed the concept further, the specific problem of D2D transmission mode selection is expanded upon, from problem description to solution approach (DAIS) and implementation specifics, and hence comparatively evaluate over other approaches (i.e., Unsupervised learning ML techniques, centralised control techniques, random techniques). The results demonstrated that DAIS provides, among other performance metrics, improved mobile network Spectral Efficiency (SE) and Power Consumption (PC), better and more efficient cluster formation and reduced control decision delay.
Keywords: 5G; 6G; D2D; Intelligence edge; Distributed Artificial Intelligence; Machine Learning; Distributed Machine Learning; Unsupervised learning; BDIx agent

Mebanjop Kharjana, Fabiola Hazel Pohrmen, Subhas Chandra Sahana, Goutam Saha,
Blockchain-based key management system in Named Data Networking: A survey,
Journal of Network and Computer Applications,
Volume 220,
2023,
103732,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103732.
(https://www.sciencedirect.com/science/article/pii/S1084804523001510)
Abstract: The Named Data Networking is a popular future internet architecture designed around the content name. This data-centric network architecture sought to improve routing techniques, traffic management, network performance, and more. It also adopted a data-centric security paradigm to ensure content authenticity, confidentiality, integrity, and privacy. Several challenges need to be addressed to achieve such a comprehensive security goal. Innovative blockchain-based security solutions were widely used to address such challenges. Blockchain could offer a secure platform for advanced security features like decentralized identity, distributed trust management, and many more. The current trend is to use blockchain to secure key management operations in Named Data Networking. This survey aimed to identify key management challenges in Named Data Networking and how they were mitigated using blockchain. For this purpose, the survey comprehensively reviewed the existing blockchain-based key management systems for Named Data Networking. It summarized how these systems solve different key management problems using blockchain. It also performed a comparative analysis of these systems based on three references. Firstly, the systems were analyzed to investigate their conformance to the National Institute of Standards and Technology recommendations for secure key management. Secondly, the systems were examined concerning the Named Data Networking platform to reveal information about their key namespace and key object structure. Thirdly, the systems were analyzed based on the blockchain platform and compared regarding their functionality, performance, and security. The content stored in the blockchain was also examined to uncover the relationship between the key, key name, and blockchain. The survey concluded by identifying research gaps and opportunities for further research. This paper contributed to gaining knowledge about key management security challenges faced by Named Data Networking. It revealed how blockchain technology addressed these key management security challenges. It also identified blockchain features that can be adopted to secure key management operations further. The survey results provided a template to guide future research to secure key management in NDN with blockchain.
Keywords: Named Data Networking; NDN security; Blockchain; Key management system; NIST key management

Francesco Marino, Corrado Moiso, Matteo Petracca,
Automatic contract negotiation, service discovery and mutual authentication solutions: A survey on the enabling technologies of the forthcoming IoT ecosystems,
Computer Networks,
Volume 148,
2019,
Pages 176-195,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.11.011.
(https://www.sciencedirect.com/science/article/pii/S1389128618312167)
Abstract: The Internet-of-Things (IoT) paradigm envisions the full integration of several technologies in order to enable a new class of applications relying on an unprecedented sensing and actuating infrastructure. However, traditional IoT applications are static, closed, vertically integrated solutions which do not allow to fully exploit the potentialities of this new technological platform. For this reason IoT systems are now evolving towards more open and dynamic architectures and solutions in which dynamicity is considered an added value to build context-aware and self-adapting applications. In this new continuous changing scenario, solutions for automatic contract negotiation and management, service discovery and mutual authentication will play a central role, since they can be considered the main building blocks to create secure and dynamic applications. The paper surveys research activities in the these fields, pointing out the challenges and the open research issues that arise in such a new emerging scenario.
Keywords: Dynamic IoT; Distributed systems; Heterogeneous systems; Contract negotiation; Service discovery; Mutual authentication

Wenjia Wu, Yujing Liu, Jiazhi Yao, Xiaolin Fang, Feng Shan, Ming Yang, Zhen Ling, Junzhou Luo,
Learning-aided client association control for high-density WLANs,
Computer Networks,
Volume 212,
2022,
109043,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109043.
(https://www.sciencedirect.com/science/article/pii/S138912862200192X)
Abstract: As wireless local area network (WLAN) continues to become popular, there is an increasing number of clients with huge data traffic demands. Especially, some high client-density environments are emerging, such as industrial plants, stadiums, and event centers, which poses significant challenges in terms of client association control. Under such environments, conventional client-side solutions that select access points (APs) according to simple indicators such as signal strength may result in poor network performance, and although some centralized association control mechanisms are proposed, it is still difficult that a large amount of complex global network status information needs to be effectively and efficiently utilized. To meet these challenges, we investigate the online centralized association control problem that aims to improve user quality of experience (QoE), and propose a deep reinforcement learning (DRL) aided solution, called Wi-OAC, where an image-like state pattern is designed to achieve state reformulation for deep Q-network (DQN), and the double DQN and dueling DQN strategies are combined to improve convergence speed. On the basis of offline training, Wi-OAC can determine the proper AP-client associations for the arriving clients. Both simulation experiments and real-world experiments have been conducted to validate the effectiveness of Wi-OAC. In real-world experiments, we build a Wi-OAC testbed including 3 APs and 54 clients in less than 10.5 m2 area, and the results show that Wi-OAC can significantly improve the performance on the client throughput, AP load balancing and user QoE.
Keywords: WLAN; Association control; High client density; Deep reinforcement learning

Muchtar Farkhana, Abdullah Abdul Hanan, Hassan Suhaidi, Khader Ahamad Tajudin, Zamli Kamal Zuhairi,
Energy conservation of content routing through wireless broadcast control in NDN based MANET: A review,
Journal of Network and Computer Applications,
Volume 131,
2019,
Pages 109-132,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.01.004.
(https://www.sciencedirect.com/science/article/pii/S1084804519300049)
Abstract: Research in Named Data Networking-based Mobile Ad hoc Network (NDN based MANET) experienced a lot of momentum and development in recent years. Such robust developments in the specific area surely contributes to advanced possibilities that Named Data Networking (NDN) can provide compared to traditional host centric networking solutions such as TCP/IP for dynamic routing that is much needed for MANET environment. Based on our observations, most existing work of NDN based MANET studies chose to use full wireless broadcast approach method for dynamic content routing as their solution in MANET environment. This review is carried out by analyzing how energy conservation of dynamic content routing was conducted in previous studies that has employed various methods of wireless broadcast smart control that are totally different from one another. We then discuss the disadvantages of the suggested solution from previous studies and scrutinizes it from the aspects of energy conservation of content routing in NDN based MANET point of view. Following that, we proceed to suggest on how dynamic content routing should have been done in order to achieve beneficial energy efficiency improvements of content routing mechanism in NDN based MANET. Following that, we suggest how dynamic content routing mechanisms for energy conservation can be improved in NDN based MANET. At the same time, we show the differences between our suggested solution and existing solution as a proposal towards the creation of next generation of content routing solution for NDN based MANET implementation.
Keywords: MANET; NDN; ICN; Content routing; NDN based MANET; Energy conservation; Energy efficiency; Broadcast control

Jinwoo Kim, Minjae Seo, Seungsoo Lee, Jaehyun Nam, Vinod Yegneswaran, Phillip Porras, Guofei Gu, Seungwon Shin,
Enhancing security in SDN: Systematizing attacks and defenses from a penetration perspective,
Computer Networks,
Volume 241,
2024,
110203,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2024.110203.
(https://www.sciencedirect.com/science/article/pii/S1389128624000355)
Abstract: Over the past 15 years, Software-Defined Networking (SDN) has garnered widespread support in research and industry due to its open and programmable nature. This paradigm enables various stakeholders, such as researchers, practitioners, and developers, to innovate networking services using robust APIs and a global network view, eliminating dependence on vendor-specific control planes. However, the adaptable architecture of SDN has introduced numerous security challenges not present in traditional network environments. While several surveys have highlighted existing attacks, there is a notable absence of a systematic penetration perspective, essential for understanding the attacks and their origins. This paper seeks to analyze prior literature that has exposed instances of attacks in SDN, examining their vulnerabilities, penetration routes, and root causes. Furthermore, we offer a thorough and comprehensive discussion of the underlying issues associated with these attacks, presenting defenses proposed by researchers to mitigate them and analyzing how the root causes are addressed. We also explore how our survey can assist practitioners in preparing suitable defenses by providing insights into penetration routes. Through this study, our goal is to shed light on existing security issues within the current SDN architecture, prompting a reassessment of various security problems and offering a guideline for future research in SDN security.
Keywords: Software-Defined Networking (SDN); SDN Security; Survey; Systematization of Knowledge (SoK)

Khaled Hejja, Xavier Hesselbach,
Offline and online power aware resource allocation algorithms with migration and delay constraints,
Computer Networks,
Volume 158,
2019,
Pages 17-34,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.04.030.
(https://www.sciencedirect.com/science/article/pii/S1389128618314063)
Abstract: In order to handle advanced mobile broadband services and Internet of Things (IoT), future Internet and 5G networks are expected to leverage the use of network virtualization, be much faster, have greater capacities, provide lower latencies, and significantly be power efficient than current mobile technologies. Therefore, this paper proposes three power aware algorithms for offline, online, and migration applications, solving the resource allocation problem within the frameworks of network function virtualization (NFV) environments in fractions of a second. The proposed algorithms target minimizing the total costs and power consumptions in the physical network through sufficiently allocating the least physical resources to host the demands of the virtual network services, and put into saving mode all other not utilized physical components. Simulations and evaluations of the offline algorithm compared to the state-of-art resulted on lower total costs by 32%. In addition to that, the online algorithm was tested through four different experiments, and the results argued that the overall power consumption of the physical network was highly dependent on the demands’ lifetimes, and the strictness of the required end-to-end delay. Regarding migrations during online, the results concluded that the proposed algorithms would be most effective when applied for maintenance and emergency conditions.
Keywords: NFV resource allocation; Virtualization; Offline; Online; Migrations; Power consumption; 5G delay

Abla Smahi, Hui Li, Wang Han, Ahmed Ameen Fateh, Ching Chuen Chan,
VFL-Chain: Bulletproofing Federated Learning in the V2X environments,
Future Generation Computer Systems,
Volume 155,
2024,
Pages 419-436,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2024.02.012.
(https://www.sciencedirect.com/science/article/pii/S0167739X24000578)
Abstract: Federated Learning (FL) has gained significant traction as a promising approach to enable collaborative machine learning (ML) while safeguarding data privacy across diverse applications, with the Vehicle-to-Everything (V2X) environment being a notable use case. However, conventional FL systems remain susceptible to model poisoning attacks, wherein malicious participants can introduce inaccurate or deliberately misleading local model updates to the central aggregator. In the context of V2X, such attacks could spawn catastrophic outcomes and jeopardize safety-critical applications. To tackle this pressing concern, we introduce a privacy-preserving and verifiable FL framework, dubbed VFL-Chain, designed to facilitate secure and efficient collaboration among intelligent connected vehicles (ICVs). VFL-Chain aspires to deliver a privacy-centric, verifiable FL experience complemented by a robust incentive mechanism tailored for ICVs. Our proposed solution leverages Bulletproofs to enable efficient verification of model update integrity, which are assimilated within smart contracts on an underlying permissioned blockchain. Furthermore, VFL-Chain presents a fair incentive mechanism that rewards honest participation, fostering a resilient and efficient FL implementation. We conduct an exhaustive security analysis and performance assessment of our proposed system, which underscores its efficacy in countering data poisoning attacks and augmenting the accuracy of FL.
Keywords: Federated Learning (FL); Vehicle-to-Everything (V2X); Bulletproofs; Blockchain; Verifiable computation; Incentive mechanism; Contract theory; Quantization

Shi Dong, Ping Wang, Khushnood Abbas,
A survey on deep learning and its applications,
Computer Science Review,
Volume 40,
2021,
100379,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100379.
(https://www.sciencedirect.com/science/article/pii/S1574013721000198)
Abstract: Deep learning, a branch of machine learning, is a frontier for artificial intelligence, aiming to be closer to its primary goal—artificial intelligence. This paper mainly adopts the summary and the induction methods of deep learning. Firstly, it introduces the global development and the current situation of deep learning. Secondly, it describes the structural principle, the characteristics, and some kinds of classic models of deep learning, such as stacked auto encoder, deep belief network, deep Boltzmann machine, and convolutional neural network. Thirdly, it presents the latest developments and applications of deep learning in many fields such as speech processing, computer vision, natural language processing, and medical applications. Finally, it puts forward the problems and the future research directions of deep learning.
Keywords: Deep learning; Stacked auto encoder; Deep belief networks; Deep Boltzmann machine; Convolutional neural network

Nur Fadhilah Mohd Shari, Amizah Malip,
State-of-the-art solutions of blockchain technology for data dissemination in smart cities: A comprehensive review,
Computer Communications,
Volume 189,
2022,
Pages 120-147,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.03.013.
(https://www.sciencedirect.com/science/article/pii/S0140366422000883)
Abstract: The concept of a smart city has been introduced to address the shortfalls of urbanization. The aid of technologies in smart cities is envisioned to be the key initiative to mitigate these challenges. However, the technologies exposure imposes challenging issues for the traditional centralized data dissemination schemes in smart cities. Consequently, smart city is undergoing a transition to a decentralized system through the implementation of blockchain technology. This paper presents an extensive survey of blockchain applicability for data dissemination in smart cities. We focus on the components of smart: transportation, healthcare, education, energy and building in smart cities and analyze state-of-the-art work on the integration of these components with blockchain. We discuss the advantages and shortcomings of existing literature and evaluate the performance efficiency and the extent of security and privacy achieved. The analysis leads to our new design of a secure smart city data dissemination framework using blockchain technology. To the best of our knowledge, this is the first comprehensive abstraction in the literature that can be utilized for the designation of future blockchain-smart cities data dissemination schemes with the inclusion of participating entities. Finally, we present open research issues and challenges and discuss future research directions.
Keywords: Blockchain; Smart cities; Data dissemination; Security; Privacy

Manish Snehi, Abhinav Bhandari,
Vulnerability retrospection of security solutions for software-defined Cyber–Physical System against DDoS and IoT-DDoS attacks,
Computer Science Review,
Volume 40,
2021,
100371,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100371.
(https://www.sciencedirect.com/science/article/pii/S1574013721000113)
Abstract: The wide dispersion of the Internet of Things (IoT), Software-defined Networks and Cloud Computing have given the wings to Cyber–Physical System adoption. The newfangled society relies so much on Cyber–Physical Systems, such as Smart Cities, Smart Agriculture, Medical Cyber System, that a dearth to any of the available services may lead to severe concerns. The IoT devices are unwittingly contributing to the denial of service attacks. Though the neoteric Software-defined Anything (SDx) paradigm has offered effective solution approaches to catastrophic IoT-based DDoS attacks, the novel designed solutions confront various vulnerabilities due to less secure IoT devices, high-volume real-time network traffic generated by the colossal amount of IoT devices, etc. In this paper, we present a comprehensive survey on vulnerability analysis of security solutions for Software-defined Cyber–Physical System. The paper delineates the architectural details of the Software-defined Cyber–Physical System and recommends amalgamation of Fog Computing as one of the architectural layers for overcoming a number of vulnerabilities. As contemporary technologies like IoT, Software-defined Networking and Cloud Computing are the soup ingredients of the Software-defined Cyber–Physical System, each of the individual components has been auscultated individually for security vulnerabilities with a focus on Distributed Denial of Service (DDoS and IoT-based DDoS) attacks. To anticipate the future recasting of the novel paradigm, we discuss the ongoing research and detailed vulnerability analysis with a focus on resiliency, performance, and scalability. Last but not least, we discuss the lessons learned and prospects to conclude.
Keywords: Cyber–Physical System; Internet of Things; IoT; Software-defined networks; SDN; Fog Computing; Distributed Denial of Service; DDoS; IoT-DDoS

Abbas Dehghani, Keyvan RahimiZadeh,
Design and performance evaluation of Mesh-of-Tree-based hierarchical wireless network-on-chip for multicore systems,
Journal of Parallel and Distributed Computing,
Volume 123,
2019,
Pages 100-117,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2018.09.008.
(https://www.sciencedirect.com/science/article/pii/S0743731518306592)
Abstract: Hybrid Wireless Network on Chip (WNoC) architecture has been proposed as a promising solution for addressing on-chip communication problems in many multicore systems. The choice of infrastructure topology directly affects the performance gain of the architecture. For exploiting the benefits offered by both mesh and tree topologies, we employ Mesh-of-Tree (MoT) topology as a new communication infrastructure for hybrid WNoC architectures. Moreover, long-distance links are effectively added to the MoT topology to form wireless MoT architecture and an appropriate communication routing scheme is presented to employ this paradigm. The performance and the system cost of the proposed WNoC architecture have been evaluated and compared with other notable WNoC architectures through comprehensive network-level simulations. Experimental results demonstrate the effectiveness of this wireless MoT architecture under both synthetic and realistic traffic patterns in terms of network throughput, latency, and power consumption. The results demonstrate that the proposed architecture can approximately achieve 35% improvement in saturation throughput, 58% reduction in transmission latency, and 43% improvement in power consumption over a wireline MoT-NoC, on average.
Keywords: Multicore systems; Network-on-Chip (NoC); Wireless interconnections; Mesh-of-Tree (MoT); Communication protocol

Parimal Mehta, Rajesh Gupta, Sudeep Tanwar,
Blockchain envisioned UAV networks: Challenges, solutions, and comparisons,
Computer Communications,
Volume 151,
2020,
Pages 518-538,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.023.
(https://www.sciencedirect.com/science/article/pii/S0140366419318675)
Abstract: Unmanned Aerial Vehicles (UAV) or drones is a technology with a huge potential for proving different efficient solutions in the smart city. It has been mostly adopted by military, aviation, civil, and commercial sectors for audio and video surveillance. The communication and security challenges of UAV is well mentioned by researchers and organizations (defense) around the world, although many of the challenges remain unresolved (especially for sensitive and consumer-related applications). Moreover, the conventional network systems may not be very efficient in dealing with the dynamic requirements of UAVs. As the UAVs are generally deployed in tough environments and terrains, therefore it is quite essential to provide a strong and secure network. Motivated from these facts, this paper presents an extensive survey on security issues in 5G-enabled UAV networks as per the literature published till the fourth quarter of 2019. It also presents the taxonomy of existing security issues in 5G-enabled UAV networks. Based on the findings from the survey, we present a Blockchain (BC)-based security solution and a summary of research challenges in the integration of BC with 5G-enabled UAV. Then, we present a case study of implementing BC with UAVs to secure industrial applications.
Keywords: Blockchain; Industry 4.0; Latency; Reliability; 5G; IoT; Unmanned aerial vehicles

Rasheed Hussain, Fatima Hussain, Sherali Zeadally,
Integration of VANET and 5G Security: A review of design and implementation issues,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 843-864,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.07.006.
(https://www.sciencedirect.com/science/article/pii/S0167739X19306909)
Abstract: The commercial adaptation of Vehicular Ad hoc NETwork (VANET) to achieve secure Intelligent Transportation System (ITS) heavily depends on the security guarantees for the end-users and consumers. Current VANET security standards address most of the security challenges faced by the vehicular networks. However, with the emergence of 5th Generation (5G) networks, and the demand for a range of new applications and services through vehicular networks, it is imperative to integrate 5G and vehicular networks. To achieve a seamless integration, various design and implementation issues related to 5G and VANETs must be addressed. We focus on the security issues that need to be considered in order to enable the secure integration of 5G and VANETs. More precisely, we conduct in-depth study of the current security issues, solutions, and standards used in vehicular networks and then we identify the security gaps in the existing VANET security solutions. We investigate the security features of 5G networks and discuss how they can be leveraged in vehicular networks to enable a seamless and efficient integration. We also propose a security architecture for vehicular networks wherein the current VANET security standards and 5G security features coexist to support secure VANET applications. Finally, we discuss some future challenges and research directions for 5G-enabled secure vehicular networks.
Keywords: Architecture; Connected car; 5G VANET; Security; VANET applications; VANET security standards

Muna Al-Hawawreh, Mamoun Alazab, Mohamed Amine Ferrag, M. Shamim Hossain,
Securing the Industrial Internet of Things against ransomware attacks: A comprehensive analysis of the emerging threat landscape and detection mechanisms,
Journal of Network and Computer Applications,
Volume 223,
2024,
103809,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103809.
(https://www.sciencedirect.com/science/article/pii/S108480452300228X)
Abstract: Due to the complexity and diversity of Industrial Internet of Things (IIoT) systems, which include heterogeneous devices, legacy and new connectivity protocols and systems, and distributed networks, sophisticated attacks like ransomware will likely target these systems in the near future. Researchers have focused on studying and addressing ransomware attacks against various platforms in recent years. However, to the best of our knowledge, no existing study investigates the new trends of ransomware tactics and techniques and provides a comprehensive analysis of ransomware attacks and their detection techniques for IIoT systems. Therefore, this paper investigates this attack and its associated detection techniques in IIoT systems in various aspects, including recent ransomware tactics, types, infected operating systems, and platforms. Specifically, we initially discuss the evolution of the IIoT system and its common architecture. Then, we provide an in-depth examination of the development of ransomware attacks and their constituent blocks, outline recent tactics and types of ransomware, and provide an extensive overview of the latest research on detection models. We also summarize numerous significant issues that have yet to be addressed and require further research. We conclude that offensive and defensive research is urgently needed to protect IIoT against ransomware attacks.
Keywords: IIoT; Ransomware attacks; Artificial intelligence; Detection; IT; OT

Shunliang Zhang, Yongming Wang, Weihua Zhou,
Towards secure 5G networks: A Survey,
Computer Networks,
Volume 162,
2019,
106871,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106871.
(https://www.sciencedirect.com/science/article/pii/S138912861830817X)
Abstract: To support various new use cases from vertical industries besides enhanced mobile broadband communication services, the 5G system aims to provide higher speed, lower latency, and massive connectivity to various devices by leveraging the evolution of 4G with the addition of new radio technology, service-based architecture, and cloud infrastructure. The introduction of new technologies, new use cases and people’s growing concerns regarding privacy issues brings new challenges to providing security and privacy protection for 5G. This paper makes an extensive review of the state of the art towards ensuring 5G security and privacy. By analyzing the lessons from the 4G security system, the requirements from new scenarios and models, the challenges resulting from new technology and paradigm, we identify typical security and privacy issues to be solved in 5G. Then, we discuss potential solutions from academia and industry to secure 5G networks from several perspectives, including the overall 5G security framework, core network, radio access network, cloud infrastructure, and the Internet of things. Finally, several key open issues and potential research directions are identified and discussed.
Keywords: 5G; Security; Privacy; Network slicing; SBA; New radio; D2D; NFV; SDN; MTC; MEC

Diego Hortelano, Teresa Olivares, M. Carmen Ruiz,
Reducing the energy consumption of the friendship mechanism in Bluetooth mesh,
Computer Networks,
Volume 195,
2021,
108172,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108172.
(https://www.sciencedirect.com/science/article/pii/S1389128621002309)
Abstract: This paper addresses new challenges arising from the emergence of Bluetooth mesh and its rapid adoption in Internet of Things (IoT) and Industrial IoT. One of the main problems associated with Bluetooth mesh is the power consumption of the mesh nodes, since the network needs to be scanned constantly to receive and relay messages. In order to reduce the energy consumption in the nodes where possible, the friendship mechanism is provided as a solution by the Bluetooth mesh specification. The friendship mechanism enables Low Power Nodes (LPNs) to spend most of their lifetime in sleep mode without losing the messages addressed to them. For this purpose, a node that is constantly in scan mode, called friend node (FN), stores these messages, sending them on request. This mechanism works with a stop-and-wait protocol, resulting in inefficient channel utilisation. However, to the best of our knowledge, there are no proposals for the improvement of this mechanism, likely due to its novelty. This paper studies different approaches for the improvement of the friendship mechanism: improving the Bearer Layer, improving time synchronisation and improving advertising channel utilisation. The results of this study reveal that each of the proposals presents a limitation that makes its final implementation unfeasible, and thus the challenge arises of developing a proposal for its optimisation. Accordingly, this work presents a proposal to improve the friendship mechanism with Burst Transmissions and Listen Before Transmit (BTLBT), which has been evaluated and validated in real hardware platforms. The results obtained for 4 LPNs show a reduction in the time required to receive the FN messages, as well as a 66.65% reduction in the time required to scan for each message. Finally, a consumption study is presented, where the results show that our proposal achieves a 19.81% improvement in lifetime compared to the standard friendship mechanism.
Keywords: Bluetooth mesh; Friendship; Low Power Nodes; Friend Nodes; Improvement

Adam Brinckman, Ewa Deelman, Sandeep Gupta, Jarek Nabrzyski, Soowang Park, Rafael Ferreira da Silva, Ian J. Taylor, Karan Vahi,
Collaborative circuit designs using the CRAFT repository,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 841-853,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.01.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X17310683)
Abstract: This paper provides an overview of the CRAFT repository, which exposes a collaborative gateway enabling circuit designers to share methods, documentation and intellectual property. The main goal for the repository’s development is to ensure that future designs for custom integrated circuits need not be reinvented for each design and fabrication cycle. This paper presents the architecture, design, and implementation of the collaborative repository, which capitalizes on the recent advances in production quality open-source collaborative frameworks, which are interfaced using a lightweight Javascript front-end to satisfy the requirements of the DARPA’s CRAFT program. The repository has been developed as an EmberJS application (front-end), that interacts with an instance of the Open Science Framework (OSF). This paper contextualizes the framework from the viewpoint of circuit designers and outlines the advantages of the tools and visualizations offered by the repository, in terms of increasing the efficiency of designers’ tasks. To this end, we also provide a description of two specific tools that have been exposed using the repository, which build on JSON schemas and allow users to develop and visualize the circuit design flow diagrams and the intellectual property they can reuse to accelerate their design process.
Keywords: Collaborative environments; Design flows; Chip design

Fadi Al-Turjman,
Intelligence and security in big 5G-oriented IoNT: An overview,
Future Generation Computer Systems,
Volume 102,
2020,
Pages 357-368,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.08.009.
(https://www.sciencedirect.com/science/article/pii/S0167739X19301074)
Abstract: Internet of Nano-Things (IoNT) overcomes critical difficulties and additionally open doors for wearable sensor based huge information examination. Conventional computing and/or communication systems do not offer enough flexibility and adaptability to deal with the gigantic amount of assorted information nowadays. This creates the need for legitimate components that can efficiently investigate and communicate the huge data while maintaining security and quality of service. In addition, while developing the ultra-wide Heterogeneous Networks (HetNets) associated with the ongoing Big Data project and 5G-based IoNT, it is required to resolve the emerging difficulties as well. Accordingly, these difficulties and other relevant design issues have been comprehensively reported in this survey. It mainly focuses on security issues and associated intelligence to be considered while managing these issues.
Keywords: IoNT; Security; Big data; Design factors

Shan Ji, Jiale Zhang, Yongjing Zhang, Zhaoyang Han, Chuan Ma,
LAFED: A lightweight authentication mechanism for blockchain-enabled federated learning system,
Future Generation Computer Systems,
Volume 145,
2023,
Pages 56-67,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2023.03.014.
(https://www.sciencedirect.com/science/article/pii/S0167739X23000912)
Abstract: Federated learning, as an emerging distributed machine learning technology, can use cross-device data to train a usable and secure shared model under the premise of protecting data privacy. However, the existing federated learning usually uploads the intermediate parameters to the central server to achieve model aggregation, which will cause significant privacy leakage. Recently, blockchain technology has become a research hotspot due to its advantages of decentralized and non-tampering features, providing new ideas for the realization of security certification for federated learning. However, blockchain-enabled federated learning also faces the following two challenges: (1) the identity authentication relies on the central server being fully trusted and the computation cost is heavy; (2) center-less authentication faces the challenges of efficiency and privacy leakage. To solve the above challenges, we propose a lightweight authentication mechanism for blockchain-enabled federated learning system, named LAFED. The innovations of LAFED are three-fold: (1) a lightweight authentication framework for blockchain-enabled federated learning; (2) a flexible consensus algorithm with zero-knowledge proof to verify the identity of each participant; (3) an adaptive model aggregation algorithm based on the model quality and node contribution to improve the performance. Extensive experimental results demonstrate that the proposed LAFED can achieve lightweight authentication while ensuring a high model accuracy.
Keywords: Federated learning; Blockchain; Lightweight authentication; Differential privacy; Consensus algorithm

Narges Yousefnezhad, Avleen Malhi, Kary Främling,
Security in product lifecycle of IoT devices: A survey,
Journal of Network and Computer Applications,
Volume 171,
2020,
102779,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102779.
(https://www.sciencedirect.com/science/article/pii/S1084804520302538)
Abstract: The Internet of Things (IoT) paradigm is considerably impacted by security challenges, which has lately demanded substantial consideration. Accordingly, certain reviews and surveys have been presented, focusing on disparate IoT-related domains, including IoT security, intrusion detection systems, and emerging technologies. However, in this article, we solely target IoT security with respect to product lifecycle stages. In that regard, we provide a comprehensive comparison of state-of-the-art surveys in an initial phase which concentrate on distinct parameters required for IoT security. Further, we present prominent solutions for addressing product lifecycle security in IoT. In this context, the contributions of this article are: (a) IoT product lifecycle security, (b) security taxonomy in IoT product lifecycle, (c) security solutions for each lifecycle phase in product lifecycle stages, and (d) open issues in these lifecycle stages that pose new research challenges. Consequently, the advancing research related to IoT security, especially with respect to product lifecycle, is explored through state-of-the-art developments in the domain of product lifecycle security.
Keywords: Internet of things; Product lifecycle; Lifecycle phases; Security solutions; Product lifecycle security; Device security

Junaid Arshad, Muhammad Ajmal Azad, Khaled Salah, Razi Iqbal, Muhammad Imran Tariq, Tariq Umer,
Performance analysis of content discovery for ad-hoc tactile networks,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 726-739,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.11.037.
(https://www.sciencedirect.com/science/article/pii/S0167739X1831834X)
Abstract: Tactile Internet evolves communications to encompass sensory information such as smell and haptic sensations combining ultra-low latency with extremely high availability, reliability, and security. Tactile Internet is realized through underpinning technologies such as Multi-access Edge and Fog computing which facilitate decentralized infrastructures and machine to machine (M2M) communications. Mobile ad hoc networks (MANETs) form the foundation layer of such infrastructures, enabling direct communication between autonomous and decentralized devices such as sensors and vehicles. Among other applications, autonomous ad hoc vehicular networks (VANETs) and vehicle to vehicle (V2V) communications require efficient content discovery and quality of data transfer. The mobility patterns of vehicles within this communication model could effect the quality of data exchanged between devices in a tactile network. Several mobility models exist describing mobility patterns of mobile users in MANETs. In this paper, we present a first performance study to evaluate the impact of different mobility models on content discovery techniques for tactile Internet comprising of fast-moving vehicles and devices. This study combines direct and derived mobility metrics evaluating impact on content discovery and content dissemination using NS-3. Our simulation results indicate that unstructured techniques may not scale well within a tactile network of fast moving vehicles while maintaining low latency and could suffer from performance degradation in a saturated environment. Furthermore, simulation results also demonstrate the resilience of the unstructured content discovery protocol in mobility scenarios with proactive routing and diverse behavior.
Keywords: Tactile internet; Mobility models; Content discovery; MANETs

Khaled Hejja, Xavier Hesselbach,
Evaluating impacts of traffic migration and virtual network functions consolidation on power aware resource allocation algorithms,
Future Generation Computer Systems,
Volume 101,
2019,
Pages 83-98,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.06.015.
(https://www.sciencedirect.com/science/article/pii/S0167739X19304108)
Abstract: Power consumption minimization and speed of solving the resource allocation problem on cloud datacenters adopting network function virtualization architecture are among the hot topics for future Internet networks. Therefore, this paper proposes a new power aware resource allocation algorithm supporting physical servers’ consolidations combined with virtual networks consolidation to minimize datacenters’ total costs for offline scenario. In addition, the new algorithm is also integrated with an optional standalone traffic migration algorithm that can be triggered according to specific conditions and at anytime. Simulations and evaluations of the algorithm resulted on lower total costs by 30% compared to recent algorithms from Eramo et al. (2017), and when virtual network functions consolidations option was activated, total costs were 25% lower than when it was inactive. However, when migrations option was activated in the proposed allocation algorithm it did not provide any significant savings in the total power consumptions, mainly because of the allocation strategy used by the algorithm in the first place, which managed to help it to precisely allocate and efficiently utilize the least physical resources. Finally, the results showed that without migrations, allocation times where faster by 10 times than activating migrations, suggesting to apply the migration option for emergency or maintenance conditions, and use the algorithm without migrations for faster allocations and efficient power consumptions.
Keywords: NFV resource allocation; Virtual machines consolidations; Traffic migration

Mauro Conti, Muhammad Hassan, Chhagan Lal,
BlockAuth: BlockChain based distributed producer authentication in ICN,
Computer Networks,
Volume 164,
2019,
106888,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106888.
(https://www.sciencedirect.com/science/article/pii/S1389128619308308)
Abstract: In Information Centric Networking (ICN), consumer mobility is supported by design in virtue of its connection-less pull-based communication model. However, producer mobility management is challenging as it focuses on the named-based resolution mechanism, which applies a dynamic and direct interaction between the producer and forwarding plane. In this paper, we consider the fundamental security issues related to producer mobility in ICN. These security issues exist mainly due to the insecure interaction of producer with the network’s forwarding information management system. We show that the current mobility solutions lack an adequate security mechanism and they invite severe security threats in the network (e.g., prefix hijacking and Denial of Service (DoS) attacks). To address such security threats, we propose a Blockchain based lightweight distributed mobile producer Authentication (BlockAuth) protocol to enable secure and efficient mobility management in ICN. BlockAuth authenticates the producers’ prefix(es) and enforce them to express only genuine routing updates for the prefix(es) to which they are entitled to advertise. The qualitative security analysis confirms that BlockAuth is robust against various security attacks to which mobile network and blockchain are particularly vulnerable (e.g., prefix hijacking, double spending, DoS attack). Additionally, the performance evaluation of BlockAuth shows that it maintains significant performance gain compared to the state-of-the-art prefix attestation proposals. In particular, it maintains up to 94% of the network’s original throughput, while it needs additional storage of just tens of megabytes.
Keywords: ICN; Mobility management; Authentication; Blockchain; Prefix hijacking; Security

Parvaneh Asghari, Amir Masoud Rahmani, Hamid Haj Seyyed Javadi,
Internet of Things applications: A systematic review,
Computer Networks,
Volume 148,
2019,
Pages 241-261,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.12.008.
(https://www.sciencedirect.com/science/article/pii/S1389128618305127)
Abstract: Internet of Things (IoT) is considered as an ecosystem that contains smart objects equipped with sensors, networking and processing technologies integrating and working together to provide an environment in which smart services are taken to the end users. The IoT is leading numerous benefits into the human life through the environment wherein smart services are provided to utilize every activity anywhere and anytime. All these facilities and services are conveyed through the diverse applications which are performed in the IoT environment. The most important utilities that are achieved by the IoT applications are monitoring and consequently immediate decision making for efficient management. In this paper, we intend to survey in divers IoT application domains to comprehend the different approaches in IoT applications which have been recently presented based on the Systematic Literature Review (SLR) method. The aim of this paper is to categorize analytically and statistically, and analyze the current research techniques on IoT applications approaches published from 2011 to 2018. A technical taxonomy is presented for the IoT applications approaches according to the content of current studies that are selected with SLR process in this study including health care, environmental monitoring, smart city, commercial, industrial and general aspects in IoT applications. IoT applications are compared with each other according to some technical features such as Quality of Service (QoS), proposed case study and evaluation environments. The achievements and disadvantages of each study is discussed as well as presenting some hints for addressing their weaknesses and highlighting the future research challenges and open issues in IoT applications.
Keywords: Application-based services; Internet of things; Systematic literature review; Smart objects; Quality of service

Kentaro Kita, Yuki Koizumi, Toru Hasegawa,
Private retrieval of location-related content using k-anonymity and application to ICN,
Computer Networks,
Volume 209,
2022,
108908,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.108908.
(https://www.sciencedirect.com/science/article/pii/S1389128622001013)
Abstract: Several platforms to efficiently retrieve content from Internet of Things (IoT) devices installed in various locations have been proposed for information-centric networking (ICN). However, location privacy is at stake in such platforms because consumers retrieve content by specifying the plaintext names of the locations of their interest (LOIs). Previous studies on IP have leveraged k-anonymity of location offered by a trusted proxy called an anonymizer to hide LOIs. Specifically, an anonymizer sends content requests to k locations in a location anonymity set, which comprises an LOI and the other dummy locations. This technique can be applied to ICN; however, two problems need to be solved: the adversary models are unrealistic and the requirements for a location anonymity set have been defined in ad-hoc manners. In this study, we assume a semi-honest anonymizer and define the requirements rigorously using the notions of entropy and t-closeness. Next, we design an architecture for location privacy protection and an algorithm for location anonymity set generation. Finally, we evaluate the overhead incurred by our architecture and the quality of generated location anonymity sets through experiments under a realistic scenario. Our results indicate that our architecture and algorithm offer strong location privacy with marginal overhead.
Keywords: Information-centric networking; Privacy; Anonymity; Location; Internet of Things

Karel Durkota, Viliam Lisý, Branislav Bošanský, Christopher Kiekintveld, Michal Pěchouček,
Hardening networks against strategic attackers using attack graph games,
Computers & Security,
Volume 87,
2019,
101578,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2019.101578.
(https://www.sciencedirect.com/science/article/pii/S0167404819300689)
Abstract: We consider the problem faced by a network administrator (defender) when deploying limited security resources to protect a network against a strategic attacker. To evaluate the effectiveness of a defense strategy, one must consider possible counterattacks that an attacker can choose. We use game theory to model the interaction between the defender and the attacker. Game theory provides relevant concepts and algorithms for computing optimal strategies in environments with multiple decision makers. To model the space of attacker’s possible actions, we use attack graphs, that compactly represent all known sequences of attacker’s action that may lead to successful attack for a given network. We demonstrate our approach on a specific type of defense actions, where the defender deploys deceptive hosts and services (honeypots) to detect and mitigate attacks. We assume the worst-case attacker who has a complete knowledge of the (typically randomized) defense strategy. We seek the optimal defense strategy against this attacker in the form of a Stackelberg equilibrium. Computing this solution exactly using standard techniques has limited scalability, so we investigate several approaches for increasing scalability to realistic problems. We introduce optimization methods for finding exact solutions for these games and then propose a variety of polynomial heuristic algorithms that scale to significantly larger games. We analyze the scalability and the quality of these heuristic solutions on realistic network topologies. We show that the strategies found by the heuristics are often near-optimal and that they outperform non-game-theoretic baselines. Finally, we show how attack graph games can be used to answer various research questions relevant to network security administrators.
Keywords: Game theory; Network security; Honeypot; Network configuration management; Stackelberg equilibrium; Heuristic algorithms

Robert Luh, Helge Janicke, Sebastian Schrittwieser,
AIDIS: Detecting and classifying anomalous behavior in ubiquitous kernel processes,
Computers & Security,
Volume 84,
2019,
Pages 120-147,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2019.03.015.
(https://www.sciencedirect.com/science/article/pii/S0167404818314457)
Abstract: Targeted attacks on IT systems are a rising threat against the confidentiality, integrity, and availability of critical information and infrastructures. With the rising prominence of advanced persistent threats (APTs), identifying and understanding such attacks has become increasingly important. Current signature-based systems are heavily reliant on fixed patterns that struggle with unknown or evasive applications, while behavior-based solutions usually leave most of the interpretative work to a human analyst. In this article we propose AIDIS, an Advanced Intrusion Detection and Interpretation System capable to explain anomalous behavior within a network-enabled user session by considering kernel event anomalies identified through their deviation from a set of baseline process graphs. For this purpose we adapt star structures, a bipartite representation used to approximate the edit distance between two graphs. Baseline templates are generated automatically and adapt to the nature of the respective operating system process. We prototypically implemented smart anomaly classification through a set of competency questions applied to graph template deviations and evaluated the approach using both Random Forest and linear kernel support vector machines. The determined attack classes are ultimately mapped to a dedicated APT attacker/defender meta model that considers actions, actors, as well as assets and mitigating controls, thereby enabling decision support and contextual interpretation of ongoing attacks.
Keywords: Intrusion detection; Malware; Anomaly detection; Graph matching; Star structure; Security model; Semantic gap; Machine learning; Classification; SVM

Yang Yang, Songtao Guo, Guiyan Liu, Lin Yi,
Fine granularity resource allocation of virtual data center with consideration of virtual switches,
Journal of Network and Computer Applications,
Volume 175,
2021,
102916,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102916.
(https://www.sciencedirect.com/science/article/pii/S1084804520303751)
Abstract: Virtual data center (VDC) embedding is being regarded as a critical issue to provide performance guarantees for the future success of cloud computing. Most existing works neglected the embedding of virtual switches (VSs), especially, on the physical switches (PSs), resulting in low utilization of physical resources. Moreover, most works assumed that multiple virtual nodes in the same request cannot be embedded in the same physical node, leading to low embedding efficiency in data centers. To address the above two problems, in this paper, we first propose two fine granularity models to formulate the VDC embedding problem efficiently with the consideration of two possible embedding positions of VSs. Then traditional flow conservation law is modified to allow multiple virtual nodes to coexist on the same physical node. Finally, we propose multiple efficient embedding algorithms to solve two NP-hard problems. Comparing with existing methods, our algorithms employing heuristic information from the relaxed model can find sub-optimal solutions in polynomial time. Extensive simulation results show that our proposals outperform the existing methods in terms of acceptance ratio, revenue, and utilization.
Keywords: Cloud computing; Virtual data center; Virtual switch; Embedding; Heuristic algorithm

Yifei Lu, Changjiang Cui, Xu Ma, Zeqi Ruan,
A3DCT: A cubic acceleration TCP for data center networks,
Journal of Network and Computer Applications,
Volume 216,
2023,
103654,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103654.
(https://www.sciencedirect.com/science/article/pii/S1084804523000735)
Abstract: In recent years, online data-intensive (OLDI) applications have become particularly common in data center networks (DCNs), such as web search, advertisement systems, and distributed machine learning. OLDI applications have strict latency requirements for short flows and must operate under soft-real-time constraints (e.g., 300 ms latency). On the other hand, there exist long flows that are unaware of flow latency but are sensitive to throughput. Thus, a special transmission protocol that can satisfy different demands for both long flows and short flows is immediately needed. L2DCT is one of the most representative differentiated transmission protocols, which aim at the reduction in completion time for short flows. However, the performance of L2DCT becomes less significant in the scenario where short flows account for the majority (e.g., data mining). For the purposes of minimizing the flow completion time (FCT) of short flows, achieving the minimum average flow completion time (AFCT), and guaranteeing the deadline constraints upon flow transmission times, we propose a cubic acceleration TCP for DCNS, which is referred to as A3DCT. A3DCT employs the Shortest Remaining Process Time (SRPT) scheduling policy to adjust its congestion window according to the remaining bytes of the flow. Moreover, to improve the priority of short flows without affecting the throughput of long flows, A3DCT leverages a cubic function to indicate the urgency of short flows. Finally, A3DCT accelerates the recovery speed, in a flexible manner, depending on the urgency of different flows in a non-congestion state to request the bandwidth to finish transmission. We perform simulations of different scales by using NS-3 simulator to evaluate the performance of A3DCT. The evaluation results justify that A3DCT can not only achieve a low flow completion time, especially for short flows, but also guarantee the high throughput of long flows.
Keywords: OLDI application; Data center network; TCP; Cubic acceleration; SRPT

Gonçalo Vítor, Pedro Rito, Susana Sargento, Filipe Pinto,
A scalable approach for smart city data platform: Support of real-time processing and data sharing,
Computer Networks,
Volume 213,
2022,
109027,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109027.
(https://www.sciencedirect.com/science/article/pii/S1389128622001839)
Abstract: The concept of a smart city comes with the need to support a data platform that can gather, process and export the data of millions of sensors, coming from different sources, with information in different formats, in a scalable approach for real-time and historical data visualization, processing, and actuation in the city. This paper proposes a Data Platform for the Aveiro Tech City Living Lab, to gather, process, visualize and actuate on mobility, environmental and network data. The architecture of the platform provides a real open platform that is accessible for third-parties to collect data and to experiment their own solutions, through a secure and open data platform at their disposal. The platform was also designed to be scaled and modified when necessary, being composed by distributed solutions across the existent modules. The results with respect to the amount of data gathered, the ingestion and query performance comparison between the persistence solutions considered, and examples of data, show how this platform can be used to efficiently develop new applications and use both real-time and historical data for future predictions and actuations in a smart city.
Keywords: Internet of Things (IoT); Smart cities; Time series analysis; Data management design; Data analytics; Persistence; Scalability

Saniya Zafar, Sobia Jangsher, Ouns Bouachir, Moayad Aloqaily, Jalel Ben Othman,
QoS enhancement with deep learning-based interference prediction in mobile IoT,
Computer Communications,
Volume 148,
2019,
Pages 86-97,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.09.010.
(https://www.sciencedirect.com/science/article/pii/S0140366419306620)
Abstract: With the acceleration in mobile broadband, wireless infrastructure plays a significant role in Internet-of-Things (IoT) to ensure ubiquitous connectivity in mobile environment, making mobile IoT (mIoT) as center of attraction. Usually intelligent systems are accomplished through mIoT which demands for the increased data traffic. To meet the ever-increasing demands of mobile users, integration of small cells is a promising solution. For mIoT, small cells provide enhanced Quality-of-Service (QoS) with improved data rates. In this paper, mIoT-small cell based network in vehicular environment focusing city bus transit system is presented. However, integrating small cells in vehicles for mIoT makes resource allocation challenging because of the dynamic interference present between small cells which may impact cellular coverage and capacity negatively. This article proposes Threshold Percentage Dependent Interference Graph (TPDIG) using Deep Learning-based resource allocation algorithm for city buses mounted with moving small cells (mSCs). Long–Short Term Memory (LSTM) based neural networks are considered to predict city buses locations for interference determination between mSCs. Comparative analysis of resource allocation using TPDIG, Time Interval Dependent Interference Graph (TIDIG), and Global Positioning System Dependent Interference Graph (GPSDIG) is presented in terms of Resource Block (RB) usage and average achievable data rate of mIoT-mSC network.
Keywords: Deep learning; Internet-of-Things (IoT); Mobile IoT (mIoT); Dependent Interference; Resource allocation; Moving small cells; Interference Graph

Morteza Safaei Pour, Christelle Nader, Kurt Friday, Elias Bou-Harb,
A Comprehensive Survey of Recent Internet Measurement Techniques for Cyber Security,
Computers & Security,
Volume 128,
2023,
103123,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103123.
(https://www.sciencedirect.com/science/article/pii/S0167404823000330)
Abstract: As the Internet has transformed into a critical infrastructure, society has become more vulnerable to its security flaws. Despite substantial efforts to address many of these vulnerabilities by industry, government, and academia, cyber security attacks continue to increase in intensity, diversity, and impact. Thus, it becomes intuitive to investigate the current cyber security threats, assess the extent to which corresponding defenses have been deployed, and evaluate the effectiveness of risk mitigation efforts. Addressing these issues in a sound manner requires large-scale empirical data to be collected and analyzed via numerous Internet measurement techniques. Although such measurements can generate comprehensive and reliable insights, doing so encompasses complex procedures involving the development of novel methodologies to ensure accuracy and completeness. Therefore, a systematic examination of recently developed Internet measurement approaches for cyber security must be conducted to enable thorough studies that employ several vantage points, correlate multiple data sources, and potentially leverage past successful techniques for more recent issues. Unfortunately, performing such an examination is challenging, as the literature is highly scattered. In large part, this is due to each research effort only focusing on a small portion of the many constituent parts of the Internet measurement domain. Moreover, to the best of our knowledge, no studies have offered an in-depth examination of this critical research domain in order to promote future advancements. To bridge these gaps, we explore all pertinent facets of utilizing Internet measurement techniques for cyber security, ranging from threats within specific application domains to threats themselves. We provide a taxonomy of cyber security-related Internet measurement studies across two dimensions. One dimension relates to the many vertical layers (and components) of the Internet ecosystem, while the other relates to internal normal functions vs. the negative impact of external parties in the Internet and physical world. A comprehensive comparison of the gathered studies is also offered in terms of measurement technique, scope, measurement size, vantage size, and the analysis approach that was leveraged. Finally, a discussion of the roadblocks to performing effective Internet measurements and possible future research directions is elaborated.
Keywords: Internet measurement; Cyber security; Large-scale analysis; Security threats

Bam Bahadur Sinha, R. Dhanalakshmi,
Recent advancements and challenges of Internet of Things in smart agriculture: A survey,
Future Generation Computer Systems,
Volume 126,
2022,
Pages 169-184,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.08.006.
(https://www.sciencedirect.com/science/article/pii/S0167739X21003113)
Abstract: The Internet of Things (IoT) is an evolving paradigm that seeks to connect different smart physical components for multi-domain modernization. To automatically manage and track agricultural lands with minimal human intervention, numerous IoT-based frameworks have been introduced. This paper presents a rigorous discussion on the major components, new technologies, security issues, challenges and future trends involved in the agriculture domain. An in-depth report on recent advancements has been covered in this paper. The goal of this survey is to help potential researchers detect relevant IoT problems and, based on the application requirements, adopt suitable technologies. Furthermore, the significance of IoT and Data Analytics for smart agriculture has been highlighted.
Keywords: IoT; Automated irrigation; Data analytics; IoT challenges; IoT smart farming

Guolin Sun, Tong Zhan, Boateng Gordon Owusu, Ayepah-Mensah Daniel, Guisong Liu, Wei Jiang,
Revised reinforcement learning based on anchor graph hashing for autonomous cell activation in cloud-RANs,
Future Generation Computer Systems,
Volume 104,
2020,
Pages 60-73,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.09.044.
(https://www.sciencedirect.com/science/article/pii/S0167739X18318788)
Abstract: Cloud radio access networks (C-RANs) have been regarded in recent times as a promising concept in future 5G technologies where all DSP processors are moved into a central base band unit (BBU) pool in the cloud, and distributed remote radio heads (RRHs) compress and forward received radio signals from mobile users to the BBUs through radio links. In such dynamic environment, automatic decision-making approaches, such as artificial intelligence based deep reinforcement learning (DRL), become imperative in designing new solutions. In this paper, we propose a generic framework of autonomous cell activation and customized physical resource allocation schemes to balance energy consumption and QoS satisfaction in wireless networks. We formulate the cell activation problem as a Markov decision process and set up a revised reinforcement learning model based on K-means clustering and anchor-graph hashing to satisfy the QoS requirements of users and to achieve low energy consumption with the minimum number of active RRHs under varying traffic demand and user mobility. Extensive simulations are conducted to show the effectiveness of our proposed solution compared with existing schemes.
Keywords: Reinforcement learning; Anchor graph hashing; K-means clustering; Autonomous cell activation; Cloud radio access networks

Mustafa M. Al-Sayed, Hesham A. Hassan, Fatma A. Omara,
CloudFNF: An ontology structure for functional and non-functional features of cloud services,
Journal of Parallel and Distributed Computing,
Volume 141,
2020,
Pages 143-173,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2020.03.019.
(https://www.sciencedirect.com/science/article/pii/S0743731519300437)
Abstract: Recently, cloud computing becomes one of the main orientations of many researchers and companies in the IT area. Therefore, a huge number of cloud services have been developed. Because of the diversity and heterogeneity in providing these services, it is urgently needed to develop a unified cloud ontology. Such ontology can classify these services appropriately and participate as a mapping layer to present such services in a unified description format. Although many studies have been conducted to build cloud ontologies, they have adopted the cloud service layers-based structure. On the other hand, the existing cloud services may involve functionalities from different layers due to the continual increase in the complexity of customer demands. Unfortunately, there are no clear relations to organize the interventions among these layers. Therefore, such services may be difficult to be classified into a specific cloud service layer. Additionally, the layers-based structure of ontologies represents an obstacle to address important issues, such as cloud service recommendation. Despite there are few cloud service functionality-based cloud ontologies, they suffer from many overlaps, lack of semantic relations, or poor granularity of concepts. Also, all the existing cloud ontologies (i.e., layers-based and functionality-based) lack important criteria, such as completeness, consistency, conciseness, clarity, preciseness, and granularity. In this paper, a comprehensive cloud ontology called CloudFNF has been proposed to overcome such drawbacks. According to the structure of the proposed ontology, cloud services are classified as functionality-based instead of layers-based. Also, non-functional features of cloud services (i.e., configuration and QoS features) are considered to enable services of the same functionalities to be ranked efficiently. Based on our previously suggested cloud ontology evaluation framework, our proposed cloud ontology has been evaluated compared to the most related cloud ontologies. The evaluation results show that the proposed CloudFNF ontology outperforms the other ontologies.
Keywords: Cloud computing; Cloud ontology; Cloud taxonomy; QoS; Cloud service recommendation

An Xie, Huawei Huang, Xiaoliang Wang, Song Guo, Zhuzhong Qian, Sanglu Lu,
Dual: Deploy stateful virtual network function chains by jointly allocating data-control traffic,
Computer Networks,
Volume 162,
2019,
106868,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106868.
(https://www.sciencedirect.com/science/article/pii/S1389128619303962)
Abstract: Network Function Virtualization (NFV) allows to deploy network functions at low cost and high flexibility. Usually traffic needs to pass through several network functions in a particular order, which is known as network function chaining. Due to capacity limitation, one instance of a particular network function in the chain usually needs to be scaled to multiple instances when the traffic load increases. Recent studies show that network functions maintain rich internal states, which correlate with flows and determine their processing actions. In order to guarantee network functions to behave the same as before the scaling, these states on multiple instances need to be synchronized. As a result, communication requirement for synchronizing these states arises. Our experimental investigations show that such communication traffic for state synchronization is non-negligible. Unfortunately, to our best knowledge, no existing work for network function chain deployment considered such traffic. In this paper, we consider the problem of allocating both flow traffic and synchronization traffic with the goal to minimize the consumption of network interface card’s bandwidth, which is considered as the bottleneck resource of the network. The main challenge is that such two kinds of traffic are correlated and even conflicting. To deal with this challenge, a new graph-based model named Dual (Deploy Stateful Virtual Network Function Chains by Jointly Allocating Data-Control Traffic) is proposed, which characterizes the interaction of data-control traffic in an elegant manner. Based on Dual, the problem mentioned above is formulated through ILP and an approximation algorithm is designed. Extensive simulations show that the proposed approach is effective in terms of reducing server bandwidth usage.
Keywords: Network function virtualization; Stateful virtual network function; Network function chain deployment

Rajesh Gupta, Sudeep Tanwar, Sudhanshu Tyagi, Neeraj Kumar,
Machine Learning Models for Secure Data Analytics: A taxonomy and threat model,
Computer Communications,
Volume 153,
2020,
Pages 406-440,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.02.008.
(https://www.sciencedirect.com/science/article/pii/S0140366419318493)
Abstract: In recent years, rapid technological advancements in smart devices and their usage in a wide range of applications exponentially increases the data generated from these devices. So, the traditional data analytics techniques may not be able to handle this extreme volume of data known as Big Data (BD) generated by different devices. However, this exponential increase of data opens the doors for the different type of attackers to launch various attacks by exploiting various vulnerabilities (SQL injection, OS fingerprinting, malicious code execution, etc.) during data analytics. Motivated from the aforementioned discussion, in this paper, we explored Machine Learning (ML) and Deep Learning (DL)-based models and techniques which are capable off to identify and mitigate both the known as well as unknown attacks. ML and DL-based techniques have the capabilities to learn from the traffic pattern using training and testing datasets in the extensive network domains to make intelligent decisions concerning attack identification and mitigation. We also proposed a DL and ML-based Secure Data Analytics (SDA) architecture to classify normal or attack input data. A detailed taxonomy of SDA is abstracted into a threat model. This threat model addresses various research challenges in SDA using multiple parameters such as-efficiency, latency, accuracy, reliability, and attacks launched by the attackers. Finally, a comparison of existing SDA proposals with respect to various parameters is presented, which allows the end users to select one of the SDA proposals in comparison to its merits over the others.
Keywords: Big data; Secure Data Analytics; Data reduction; Machine learning models; Threat model; Data security and privacy

G. G. Md. Nawaz Ali, Kai Liu, Victor C.S. Lee, Peter H.J. Chong, Yong Liang Guan, Jun Chen,
Towards efficient and scalable implementation for coding-based on-demand data broadcast,
Computer Networks,
Volume 154,
2019,
Pages 88-104,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.02.012.
(https://www.sciencedirect.com/science/article/pii/S138912861930221X)
Abstract: Network coding has been demonstrated as a promising solution to further enhancing the bandwidth efficiency for on-demand broadcast. In this work, first, we show the performance improvement of a straightforward implementation of coding based on-demand data broadcast algorithms over the traditional on-demand broadcast approaches. Second, as the straightforward implementation of the optimal approach has overwhelming computational overhead, we propose an efficient generalized implementation scheme, which can be applied to all the existing on-demand scheduling algorithms. The proposed scheme reduces the computational overhead while achieves the same performance as the straightforward implementation. Third, to further enhance system scalability, we propose an approximate implementation method with even lower computational overhead while maintaining near optimal performance. Finally, we conduct an extensive simulation study and the results demonstrate that the proposed efficient implementation schemes can improve the system performance over 40% compared with the traditional broadcast approach, and the computational overhead can be reduced by 75% compared with the straightforward implementation. In addition, we show that the proposed approximate implementation can further reduce the computational overhead significantly and it is able to strike a balance between the service performance and system scalability.
Keywords: On-demand broadcast; Data scheduling; Network coding; Algorithm design; Performance evaluation

Mostafa Said, Ahmed Shalaby, Fayez Gebali,
Thermal-aware network-on-chips: Single- and cross-layered approaches,
Future Generation Computer Systems,
Volume 91,
2019,
Pages 61-85,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.08.041.
(https://www.sciencedirect.com/science/article/pii/S0167739X18312160)
Abstract: In the era of the billion transistors on a chip that are capable of implementing thousands of processing cores, Network-on-Chips (NoCs) are the most viable and scalable solution to connect this massive number of cores. NoCs gradually support manycore processors technology, from conventional ICs with tens to hundreds of cores to 3D integrated ones with even more cores. On the other hand, many challenges arise over time that impede the growth of such enabling technology. The thermal problem is still a serious example of NoC design complications that can greatly limit NoC designs. This forces designers to model, design, and innovate new tools and techniques either offline or at runtime to measure, manage, and solve thermal problems. As a sequel, various techniques have been introduced, covering all layers from the top-most networking application layer to the bottom most-physical layer. In this survey paper, the authors put their hands on various designs and modeling techniques for thermal-aware NoC management. Most importantly, key ideas and research directions are classified, pointed out, and demonstrated with a reasonable amount of details that enables interested researchers to come up and grasp all research directions for thermal-aware NoC technologies.
Keywords: Thermal management; Network-on-chip; System-on-chip; Manycore processors; 3D integration

Misbah Shafi, Rakesh Kumar Jha, Manish Sabraj,
A survey on security issues of 5G NR: Perspective of artificial dust and artificial rain,
Journal of Network and Computer Applications,
Volume 160,
2020,
102597,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102597.
(https://www.sciencedirect.com/science/article/pii/S1084804520300710)
Abstract: 5G NR (New Radio) incorporates concepts of novel technologies such as spectrum sharing, D2D communication, UDN, and massive MIMO. However, providing security and identifying the security threats to these technologies occupies the prime concern. This paper intends to provide an ample survey of security issues and their countermeasures encompassed in the technologies of 5G NR. Further, security concerns of each technology are defined mathematically. Thereby defining the impact on the factors of security. Moreover, a methodology is developed in which the influence on security due to artificially generated rain and artificially generated dust on the wireless communication network is studied. By doing so, an attacking scenario is identified, where a half-duplex attack in D2D communication is attained. Half-duplex attack specifies the attack solely on the downlink to spoof the allocated resources, with reduced miss-rate. Thus, ultra-reliable and adequate advances are required to be addressed to remove the obstacles that create a hindrance in achieving the secured and authenticated communicating network.
Keywords: 5G NR; Security; Artificial rain; Artificial dust; D2D; Miss-rate; Half-duplex attack

Thanasis Kotsiopoulos, Panagiotis Sarigiannidis, Dimosthenis Ioannidis, Dimitrios Tzovaras,
Machine Learning and Deep Learning in smart manufacturing: The Smart Grid paradigm,
Computer Science Review,
Volume 40,
2021,
100341,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2020.100341.
(https://www.sciencedirect.com/science/article/pii/S157401372030441X)
Abstract: Industry 4.0 is the new industrial revolution. By connecting every machine and activity through network sensors to the Internet, a huge amount of data is generated. Machine Learning (ML) and Deep Learning (DL) are two subsets of Artificial Intelligence (AI), which are used to evaluate the generated data and produce valuable information about the manufacturing enterprise, while introducing in parallel the Industrial AI (IAI). In this paper, the principles of the Industry 4.0 are highlighted, by giving emphasis to the features, requirements, and challenges behind Industry 4.0. In addition, a new architecture for AIA is presented. Furthermore, the most important ML and DL algorithms used in Industry 4.0 are presented and compiled in detail. Each algorithm is discussed and evaluated in terms of its features, its applications, and its efficiency. Then, we focus on one of the most important Industry 4.0 fields, namely the smart grid, where ML and DL models are presented and analyzed in terms of efficiency and effectiveness in smart grid applications. Lastly, trends and challenges in the field of data analysis in the context of the new Industrial era are highlighted and discussed such as scalability, cybersecurity, and big data.
Keywords: Industry 4.0; Machine Learning; Deep Learning; Industrial AI; Smart Grid

Simge Gizem Ozcan, Muge Sayit,
Multipath transmission aware ABR algorithm for SVC HAS,
Computer Communications,
Volume 201,
2023,
Pages 20-36,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2023.01.015.
(https://www.sciencedirect.com/science/article/pii/S0140366423000233)
Abstract: Adaptive video streaming over Hypertext Transfer Protocol (HTTP) is one of the most popular applications that have been used on the Internet in the last decade. In such applications, clients change the quality of the received video throughout the streaming session with respect to current network conditions. In these systems, the use of scalable coded video is one of the alternatives that can obtain video files encoded at different qualities. In this paper, we propose an adaptive bitrate (ABR) algorithm that aims to improve the quality of not only future segments but also the segments in the buffer. The quality of the already buffered segments is improved by downloading additional layers of the SVC-coded video, hence, preventing network resources from being wasted while increasing Quality of Experience (QoE). We use multiple paths between the server and the clients for transferring the video packets. In such systems, the ABR algorithm should determine which segment and layer should be requested from which path and when. The proposed ABR algorithm selects a group of segments to increase the quality of the buffered segments and for future segments, and selects paths for each segment in this group. Simulation results show that clients achieve higher QoE by using our approach when compared with conventional ABR algorithms. Furthermore, we show that using multiple paths cannot guarantee that QoE is maximized unless the end-system is aware of multipath transmission.
Keywords: Video streaming over HTTP; Scalable video coding; Adaptive bit rate algorithm; Software defined networks; Quality of experience

Qing Li, Yang Liu, Zhijie Zhu, Hengtong Li, Yong Jiang,
BOND: Flexible failure recovery in software defined networks,
Computer Networks,
Volume 149,
2019,
Pages 1-12,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2018.11.020.
(https://www.sciencedirect.com/science/article/pii/S1389128618312416)
Abstract: Failure recovery has been a hot potato in the maintenance of networks, including software defined networks (SDNs), for many years. To guarantee the network availability in SDNs, some delicate schemes and algorithms have been proposed to pre-compute and set up the backup paths for potential network failures. However, they are generally unpractical for deployment, since they ignore either the failure recovery requirements of different flows or the constrained resources (e.g., flow tables) of SDN switches. In this paper, we propose BOND, a flexible failure recovery system in SDNs. Firstly, we allocate backup rules, instead of backup paths, to forwarding switches with flows’ requirements considered. Secondly, we accelerate the failure recovery with a global hash table and precisely select backup paths to avoid potential congestions in the post-recovery network. To demonstrate the efficiency of BOND, we construct comprehensive experiments with real-world topologies. The results show that compared with Multi-Protocol Label Switching (MPLS) Fast Reroute (FRR), BOND consumes an order of magnitude less memory to achieve the same network recovery goal. Further, we implement BOND with five HUAWEI S5720 SDN switches, two Dell end hosts and a RYU controller. The prototype experiments of this real network implementation further confirm the efficiency of BOND.
Keywords: Network failure recovery; Network resilience; Software defined networking

Ayaz Ali Khan, Muhammad Zakarya, Rahim Khan, Izaz Ur Rahman, Mukhtaj Khan, Atta ur Rehman Khan,
An energy, performance efficient resource consolidation scheme for heterogeneous cloud datacenters,
Journal of Network and Computer Applications,
Volume 150,
2020,
102497,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.102497.
(https://www.sciencedirect.com/science/article/pii/S1084804519303571)
Abstract: Datacenters are the principal electricity consumers for cloud computing that provide an IT backbone for today's business and economy. Numerous studies suggest that most of the servers, in the US datacenters, are idle or less-utilised, making it possible to save energy by using resource consolidation techniques. However, consolidation involves migrations of virtual machines, containers and/or applications, depending on the underlying virtualisation method; that can be expensive in terms of energy consumption and performance loss. In this paper, we: (a) propose a consolidation algorithm which favours the most effective migration among VMs, containers and applications; and (b) investigate how migration decisions should be made to save energy without any negative impact on the service performance. We demonstrate through a number of experiments, using the real workload traces for 800 hosts, approximately 1516 VMs, and more than million containers, how different approaches to migration, will impact on datacenter's energy consumption and performance. We suggest, using reasonable assumptions for datacenter set-up, that there is a trade-off involved between migrating containers and virtual machines. It is more performance efficient to migrate virtual machines; however, migrating containers could be more energy efficient than virtual machines. Moreover, migrating containerised applications, that run inside virtual machines, could lead to energy and performance efficient consolidation technique in large-scale datacenters. Our evaluation suggests that migrating applications could be ~5.5% more energy efficient and ~11.9% more performance efficient than VMs migration. Further, energy and performance efficient consolidation is ~14.6% energy and ~7.9% performance efficient than application migration. Finally, we generalise our results using several repeatable experiments over various workloads, resources and datacenter set-ups.
Keywords: Containers; Performance; Migrations; Energy efficiency; Clouds

Muthu M. Baskaran, Thomas Henretty, James Ezick, Richard Lethin, David Bruns-Smith,
Enhancing Network Visibility and Security through Tensor Analysis,
Future Generation Computer Systems,
Volume 96,
2019,
Pages 207-215,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.01.039.
(https://www.sciencedirect.com/science/article/pii/S0167739X18302073)
Abstract: The increasing size, variety, rate of growth and change, and complexity of network data has warranted advanced network analysis and services. Tools that provide automated analysis through traditional or advanced signature-based systems or machine learning classifiers suffer from practical difficulties. These tools fail to provide comprehensive and contextual insights into the network when put to practical use in operational cyber security. In this paper, we present an effective tool for network security and traffic analysis that uses high-performance data analytics based on a class of unsupervised learning algorithms called tensor decompositions. The tool aims to provide a scalable analysis of the network traffic data and also reduce the cognitive load of network analysts and be network-expert-friendly by presenting clear and actionable insights into the network. In this paper, we demonstrate the successful use of the tool in two completely diverse operational cyber security environments, namely, (1) security operations center (SOC) for the SCinet network at the SuperComputing (SC) Conference in 2016 and 2017 and (2) Reservoir Labs’ Local Area Network (LAN). In each of these environments, we produce actionable results for cyber security specialists including (but not limited to) (1) finding malicious network traffic involving internal and external attackers using port scans, SSH brute forcing, and NTP amplification attacks, (2) uncovering obfuscated network threats such as data exfiltration using DNS port and using ICMP traffic, and (3) finding network misconfiguration and performance degradation patterns.
Keywords: Network analysis; Cyber security; Tensor decompositions; Network threats

Amritpal Singh, Sahil Garg, Shalini Batra, Neeraj Kumar,
Probabilistic data structure-based community detection and storage scheme in online social networks,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 173-184,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.11.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X17310300)
Abstract: With the wide popularity of various social network sites such as — Facebook, Twitter, and Instagram; processing, storage, and analysis of a large volume of data are becoming challenging issues. In general, social networks are assumed to be graphs having nodes for representation of a group of persons in order to explore the relationship between them for Social Network Analysis (SNA). Analysts claim that interconnections in these networks are the reflection of social structure of individual where personalities with common attributes often occupy similar positions. Such similarities are caused by the prospects, opportunities, sensitivities and perceptions created by these similar network positions. Thus, clustering of these individuals is necessary to analyse their common characteristics. However, most of the existing clustering algorithms considered for community detection in SNA have high memory requirements especially in online social networks. So, to mitigate these issues, this paper proposes a novel clustering scheme for community detection for fast access, storage and retrieval of data using Probabilistic Data Structures (PDS). In the proposed scheme, Bloom filter has been used for clustering and Quotient filter has been used for storage and access of cluster nodes. It has been experimentally proved that the proposed scheme provides significant improvement in computational time which is reduced by 64% and 79% respectively in comparison to the linked list and adjacency matrix. Moreover, Quotient filter based storage schema significantly enhances the effectiveness of the proposed scheme over conventional storage methods.
Keywords: Social network analysis; Community detection; Graph storage; Probabilistic data structures; Bloom filter; Quotient filter

Kaylani Bochie, Mateus S. Gilbert, Luana Gantert, Mariana S.M. Barbosa, Dianne S.V. Medeiros, Miguel Elias M. Campista,
A survey on deep learning for challenged networks: Applications and trends,
Journal of Network and Computer Applications,
Volume 194,
2021,
103213,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103213.
(https://www.sciencedirect.com/science/article/pii/S1084804521002149)
Abstract: Computer networks are dealing with growing complexity, given the ever-increasing volume of data produced by all sorts of network nodes. Performance improvements are a non-stop ambition and require tuning fine-grained details of the system operation. Analyzing such data deluge, however, is not straightforward and sometimes not supported by the system. There are often problems regarding scalability and the predisposition of the involved nodes to understand and transfer the data. This issue is at least partially circumvented by knowledge acquisition from past experiences, which is a characteristic of the herein called “challenged networks”. The addition of intelligence in these scenarios is fundamental to extract linear and non-linear relationships from the data collected by multiple sources. This is undoubtedly an invitation to machine learning and, more particularly, to deep learning. This paper identifies five different challenged networks: IoT and sensor, mobile, industrial, and vehicular networks as typical scenarios that may have multiple and heterogeneous data sources and face obstacles concerning connectivity. As a consequence, deep learning solutions can contribute to system performance by adding intelligence and the ability to interpret data. We start the paper by providing an overview of deep learning, further explaining this approach’s benefits over the cited scenarios. We propose a workflow based on our observations of deep learning applications over challenged networks, and based on it, we strive to survey the literature on deep-learning-based solutions at an application-oriented level using the PRISMA methodology. Afterward, we also discuss new deep learning techniques that show enormous potential for further improvements as well as transversal issues, such as security. Finally, we provide lessons learned raising trends linking all surveyed papers to deep learning approaches. We are confident that the proposed paper contributes to the state of the art and can be a piece of inspiration for beginners and also for enthusiasts on advanced networking research.
Keywords: Challenged networks; Internet of Things; Sensor networks; Industrial networks; Wireless mobile networks; Vehicular networks; Deep learning; Machine learning

Rajanpreet Kaur Chahal, Neeraj Kumar, Shalini Batra,
Trust management in social Internet of Things: A taxonomy, open issues, and challenges,
Computer Communications,
Volume 150,
2020,
Pages 13-46,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.10.034.
(https://www.sciencedirect.com/science/article/pii/S0140366419310953)
Abstract: Internet of Things (IoT) is an emerging area in which billions of smart objects are interconnected with each other using Internet for data and resource sharing. These smart objects are generally used to sense various parameters such as temperature, motion of objects, and occupancy from the environment where these are deployed, the values of which are transmitted to the nearest access points to taken intelligent decisions. However, with an increased penetration of smart objects in our daily life, these objects may also participate in social events using machine-to-machine or human-to-machine interactions and are called as Social Objects. This new paradigm of interaction among social objects is referred to as Social IoT (SIoT). As these objects communicate with each other using an open channel, i.e., Internet, so security and privacy along with integrity of messages always remain a concern. Motivated from the aforementioend issues, in this paper, we provide an elaborated view of trust management among these objects with a focus on SIoT by comparing different existing trust management schemes based on the trust management process, parameters chosen for trust evaluation, characteristics of trust functions and objectives achieved by them. Moreover, we also discuss various challenges such as use of social behavior specific trust evaluation metrics and type of relationships between objects in SIoT with respect to trust management. Thus, the analysis provided in this paper provides insights to the readers for applicability of trust management scheme in SIoT environment keeping in of various challenges and constraints.
Keywords: Internet of Things; Social Internet of Things; Trust management; Trust Management Systems (TMS); Trust management taxonomy

Muhammad Imran, Hafeez Ur Rehman Siddiqui, Ali Raza, Muhammad Amjad Raza, Furqan Rustam, Imran Ashraf,
A performance overview of machine learning-based defense strategies for advanced persistent threats in industrial control systems,
Computers & Security,
Volume 134,
2023,
103445,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103445.
(https://www.sciencedirect.com/science/article/pii/S0167404823003553)
Abstract: Cybersecurity incident response is a very crucial part of the cybersecurity management system. Adversaries emerge and evolve with new cybersecurity tactics, techniques, and procedures (TTPs). It is essential to detect the TTPs in a timely manner to respond effectively and mitigate the vulnerabilities to secure business operations. This research focuses on TTP identification and detection based on a machine learning approach. Early identification and detection are paramount in protecting, responding to, and recovering from such adversarial attacks. Analyzing use cases is a critical tool to ensure proper and in-depth evaluation of sector-specific cybersecurity challenges. In this regard, this study investigates existing known methodologies for cyber-attacks such as Mitre attacks, and developed a method for identifying threat cases. In addition, Windows-based threat cases are implemented, comprehensive datasets are generated, and supervised machine learning models are applied to detect threats effectively and efficiently. Random forest outperforms other models with the highest accuracy of 99%. Future work can be done for generating threat cases based on multiple log sources, including network security and endpoint protection device, and achieve high accuracy by removing false positives using machine learning. Similarly, real-time threat detection is also envisioned for future work.
Keywords: Cybersecurity; Mitre attack; Advance persistent threats; Industrial control; Machine learning; Feature engineering

Devi Priya V S, Sibi Chakkaravarthy Sethuraman, Muhammad Khurram Khan,
Container security: Precaution levels, mitigation strategies, and research perspectives,
Computers & Security,
Volume 135,
2023,
103490,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103490.
(https://www.sciencedirect.com/science/article/pii/S0167404823004005)
Abstract: The enterprise technique for application deployment has undergone a major transformation during the past two decades. Using conventional techniques, software developers write code in a particular computing environment, frequently leading to mistakes and defects when moving it to a new computing environment. However, during the past few years, enterprises have begun to use containers & microservices to segregate infrastructure in a particular perspective and develop new models of the technology stack. Software developers could construct and deploy apps more quickly and effectively now, thanks to containerization. Despite the fact that containers have their own namespace, it is still feasible for a containerized image to attack the host system by inserting malicious software into it. This necessitates threat modeling of the container life span. During the investigation, we were able to create the elemental systematic modelling that identifies threats pertaining to container application workflow and its preliminary mitigation techniques, where attack trees are defined alongside the model, which helps academics and enthusiasts better comprehend the significance of container security. We utilize the well-known threat modeling framework, DREAD, to further advance threat modeling across the infrastructure of containers that aids in prioritizing the risks. Additionally, tools for assessing container vulnerabilities and discrete real-world exploits were researched, and approaches for security analysis in container technology were compared to the existing literature. Finally, this study brings to a conclusion by outlining the state-of-the-art survey for future research and identifying potential research topics in server-based and serverless containers.
Keywords: Microservices; Software development; Container security- root-based and rootless; Threat modeling-attack trees; DREAD

Francesco Casaril, Letterio Galletta,
Securing SatCom user segment: A study on cybersecurity challenges in view of IRIS2,
Computers & Security,
Volume 140,
2024,
103799,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2024.103799.
(https://www.sciencedirect.com/science/article/pii/S0167404824001007)
Abstract: The advancement in communications technologies and recent geopolitical events highlighted the need for fast and reliable satellite communications infrastructure for military and civil security operations. Starting from the case study of the Viasat cyberattack in February 2022, this paper analyzes the common vulnerabilities of the ground and, in particular, user segments in SatCom infrastructures, focusing on modems security, and proposes some best practices and solutions in the field of risk management to prevent such attacks. Moreover, the research compares the standards and the guidelines used in the United States concerning routers and network security with those in the European Union. Our findings highlight the need for clear and effective standards or certification schemes to cyber-proof the new components of IRIS2, the “Infrastructure for Resilience Interconnectivity and Security by Satellite”, Europe's first multi-orbital satellite constellation. This need becomes more compelling, especially in view of the entry into force of the Network and Information Security Directive or NIS2 Directive. We conclude by discussing future research directions and emerging trends in cyber risk management for the SatCom user segment. This paper aims to provide valuable insights into managing cyber risks in critical space infrastructure and can inform future efforts to improve cybersecurity in view of IRIS2.
Keywords: Satellite cybersecurity; SatCom; Space infrastructure; Risk management; Modem security

Shunliang Zhang, Dali Zhu, Yongming Wang,
A survey on space-aerial-terrestrial integrated 5G networks,
Computer Networks,
Volume 174,
2020,
107212,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107212.
(https://www.sciencedirect.com/science/article/pii/S1389128619314045)
Abstract: Thanks to their inherent advantages including large radio coverage and less dependence on terrestrial infrastructures, space and aerial networks can play an important role in 5G for ubiquitous coverage, large area broadcast services, and emergency communications. The space-aerial-terrestrial integrated networks (SATIN) are envisioned as an important trend of 5G evolution. However, due to distinct features including high altitude, moving platforms, large beam footprint and constrained payload, satellite networks are usually designed with unique protocols and functionalities, which hamper the integration of satellite networks with terrestrial mobile networks. The SATIN face many unprecedented technical challenges to achieve optimum performance, seamless user experience, and integrated security protection. This article makes a comprehensive overview of the latest researches from academia and industry to achieve the integrated 5G system. We first introduce the satellite and mobile technology trends, then present the vision of SATIN and related works in the area. Key enabling technologies from perspectives of the radio interface, networking, and security are discussed in depth. Then related standardization initiatives, especially the latest studies at 3GPP, and major industry research projects on non-terrestrial integrated 5G are reviewed. Important open issues are identified to inspire future studies for a fully integrated 5G system.

Penghao Sun, Julong Lan, Junfei Li, Zehua Guo, Yuxiang Hu, Tao Hu,
Efficient flow migration for NFV with Graph-aware deep reinforcement learning,
Computer Networks,
Volume 183,
2020,
107575,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107575.
(https://www.sciencedirect.com/science/article/pii/S1389128620312184)
Abstract: Network Function Virtualization (NFV) enables flexible deployment of network services as applications. However, it is a big challenge to guarantee the Quality of Service (QoS) under unpredictable network traffic while minimizing the processing resources. One typical solution is to realize NF scale-out, scale-in and load balancing by elastically migrating the related traffic flows. However, it is difficult to optimally migrate flows considering the resources and QoS constraints. In this paper, we propose DeepMigration to efficiently and dynamically migrate traffic flows among different NF instances. DeepMigration is a Deep Reinforcement Learning (DRL)-based solution coupled with Graph Neural Network (GNN). By taking advantages of the graph-based relationship deduction ability from our customized GNN and the self-evolution ability from the experience training of DRL, DeepMigration can accurately model the cost (e.g., migration latency) and the benefit (e.g., reducing the number of NF instances) of flow migration among different NF instances and employ dynamic and effective flow migration policies generated by the neural networks to improve the QoS. Experiment results show that DeepMigration reduces the migration latency and saves up to 71.6% of the computation time than the state-of-the-art.
Keywords: Network function virtualization; Flow migration; Deep reinforcement learning; Graph neural network

Bilal Afzal, Muhammad Umair, Ghalib Asadullah Shah, Ejaz Ahmed,
Enabling IoT platforms for social IoT applications: Vision, feature mapping, and challenges,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 718-731,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2017.12.002.
(https://www.sciencedirect.com/science/article/pii/S0167739X17312724)
Abstract: Social IoT (SIoT) is an emerging paradigm of IoT in which different IoT devices interact and establish relationships with each other to achieve a common goal. In essence, SIoT adapts a service-oriented architecture where heterogeneous IoT devices can offer or request autonomous services and collaborate on behalf of their owners. Operating Systems (OSs) are employed in IoT devices as they offer portability, threading support and access to development libraries; thus allowing easiness in IoT application development. Several OSs are available for IoT devices, but selecting an OS and hardware befitting for a particular IoT application is a critical task. In case of SIoT, the specific OS selection for hardware devices in various applications is even more challenging because of their collaborative nature. Existing surveys on OSs are mostly domain oriented and lack the discussion on hardware architectural features. As a consequence, it is infeasible for developers to choose best-suited OS for various hardware platforms which results in their underperformance in many application scenarios. This paper considers standard features of OS as well as hardware IoT platforms and provides an OS-to-hardware architectures features-mapping while exploring the unique requirements of SIoT applications. In doing so, resource-constrained IoT devices are particularly emphasized due to their memory constraints and power limitations. Further, a model OS architecture is proposed for devices in SIoT applications and associated open research challenges are identified. This research will benefit developers to best utilize IoT platform resources and to envisage an efficient OS for futuristic SIoT applications.
Keywords: Social IoT; Operating systems; Microcontroller architecture; Embedded systems; Resource-constrained devices

Muhammad Usman Younus, Saif ul Islam, Ihsan Ali, Suleman Khan, Muhammad Khurram Khan,
A survey on software defined networking enabled smart buildings: Architecture, challenges and use cases,
Journal of Network and Computer Applications,
Volume 137,
2019,
Pages 62-77,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.04.002.
(https://www.sciencedirect.com/science/article/pii/S1084804519301146)
Abstract: The rapid advancement of information and communication technologies (ICT) has drastically augmented the connectivity of actuators, computational elements and smart devices to the physical world. In this context, smart building (SB) serves as a significant domain to automatically handle and control the temperature, humidity, ventilation, safety, lighting, and other building's operations. Moreover, it plays a vital role in implementing the standards of enhanced living environments (ELE) and ambient assisted living (AAL). However, the extensive use of ICT and inflexible architecture of SB pose many challenges including heterogeneity of applications and Internet of things (IoT) devices, security, efficient networking architectures and protocols, energy efficiency, reliability and quality of service (QoS) provisioning. In this perspective, software defined networking (SDN) has gained great attention since it is being evolved as a programmable, and flexible networking framework. This paper is devoted to surveying the research work conducted on the integration of SDN to SB. Consequently, it provides a comprehensive review on SDN enabled SB, its architecture, taxonomy, communication protocols, and challenges for an immersive and interactive experience. It also presents a basic architecture of SDN over traditional networking. Moreover, SDN applications are also exploited to handle the issues of the wireless network. Furthermore, this paper demonstrates the use cases to explain the impact of SDN paradigm on the SBs.
Keywords: Smart building; SDN; Smart devices; Internet of Things

Nabajyoti Mazumdar, Saugata Roy, Amitava Nag, Sukumar Nandi,
An adaptive hierarchical data dissemination mechanism for mobile data collector enabled dynamic wireless sensor network,
Journal of Network and Computer Applications,
Volume 186,
2021,
103097,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103097.
(https://www.sciencedirect.com/science/article/pii/S1084804521001181)
Abstract: Recent studies have revealed that the energy hole problem caused by multi-hop data collection in a static data collector based wireless sensor network(WSN) is a major threat to the lifetime of the network. With the advancement of mobile terminal technology, the application of mobile data collectors (MDCs) has gained popularity in a large scale WSN. A WSN design with one or more MDC can significantly diminish the energy hole problem, which in turn prolongs the network operational lifetime. However, most of the MDC based existing schemes suffer from achieving a balance between routing energy consumption and data delivery delay. Moreover, the majority of the sensor network protocols fail to retain their impact as the network topology changes due to the inclusion or exclusion of nodes. So, for a dynamic WSN, it is essential to support a recovery mechanism to continue the data propagation despite inevitable changes in the WSN topology. Considering all the aforementioned challenges, this paper proposes an adaptive hierarchical data dissemination (AHDD) mechanism for multiple MDC enabled dynamic wireless sensor network, which alleviates the energy hole problem as well as resolves the fault-tolerance issue. Simulation results demonstrate that the proposed protocol has shown improved performance with respect to performance metrics namely, network lifetime, energy efficiency, packet delivery percentage, and end-to-end delay.
Keywords: Energy; Fault-tolerance; Mobile data collector; Routing; Wireless sensor network

Mikołaj Kowalski, Wojciech Mazurczyk,
Toward the mutual routing security in wide area networks: A scoping review of current threats and countermeasures,
Computer Networks,
Volume 230,
2023,
109778,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109778.
(https://www.sciencedirect.com/science/article/pii/S1389128623002232)
Abstract: Background:
The inter-domain routing security is often based on trust, which, as seen in practice, is an insufficient approach. Due to the deficit of native security controls in the Border Gateway Protocol (BGP), many new routing security measures were proposed to prevent control-plane abuse. They must be implemented in the majority of the Internet’s Autonomous Systems.
Objective:
This study undertakes a scoping review of the routing security domain to provide the most up-to-date and state-of-the-art broad summary of threat classification, prevention, and mitigation. The authors determine the progress of implementing countermeasures and explain the obstacles and research directions.
Results:
This paper covers the current threat landscape and the existing taxonomies of attack vectors on the routing layer. By analyzing different taxonomies, we detected overlapping incident types. Therefore, a unified and consolidated taxonomy is proposed to preserve consistency among different attack types, simultaneously giving a more detailed breakdown of incident classification. This review also contains a comprehensive comparative study of protective measures, including historical, current, and developing techniques. This study includes the efficiency of proactive (prevention) and reactive (mitigation) practices and their caveats. The authors also examine the most promising development plans for new and existing countermeasures.
Conclusion:
Global implementation efforts are focused on routing security’s safeguard mechanisms based on mutual protection, e.g., the Resource Public Key Infrastructure (RPKI) system. This determinant creates an infinite regress problem known as the chicken or the egg—the primary dilemma. The authors find that the point of critical mass is achieved but that RPKI still faces vital issues.
Keywords: Routing protocols; Network security; Telecommunication network reliability; Wide area networks

Azana Hafizah Mohd Aman, Wan Haslina Hassan, Shilan Sameen, Zainab Senan Attarbashi, Mojtaba Alizadeh, Liza Abdul Latiff,
IoMT amid COVID-19 pandemic: Application, architecture, technology, and security,
Journal of Network and Computer Applications,
Volume 174,
2021,
102886,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102886.
(https://www.sciencedirect.com/science/article/pii/S1084804520303490)
Abstract: In many countries, the Internet of Medical Things (IoMT) has been deployed in tandem with other strategies to curb the spread of COVID-19, improve the safety of front-line personnel, increase efficacy by lessening the severity of the disease on human lives, and decrease mortality rates. Significant inroads have been achieved in terms of applications and technology, as well as security which have also been magnified through the rapid and widespread adoption of IoMT across the globe. A number of on-going researches show the adoption of secure IoMT applications is possible by incorporating security measures with the technology. Furthermore, the development of new IoMT technologies merge with Artificial Intelligence, Big Data and Blockchain offers more viable solutions. Hence, this paper highlights the IoMT architecture, applications, technologies, and security developments that have been made with respect to IoMT in combating COVID-19. Additionally, this paper provides useful insights into specific IoMT architecture models, emerging IoMT applications, IoMT security measurements, and technology direction that apply to many IoMT systems within the medical environment to combat COVID-19.
Keywords: COVID-19 pandemic mitigation; IoMT application; IoMT architecture; IoMT security; IoMT technology

Shubhani Aggarwal, Rajat Chaudhary, Gagangeet Singh Aujla, Neeraj Kumar, Kim-Kwang Raymond Choo, Albert Y. Zomaya,
Blockchain for smart communities: Applications, challenges and opportunities,
Journal of Network and Computer Applications,
Volume 144,
2019,
Pages 13-48,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.06.018.
(https://www.sciencedirect.com/science/article/pii/S1084804519302231)
Abstract: Since the success of Bitcoin, there have been increased focus of studying the application of blockchain in a broad range of applications, such as in solutions facilitating identity privacy and transaction security using a decentralized architecture via different consensus mechanisms (e.g. proof-of-work) between different geo-located IoT devices/nodes in our increasingly digitalized society (e.g. smart city). In this paper, we survey the usage of blockchain technology for smart communities, focusing on key components of the blockchain applications. We also study the various process models used in the execution of secure transactions. Specifically, we present a detailed taxonomy on the applications, process models used, and communication infrastructure support needed to execute various applications.
Keywords: Blockchain; Communication infrastructure; Consensus mechanism; Distributed applications; Process models; Smart communities

A.S. Albahri, Jwan K. Alwan, Zahraa K. Taha, Sura F. Ismail, Rula A. Hamid, A.A. Zaidan, O.S. Albahri, B.B. Zaidan, A.H. Alamoodi, M.A. Alsalem,
IoT-based telemedicine for disease prevention and health promotion: State-of-the-Art,
Journal of Network and Computer Applications,
Volume 173,
2021,
102873,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102873.
(https://www.sciencedirect.com/science/article/pii/S1084804520303374)
Abstract: Numerous studies have focused on making telemedicine smart through the Internet of Things (IoT) technology. These works span a wide range of research areas to enhance telemedicine architecture such as network communications, artificial intelligence methods and techniques, IoT wearable sensors and hardware devices, smartphones and cloud computing. Accordingly, several telemedicine applications covering various human diseases have presented their works from a specific perspective and resulted in confusion regarding the IoT characteristics. Although such applications are useful and necessary for improving telemedicine contexts related to monitoring, detection and diagnostics, deriving an overall picture of how IoT characteristics are currently integrated with the telemedicine architecture is difficult. Accordingly, this study complements the academic literature with a systematic review covering all main aspects of advances in IoT-based telemedicine architecture. This study also provides a state-of-the-art telemedicine classification taxonomy under IoT and reviews works in different fields in relation to that classification. To this end, this study checked the ScienceDirect, Institute of Electrical and Electronics Engineers (IEEE) Xplore, and Web of Science databases. A total of 2121 papers were collected from 2014 to July 2020. The retrieved articles were filtered according to the defined inclusion criteria. A final set of 141 articles were selected and classified into two categories, each followed by subcategories and sections. The first category includes an IoT-based telemedicine network that accounts for 24.11% (n = 34/141). The second category includes IoT-based telemedicine healthcare services and applications that account for 75.89% (n = 107/141). This multi-field systematic review has exposed new research opportunities, motivations, recommendations and challenges that need attention for the synergistic integration of interdisciplinary works. This extensive study also lists a set of open issues and provides innovative key solutions along with a systematic review. The classification of diseases under IoT-based telemedicine is divided into 14 groups. Furthermore, the crossover in our taxonomy is demonstrated. The lifecycle of the context of IoT-based telemedicine healthcare applications is mapped for the first time, including the procedure sequencing and definition for each context. We believe that this study is a useful guide for researchers and practitioners in providing direction and valuable information for future research. This study can also address the ambiguity in the trends in IoT-based telemedicine.
Keywords: Telemedicine; Remote monitoring; Healthcare services; Diseases; Internet of things; Network

Qin Liu, Panlin Hou, Guojun Wang, Tao Peng, Shaobo Zhang,
Intelligent route planning on large road networks with efficiency and privacy,
Journal of Parallel and Distributed Computing,
Volume 133,
2019,
Pages 93-106,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.06.012.
(https://www.sciencedirect.com/science/article/pii/S0743731519300814)
Abstract: When using Location-Based Services (LBS), intelligent route planning becomes crucial to improve service quality and user experience. The state-of-the-art G-tree structure enables efficient route planning on large road networks, but lacks usability and intelligence. In this paper, we first propose a comprehensive service framework, called BCloud-IFog, which consists of blind cloud servers and intelligent fog servers. Then, we propose an Outsourced Real-time Route Planning (OR2P) scheme, where the search index is built as a G*-tree structure and each G*-tree leaf node is split into a set of non-confidential outsourced graphs. Compared with the G-tree structure, our work has the following advantages: (1) Higher usability. Unlike the G-tree structure requiring the user to perform all calculations in the search process, it delegates the most of computations to cloud servers. (2) Privacy Protection. Unlike the straightforward solution that directly outsource the G-tree structure, it outsources only the non-confidential graphs such that cloud servers cannot infer the original road network or user trajectory. (3) Better intelligence. Unlike the G-tree structure handles only static route planning, it allows fog servers to make route plans based on the dynamic real-time traffic status. Extensive experiments on real data sets demonstrate the effectiveness of our work.
Keywords: Route planning; Road networks; Privacy; Intelligence; Usability

Fabien Duchene, David Lebrun, Olivier Bonaventure,
SRv6Pipes: Enabling in-network bytestream functions,
Computer Communications,
Volume 145,
2019,
Pages 223-233,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2019.05.009.
(https://www.sciencedirect.com/science/article/pii/S0140366419304104)
Abstract: IPv6 Segment Routing is a recent IPv6 extension that is generating a lot of interest among researchers and in industry. Thanks to IPv6 SR, network operators can better control the paths followed by packets inside their networks. This provides enhanced traffic engineering capabilities and is key to support Service Function Chaining (SFC). With SFC, an end-to-end service is the composition of a series of in-network services. Simple services such as NAT, accounting or stateless firewalls can be implemented on a per-packet basis. However, more interesting services like transparent proxies, transparent compression or encryption, transcoding, etc. require functions that operate on the bytestream. In this paper, we extend the IPv6 implementation of Segment Routing in the Linux kernel to enable network functions that operate on the bytestream and not on a per-packet basis. Our SRv6Pipes enable network architects to design end-to-end services as a series of in-network functions. We evaluate the performance of our implementation with different microbenchmarks.

Alessio Netti, Zeynep Kiziltan, Ozalp Babaoglu, Alina Sîrbu, Andrea Bartolini, Andrea Borghesi,
A machine learning approach to online fault classification in HPC systems,
Future Generation Computer Systems,
Volume 110,
2020,
Pages 1009-1022,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.11.029.
(https://www.sciencedirect.com/science/article/pii/S0167739X1932045X)
Abstract: As High-Performance Computing (HPC) systems strive towards the exascale goal, failure rates both at the hardware and software levels will increase significantly. Thus, detecting and classifying faults in HPC systems as they occur and initiating corrective actions before they can transform into failures becomes essential for continued operation. Central to this objective is fault injection, which is the deliberate triggering of faults in a system so as to observe their behavior in a controlled environment. In this paper, we propose a fault classification method for HPC systems based on machine learning. The novelty of our approach rests with the fact that it can be operated on streamed data in an online manner, thus opening the possibility to devise and enact control actions on the target system in real-time. We introduce a high-level, easy-to-use fault injection tool called FINJ, with a focus on the management of complex experiments. In order to train and evaluate our machine learning classifiers, we inject faults to an in-house experimental HPC system using FINJ, and generate a fault dataset which we describe extensively. Both FINJ and the dataset are publicly available to facilitate resiliency research in the HPC systems field. Experimental results demonstrate that our approach allows almost perfect classification accuracy to be reached for different fault types with low computational overhead and minimal delay.
Keywords: High-performance computing; Exascale systems; Resiliency; Monitoring; Fault detection; Machine learning

Marco Polverini, Jaime Galán-Jiménez, Francesco G. Lavacca, Antonio Cianfrani, Vincenzo Eramo,
Improving dynamic service function chaining classification in NFV/SDN networks through the offloading concept,
Computer Networks,
Volume 182,
2020,
107480,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107480.
(https://www.sciencedirect.com/science/article/pii/S1389128620311543)
Abstract: Service Function Chaining (SFC) paradigm improves network capabilities thanks to the support of application-driven-networking, which is realized through the invocation of an ordered set of Service Functions (SFs). The programmability and flexibility provided by emerging technologies such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV) are perfect features for efficiently managing the lifecycle of SFCs. However, the limitation of Ternary Content Address Memories (TCAMs) of the SDN nodes in SDN-based SFC scenarios can lead to network performance degradation when the SFC classifier is not able to install new classification rules. To tackle the Dynamic Chain Request Classification Offloading (D-CRCO) problem presented in this paper, a hybrid eviction and split-and-distribute approach is proposed, where i) the dynamic behavior of SFC requests is exploited by removing the corresponding idle rules from the flow tables if necessary; and ii) SFC classification is not forced to be carried out by the ingress node, but by any transit node in the domain. An Integer Linear Programming (ILP) formulation and an heuristic are provided to solve D-CRCO, with the goal of maximizing the number of SFC requests that can be served, respecting TCAM size, link capacity, and SF availability constraints. Simulation and emulation results over two real topologies show that the proposed solution is able to significantly increase the number of served SFC requests with a negligible impact on the network performance.
Keywords: SDN; NFV; SFC; TCAM

Manasha Saqib, Ayaz Hassan Moon,
A Systematic Security Assessment and Review of Internet of Things in the Context of Authentication,
Computers & Security,
Volume 125,
2023,
103053,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2022.103053.
(https://www.sciencedirect.com/science/article/pii/S016740482200445X)
Abstract: The Internet of Things is emerging globally as an intriguing trend expected to connect 15 billion devices by the end of 2022. Its ability to bring intelligence and automation to various application domains provides a plethora of opportunities while posing severe security challenges. Lack of proper authentication has been attributed to data disclosure's perils over wireless communication channels. Therefore, authentication as an essential security tenet continues to be a highly researched area, especially in resource constraint networks like IoT and IIoT. This paper constructs a comprehensive systematic literature review to identify and synthesize security issues in IoT from the perspective of authentication mechanisms. Initially, the prevalent security and privacy issues are identified, followed by the explanation of security threats across various layers of the IoT architecture. Additionally, the countermeasures available for addressing security issues are also covered. The highlight of the review is to present a literature review of various authentication mechanisms and different formal security evaluations holistically required for IoT authentication. Moreover, a comparative analysis of some of the popular existing authentication mechanisms designed for the IoT in terms of various performance parameters like computational, communication overhead, and energy consumption has also been covered. Finally, the paper discusses the typical methods for assessing network security and network simulator tools used to evaluate the performance parameters of authentication schemes. This review paper attempts to assist researchers in identifying the existing research gaps in various forms of authentication employed in a typical resource constraint network like IoT that would lead them to develop new solutions. The protocol provided by Kitchenham and Charters has been used to perform this Systematic Literature Review.
Keywords: Internet of Things (IoT); Systematic literature review; Security; Authentication

Lucas B.D. Silveira, Henrique C. de Resende, Cristiano B. Both, Johann M. Marquez-Barja, Bruno Silvestre, Kleber V. Cardoso,
Tutorial on communication between access networks and the 5G core,
Computer Networks,
Volume 216,
2022,
109301,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109301.
(https://www.sciencedirect.com/science/article/pii/S1389128622003528)
Abstract: Fifth-generation (5G) networks enable a variety of use cases that require differentiated connectivity, e.g., Ultra-Reliable and Low-Latency Communications (URLLC), enhanced Mobile Broadband (eMBB), and massive Machine Type Communication (mMTC). To explore the full potential of these use cases, it is mandatory to understand the communication along with the 5G network segments and architecture components. User Equipment (UE), Radio Access Network (RAN), and 5G Core (5GC) are the main components that support these new network concepts and paradigms. 3rd Generation Partnership Project has recently published Release 16, including the protocols used to communicate between RANs and 5GC, i.e., Non-Access Stratum (NAS) and NG Application Protocol (NGAP). The main goal of this work is to present a comprehensive tutorial about NAS and NGAP specifications using a didactic and practical approach. The tutorial describes the protocol stacks and aspects of the functionality of these protocols in 5G networks, such as authentication and identification procedures, data session establishment, and resource allocation. Moreover, we review the message flows related to these protocols in UE and Next Generation Node B (gNodeB) registration. To illustrate the concepts presented in the tutorial, we developed the my5G Tester: a 5GC tester that implements NAS and NGAP for evaluating three open-source 5GC projects using a black-box testing methodology.
Keywords: NG-RAN; 5G core; NAS; NGAP

Muhammad Fahad Khan, Kok-Lim Alvin Yau, Rafidah MD. Noor, Muhammad Ali Imran,
Survey and taxonomy of clustering algorithms in 5G,
Journal of Network and Computer Applications,
Volume 154,
2020,
102539,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102539.
(https://www.sciencedirect.com/science/article/pii/S1084804520300138)
Abstract: The large-scale deployment of fifth generation (5G) is expected to produce a massive amount of data with high variability due to ultra-densification and the rapid increase in a heterogeneous range of applications and services (e.g., virtual reality, augmented reality, and driver-less vehicles), and network devices (e.g., smart gadgets and sensors). Clustering organizes network topology by segregating nodes with similar interests or behaviors in a network into logical groups in order to achieve network-level and cluster-level enhancements, particularly cluster stability, load balancing, social awareness, fairness, and quality of service. Clustering has been investigated to support mobile user equipment (UE) in access networks, whereby UEs form clusters themselves and may connect to BSs. In this paper, we present a comprehensive survey of the research work of clustering schemes proposed for various scenarios in 5G networks and highlight various aspects of clustering schemes, including objectives, challenges, metrics, characteristics, performance measures. Furthermore, we present open issues of clustering in 5G.
Keywords: 5G; Fifth generation; Cellular mobile communication; Clustering; Network topology

Muthukumar Murugan, Krishna Kant, Ajaykrishna Raghavan, David H.C. Du,
Software defined deduplicated replica management in scale-out storage systems,
Future Generation Computer Systems,
Volume 97,
2019,
Pages 340-354,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.02.018.
(https://www.sciencedirect.com/science/article/pii/S0167739X18323884)
Abstract: Given the unabated growth of storage in data centers, its energy footprint continues to grow, which makes storage energy management a crucial issue. Furthermore, with the emerging trend of underprovisioning of power and cooling infrastructures in large facilities, it is important to flexibly adapt the entire infrastructure of a client, including its storage system, to changing energy limitations. In this paper, we presentan energy-adaptive framework called flexStore that provides a flexible mechanism to specify and control the energy vs. performance tradeoffs. These mechanisms are defined and enforced by a software layer called “Policy Engine” that controls the number of active copies of deduplicated data chunks in storage containers based on energy availability. The mechanism synchronizes data chunks in inactive storage containers with those in the active containers, in the background to enable them to be put into service quickly when needed. We evaluate flexStore with different workloads on a sample data center environment and demonstrate the effectiveness of its control mechanisms in adapting to the performance and energy constraints.
Keywords: Software defined energy management; Adaptive deduplication; Energy adaptive storage; Flexible distributed storage

Mohamed Moulay, Rafael Garcia Leiva, Pablo J. Rojo Maroni, Fernando Diez, Vincenzo Mancuso, Antonio Fernández Anta,
Automated identification of network anomalies and their causes with interpretable machine learning: The CIAN methodology and TTrees implementation,
Computer Communications,
Volume 191,
2022,
Pages 327-348,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2022.05.013.
(https://www.sciencedirect.com/science/article/pii/S0140366422001657)
Abstract: Leveraging machine learning (ML) for the detection of network problems dates back to handling call-dropping issues in telephony. However, troubleshooting cellular networks is still a manual task, assigned to experts who monitor the network around the clock. To help in this task we present CIAN (from Causality Inference of Anomalies in Networks), a practical and interpretable ML methodology, which we implement in the form of a software tool named TTrees (from Troubleshooting Trees). We have designed CIAN to automate the identification of the causes of performance anomalies in cellular networks. Our methodology is unsupervised and combines multiple ML algorithms (e.g., decision trees and clustering) and Kolmogorov complexity-inspired data analysis tools that we have developed for this work. CIAN can be used with small volumes of data and is quick at training. Our experiments use diverse data sets obtained from measurements in operational commercial mobile networks. They show that the TTrees implementation of CIAN can automatically identify and accurately classify network anomalies – e.g., cases for which a network low performance is not apparently justified by operational conditions – training with just a few hundreds of data samples. The resulting information hence enables precise troubleshooting actions. In particular, we showcase how TTrees can be flexibly used to monitor the performance of TCP and QUIC protocols when they are adopted to serve mobile users.
Keywords: Troubleshooting; Anomaly detection; Feature selection; Interpretable machine learning

Arthur de M. Del Esposte, Eduardo F.Z. Santana, Lucas Kanashiro, Fabio M. Costa, Kelly R. Braghetto, Nelson Lago, Fabio Kon,
Design and evaluation of a scalable smart city software platform with large-scale simulations,
Future Generation Computer Systems,
Volume 93,
2019,
Pages 427-441,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.10.026.
(https://www.sciencedirect.com/science/article/pii/S0167739X18307301)
Abstract: Smart Cities combine advances in Internet of Things, Big Data, Social Networks, and Cloud Computing technologies with the demand for cyber–physical applications in areas of public interest, such as Health, Public Safety, and Mobility. The end goal is to leverage the use of city resources to improve the quality of life of its citizens. Achieving this goal, however, requires advanced support for the development and operation of applications in a complex and dynamic environment. Middleware platforms can provide an integrated infrastructure that enables solutions for smart cities by combining heterogeneous city devices and providing unified, high-level facilities for the development of applications and services. Although several smart city platforms have been proposed in the literature, there are still open research and development challenges related to their scalability, maintainability, interoperability, and reuse in the context of different cities, to name a few. Moreover, available platforms lack extensive scientific validation, which hinders a comparative analysis of their applicability. Aiming to close this gap, we propose InterSCity, a microservices-based, open-source, smart city platform that enables the collaborative development of large-scale systems, applications, and services for the cities of the future, contributing to turn them into truly smart cyber–physical environments. In this paper, we present the architecture of the InterSCity platform, followed by a comprehensive set of experiments that evaluate its scalability. The experiments were conducted using a smart city simulator to generate realistic workloads used to assess the platform in extreme conditions. The experimental results demonstrate that the platform can scale horizontally to handle the highly dynamic demands of a large smart city while maintaining low response times. The experiments also show the effectiveness of the technique used to generate synthetic workloads.
Keywords: Smart cities; Smart urban spaces; Middleware; Simulation; Scalability; Open source; Microservices

Md Jalil Piran, Quoc-Viet Pham, S.M. Riazul Islam, Sukhee Cho, Byungjun Bae, Doug Young Suh, Zhu Han,
Multimedia communication over cognitive radio networks from QoS/QoE perspective: A comprehensive survey,
Journal of Network and Computer Applications,
Volume 172,
2020,
102759,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102759.
(https://www.sciencedirect.com/science/article/pii/S1084804520302332)
Abstract: The stringent requirements of wireless multimedia transmission lead to very high radio spectrum solicitation. Although the radio spectrum is considered as a scarce resource, the issue with spectrum availability is not scarcity, but the inefficient utilization. Unique characteristics of cognitive radio (CR) such as flexibility, adaptability, and interoperability, particularly have contributed to it being the optimum technological candidate to alleviate the issue of spectrum scarcity for multimedia communications. However, multimedia communications over CR networks (MCRNs) as a bandwidth-hungry, delay-sensitive, and loss-tolerant service, exposes several severe challenges specially to guarantee quality of service (QoS) and quality of experience (QoE). As a result, to date, different schemes based on source and channel coding, multicast, and distributed streaming, have been examined to improve the QoS/QoE in MCRNs. In this paper, we survey QoS/QoE provisioning schemes in MCRNs. We first discuss the basic concepts of multimedia communication, CRNs, QoS and QoE. Then, we present the advantages of utilizing CR for multimedia services and outline the stringent QoS and QoE requirements in MCRNs. Next, we classify the critical challenges for QoS/QoE provisioning in MCRNs including spectrum sensing, resource allocation management, network fluctuations management, latency management, and energy consumption management. Then, we survey the corresponding feasible solutions for each challenge highlighting performance issues, strengths, and weaknesses. Furthermore, we discuss several important open research problems and provide some avenues for future research.
Keywords: Cognitive radio networks; Multimedia communication; Spectrum sensing; Resource allocation management; Network fluctuation management; Delay; Energy efficiency; Quality of service (QoS); Quality of experience (QoE); Machine learning; Game theory

Saniya Zafar, Rasheed Hussain, Fatima Hussain, Sobia Jangsher,
Interplay between Big Spectrum Data and Mobile Internet of Things: Current solutions and future challenges,
Computer Networks,
Volume 163,
2019,
106879,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106879.
(https://www.sciencedirect.com/science/article/pii/S1389128619307376)
Abstract: In the current epoch of technology trends, Internet of Things (IoT) is an emerging technology that illustrates the significant transition for enterprises to extend their businesses. Such technologies not only spark the growth in business, but also add values to our daily lives in many aspects such as livelihood, health, work, and entertainment. Nowadays, with the emergence of wireless infrastructure and the advancements in cellular technologies, Mobile IoT (MIoT) has gained considerable amount of attention. Intelligent decisions are made based on the information retrieved from raw data through sensors, machines, and other devices. These decisions are widely utilized in many fields of life such as smart health-care, smart transportation, and smart energy etc. Usually such smart systems are realized through MIoT infrastructure. The devices participating in MIoT are heterogeneous and produce huge amount of data, and contribute to cross-platform solutions covering plethora of applications. In the wake of such volume and variety, the data produced because of spectrum analysis for the entire IoT network is also huge and thus referred to as Big Spectrum Data (BSD). To this end, BSD is a way forward to get deep insights to the big data generated by MIoT. In this paper, we conduct a systematic review of the MIoT network focusing on the characteristics of mobility in MIoT from BSD perspective. More precisely, we focus on the relationship between MIoT and BSD, and discuss in detail, the BSD analytics to harness its benefits in MIoT. Furthermore, we investigate and outline the current solutions of BSD in MIoT. Additionally, through this work, we identify the existing challenges faced in dealing with BSD in MIoT and future research directions.
Keywords: Big Spectrum Data (BSD); BSD analytics; Internet of Things (IoT); Mobile Internet of Things (MIoT)

Ebrahim Zarei Zefreh, Shahriar Lotfi, Leyli Mohammad Khanli, Jaber Karimpour,
Topology and computational-power aware tile mapping of perfectly nested loops with dependencies on distributed systems,
Journal of Parallel and Distributed Computing,
Volume 129,
2019,
Pages 14-35,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2019.03.002.
(https://www.sciencedirect.com/science/article/pii/S0743731519301753)
Abstract: Nested loops are main source of the parallelism in many scientific applications. Partitioning the iteration space of nested loops with data dependencies into tiles and assigning them to processing nodes for parallel execution is essential for achieving high performance. Although most of the previous work focused on tiling on fully connected homogeneous distributed systems, some studies have been devoted to tiling on partially connected distributed systems. In this paper, we address the parallelization of perfectly nested loops with dependencies on partially connected heterogeneous distributed systems and present a topology and computational-power aware tile mapping. This work aims to take into account not only the node’s computational power when tiling iteration space of nested loops but also the exploitation of the network topology when mapping tiles to processing nodes. This approach allows minimizing the parallel execution time by improving the load balancing and minimizing the communication costs. We demonstrate the performance of proposed method by comparing it with the computational-power aware tile mapping and the topology aware tile mapping. The experimental results show that the proposed method improves the parallel execution time by up to 62% and 28% compared with the computational-power aware tile mapping and the topology aware tile mapping, respectively.
Keywords: Nested loop; Parallelization; Computational-power aware tile mapping; Topology aware tile mapping; Distributed systems

Luigi Bellomarini, Ruslan R. Fayzrakhmanov, Georg Gottlob, Andrey Kravchenko, Eleonora Laurenza, Yavor Nenov, Stéphane Reissfelder, Emanuel Sallinger, Evgeny Sherkhonov, Sahar Vahdati, Lianlong Wu,
Data science with Vadalog: Knowledge Graphs with machine learning and reasoning in practice,
Future Generation Computer Systems,
Volume 129,
2022,
Pages 407-422,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.10.021.
(https://www.sciencedirect.com/science/article/pii/S0167739X21004179)
Abstract: Following the recent successful examples of large technology companies, many modern enterprises seek to build Knowledge Graphs to provide a unified view of corporate knowledge, and to draw deep insights using machine learning and logical reasoning. There is currently a perceived disconnect between the traditional approaches for data science, typically based on machine learning and statistical modeling, and systems for reasoning with domain knowledge. In this paper, we demonstrate how to perform a broad spectrum of data science tasks in a unified Knowledge Graph environment. This includes data wrangling, complex logical and probabilistic reasoning, and machine learning. We base our work on the state-of-the-art Knowledge Graph Management System Vadalog, which delivers highly expressive and efficient logical reasoning and provides seamless integration with modern data science toolkits such as the Jupyter platform. We argue that this is a significant step forward towards practical, holistic data science workflows that combine machine learning and reasoning in data science.
Keywords: Knowledge Graphs; Data science; Machine learning; Reasoning; Probabilistic reasoning

Jianbang Dai, Xiaolong Xu, Fu Xiao,
GLADS: A global-local attention data selection model for multimodal multitask encrypted traffic classification of IoT,
Computer Networks,
Volume 225,
2023,
109652,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2023.109652.
(https://www.sciencedirect.com/science/article/pii/S138912862300097X)
Abstract: With the rapid development of the Internet of Things (IoT), numerous of IoT devices and different characteristics in IoT traffic patterns need traffic classification to enable many important applications. Deep-learning-based (DL-based) traffic methods have gained increasing attention due to their high accuracy and because manual feature extraction is not needed. Furthermore, seek a lightweight, multitask methods that supports a “performance-speed” trade-off. Thus, we proposed the 0.11 M global-local attention data selection (GLADS) model. The core of the GLADS model includes an “indicator” mechanism and a “local + global” framework. The “indicator” mechanism is a completely different method for handling multimodal input that allows the model to efficiently extract features from multimodal input with a single-modal-like approach. The “local + global” framework for the “performance-speed” trade-off includes a “local” part to obtain the features of each patch in the model input and a Global-Local Attention mechanism in the “global” part outputs the classification results under all possible lengths. Tests on the ISCX-VPN-2016, ISCX-Tor-2016, USTC-TFC-2016, and TON_IoT datasets show that GLADS achieves better performance than several state-of-the-art baselines, ranging from 2.42% to 7.76%. Furthermore, we also propose the “indicator,” which allows the model to simply cope with multimodal input. Based on global-local attention, we analyze the relation of the input section and model performance in detail.
Keywords: Traffic classification; Encrypt traffic; Deep learning; Multimodal; Multitasks; Information fusion

Anwitaman Datta, Frédérique Oggier,
Quorums over codes,
Journal of Parallel and Distributed Computing,
Volume 161,
2022,
Pages 1-19,
ISSN 0743-7315,
https://doi.org/10.1016/j.jpdc.2021.11.002.
(https://www.sciencedirect.com/science/article/pii/S0743731521002094)
Abstract: We consider the design and analysis of quorum systems over erasure coded warm data (with low frequency of writes and accesses in general) to guarantee sequential consistency under a fail-stop model while supporting atomic read-modify-write operations by multiple clients. We propose a definition of asymmetric quorum systems that suit the framework of coded data by explicitly exploiting the structural properties of code and instantiate it over distinct families of coding strategies: maximum distance separable (MDS) codes and codes with locality, and we indicate a mechanism for synchronizing stale nodes using differential updates, which again exploits the code structures. The proposed quorum system's behavior is analyzed theoretically, exploring several aspects: viability of quorums under node unavailability; contention of resources between read and write operations; and quorum load. We complement these theoretical exploration with simulation based experiments to quantify the behavior of the proposed mechanism. The overall study demonstrates the feasibility and practicality of quorums over codes under practicable assumptions for achieving a stringent form of consistency, specifically, sequential consistency, while the stored data is being mutated by potentially multiple processes that might read and then modify the existing data. We achieve this in-place, without having to resort to store multiple versions of the data.
Keywords: Erasure codes; Quorums; Read-modify-write; Sequential consistency

Marco Casazza, Mathieu Bouet, Stefano Secci,
Availability-driven NFV orchestration,
Computer Networks,
Volume 155,
2019,
Pages 47-61,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.02.017.
(https://www.sciencedirect.com/science/article/pii/S138912861930235X)
Abstract: Virtual Network Functions as a Service (VNFaaS) is a promising business whose technical directions consist of providing network functions as a Service instead of delivering standalone network appliances, leveraging a virtualized environment named NFV Infrastructure (NFVI) to provide higher scalability and reduce maintenance costs. Operating the NFVI under stringent availability guarantees is fundamental to ensure the proper functioning of the VNFaaS against software attacks and failures, as well as common physical device failures. Indeed the availability of a VNFaaS relies on the failure rate of its single components, namely the physical servers, the hypervisor, the VNF software, and the communication network. In this paper, we propose a versatile orchestration model able to integrate an elastic VNF protection strategy with the goal to maximize the availability of an NFVI system serving multiple VNF demands. The elasticity derives from (i) the ability to use VNF protection only if needed, or (ii) to pass from dedicated protection scheme to shared VNF protection scheme when needed for a subset of the VNFs, (iii) to integrate traffic split and load-balancing as well as mastership role election in the orchestration decision, (iv) to adjust the placement of VNF masters and slaves based on the availability of the different system and network components involved. We propose a VNF orchestration algorithm based on Variable Neighboring Search, able to integrate both protection schemes in a scalable way and capable to scale, while outperforming standard online policies.
Keywords: High Availability NFV; Virtual network functions; NFV Orchestration; Variable neighborhood search; Greedy heuristic

Zhuhua Liao, Zengde Teng, Jian Zhang, Yizhi Liu, Hao Xiao, Aiping Yi,
A Semantic Concast service for data discovery, aggregation and processing on NDN,
Journal of Network and Computer Applications,
Volume 125,
2019,
Pages 168-178,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2018.10.017.
(https://www.sciencedirect.com/science/article/pii/S1084804518303394)
Abstract: Offering a flexible paradigm for intelligently discovering, aggregating and processing big distributed data is a crucial requirement in large content-centric Internet. However, the major hindrances to this paradigm are network's dynamic feature, traffic balance, wired forwarding and the absence of cooperation between communications and computations. In this paper, we present a scalable Semantic Concast service on Named Data Networking (NDN) being considered as a promising paradigm for the future Internet. The service enables cooperation between data discovering, aggregating and processing among intermediate nodes for a user's Interest that contained a hierarchical name and semantic constraints. Specifically, multiple types and strategies of data aggregation and processing for combining and processing the positive data and suppressing the negative, futile data, as well as a determination of response completeness are introduced for enhancing relevant results recall and sharing. The experimentation demonstrated the Semantic Concast service can effectively improve service quality, reduce network traffic and shorten response time.
Keywords: Named Data Networking; Semantic Concast; Data discovery; Data aggregation; Distributed processing

Muhammad Imran, Muhammad Hanif Durad, Farrukh Aslam Khan, Abdelouahid Derhab,
Toward an optimal solution against Denial of Service attacks in Software Defined Networks,
Future Generation Computer Systems,
Volume 92,
2019,
Pages 444-453,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2018.09.022.
(https://www.sciencedirect.com/science/article/pii/S0167739X18302930)
Abstract: Software Defined Networking (SDN) separates the control logic from data forwarding and shifts the whole decision power to the controller, making the switch a dumb device. SDNs are becoming more and more important due to the key features like scalability, flexibility and monitoring. The centralized control of SDN makes it vulnerable to different attacks such as Flooding, Spoofing, Denial of Service (DoS), etc. These attacks can degrade the SDN performance by overwhelming its different components such as controller, switch and control channel. This paper provides a comprehensive review of different mitigation approaches and categorizes them into three different classes on the basis of their methodology to handle the malicious traffic. In addition to that, we find out limitations in these mitigation approaches and propose the possible features of an optimal solution against DoS attacks. To the best of our knowledge, this work is the first attempt toward classifying DoS mitigation strategies and finding out their limitations in the SDN environment.
Keywords: Software Defined Networking (SDN); Denial of Service (DoS) attack; DoS mitigation strategies; SDN security analysis; Classification of DoS mitigation techniques

Surbhi Saraswat, Vishal Agarwal, Hari Prabhat Gupta, Rahul Mishra, Ashish Gupta, Tanima Dutta,
Challenges and solutions in Software Defined Networking: A survey,
Journal of Network and Computer Applications,
Volume 141,
2019,
Pages 23-58,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.04.020.
(https://www.sciencedirect.com/science/article/pii/S1084804519301444)
Abstract: Software Defined Network (SDN) has become one of the most preferred solutions for the management of large-scale complex networks. The network policies in the large-scale network are difficult to embed on entire network devices simultaneously, whereas in SDN these policies can be embedded on the top of the network. The SDN network is divided into two parts which are vertically integrated to form the entire network. The policies and control mechanism are handled by the top layer called the control plane, whereas the data forwarding is performed at the bottom layer called the data plane. Thus the SDN forms a logically centralized network. In this paper, we present a comprehensive survey on the various issues, challenges, and solutions in the designing, implementation, performance, and verification of SDN. The network design emphasized on scalability, fault-tolerance, flexibility, and elasticity. Further implementation issues discuss resource management and virtualization. Similarly, latency, efficiency, and consistency discuss under-performance measure. Lastly, we focus on security and debugging in verification of SDN. We elaborate all the above issues and further discuss the different solutions that exist and discussed the future research trends in SDN.
Keywords: Design issues; Research challenges; Software defined network; Solutions

Feras M. Awaysheh, Mamoun Alazab, Maanak Gupta, Tomás F. Pena, José C. Cabaleiro,
Next-generation big data federation access control: A reference model,
Future Generation Computer Systems,
Volume 108,
2020,
Pages 726-741,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.02.052.
(https://www.sciencedirect.com/science/article/pii/S0167739X19315948)
Abstract: This paper discusses one of the most significant challenges of next-generation big data (BD) federation platforms, namely, Hadoop access control. Privacy and security on a federation scale remain significant concerns among practitioners in both industry and academia. Hadoop’s current primitive access control presents security concerns and limitations, such as the complexity of deployment and the consumption of resources. However, this major concern has not been a subject of intensive study in the literature. This paper critically reviews and investigates these security limitations and provides a framework called BD federation access broker to address 8 main security limitations. This paper proposes the federated access control reference model (FACRM) to formalize the design of secure BD solutions within the Apache Hadoop stack. Furthermore, this paper discusses the implementation of the access broker and its usefulness for security breach detection and digital forensics investigations. The efficiency of the proposed access broker has not sustainably affected the performance overhead. The experimental results show only 1% of each 100 MB read/write operation in a WebHDFS. Overall, the findings of the paper pave the way for a wide range of revolutionary and state-of-the-art enhancements and future trends within Hadoop stack security and privacy.
Keywords: Big data; Hadoop 3.x; Identification and access management; HDFS federation; Reference model; Security broker; Access logs analysis

Ali Parsa, Neda Moghim, Pouyan Salavati,
Joint power allocation and MCS selection for energy-efficient link adaptation: A deep reinforcement learning approach,
Computer Networks,
Volume 218,
2022,
109386,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2022.109386.
(https://www.sciencedirect.com/science/article/pii/S1389128622004200)
Abstract: Link adaptation is a promising tool of modern networks to combat the time-variant quality of channels. Modulation and Coding Scheme (MCS) selection is essentially used for link adaptation with channel dynamism. However, future generation networks need flexible link adaptation schemes that consider more parameters to improve the network performance. This paper proposes an energy-efficient link adaptation algorithm, in which a Deep Reinforcement Learning (DRL) agent is used to find the best match between the channel condition and the link parameters. Also, the downlink transmission power has been considered as a link parameter in addition to the modulation order and coding rate to make the link adaptation more flexible and efficient. Simulation results show that the proposed algorithm outperforms the benchmark algorithms regarding energy efficiency and link throughput.
Keywords: Link adaptation; Modulation and coding scheme; Energy efficiency; Deep reinforcement learning

Martina Šestak, Marjan Heričko, Tatjana Welzer Družovec, Muhamed Turkanović,
Applying k-vertex cardinality constraints on a Neo4j graph database,
Future Generation Computer Systems,
Volume 115,
2021,
Pages 459-474,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.09.036.
(https://www.sciencedirect.com/science/article/pii/S0167739X19324094)
Abstract: As with any other database solution, graph databases also need to be able to implement business rules related to a given application domain. At the moment, aside from integrity constraints, there is a limited number of mechanisms for business rules implementation in Graph Database Management Systems (GDBMSs). The underlying property graph data model does not include any formal notation on how to represent different constraints. Specifically, this paper discusses the problem of representing cardinality constraints in graph databases. We introduce the novel concept of k-vertex cardinality constraints, which enable us to specify the minimum and maximum number of edges between a vertex and a subgraph. We also propose an approach, which includes the representation of cardinality constraints through the property graph data model, and demonstrate its implementation through a series of stored procedures in Neo4j GDBMS. The proposed approach is then evaluated by performing experiments on synthetic and real datasets to test the influence of checking cardinality constraints on query execution times (QETs) when adding new edges. Additionally, a comparison is performed on synthetic datasets with varying outgoing vertex degrees in order to gain an insight into how increasing the vertex degree affects QETs. In general, the results obtained for each test scenario show that the implemented k-vertex cardinality constraints model does not significantly affect QETs. Also, the results indicate that the model is dependent on the order of the underlying k-vertex cardinality constraints and outgoing vertex degree in the dataset.
Keywords: k-vertex cardinality constraints; Graph databases; Property graph data model; Procedures; Business rules; Graph schema

Tao Hu, Peng Yi, Julong Lan, Yuxiang Hu, Penghao Sun,
ACST: Audit-based compromised switch tolerance for enhancing data plane robustness in software-defined networking,
Computer Networks,
Volume 161,
2019,
Pages 264-280,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.06.013.
(https://www.sciencedirect.com/science/article/pii/S1389128619300180)
Abstract: Software-defined networking has stimulated the worldwide interests in both academia and industry for its proven advantages. However, switches in data plane are more vulnerable due to malicious attacks. Consequently, the network may consist of a switch misguiding phenomenon: the switches are compromised by the attackers and send the faked statistics while interacting with the controller to mislead the control decision (e.g., optimal routing). In this paper, we introduce an audit-based compromised switch tolerance (ACST) scheme, which aims at tolerating compromised switches and dealing with switch misguiding phenomenon when switches are trustless. Our main idea is to audit the statistics (specifically, state messages) delivered by switches not only to make the controller receive the correct messages but also to identify the compromised switches. Following this idea, we first investigate the switch misguiding phenomenon. Then, we design ACST to ensure that the controller gets the correct state messages even if the compromised switches exist. ACST introduces a special logic plane called fault tolerance proxy plane between data plane and control plane. Each proxy consists of specific function modules, which are used for extracting original state messages and performing statistics auditing. Finally, the proxies output the auditing results, including corrected state messages and the compromised switch IDs. The corresponding algorithm and theoretical proof of its robustness enhancement are also presented. Results show our proposal can successfully resist different manipulating attacks launched by the compromised switches and guarantee a high correctness rate of state messages (approaching 100%). Besides, ACST shows good topological adaptability and produces low overheads.
Keywords: Software-defined networking (SDN); Data plane; Compromised switch; Fault tolerance; Robustness

Mirko D’Angelo, Mauro Caporuscio, Vincenzo Grassi, Raffaela Mirandola,
Decentralized learning for self-adaptive QoS-aware service assembly,
Future Generation Computer Systems,
Volume 108,
2020,
Pages 210-227,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.02.027.
(https://www.sciencedirect.com/science/article/pii/S0167739X19312439)
Abstract: The highly dynamic nature of future computing systems, where applications dynamically emerge as opportunistic aggregation of autonomous and independent resources available at any given time, requires a radical shift in the adopted computing paradigms. Indeed, they should fully reflect the decentralized perspective of the execution environment and consider QoS, scalability and resilience as key objectives. In this context, the everything-as-a-service (XaaS) paradigm, which envisions the creation of new services as an assembly of independent services available within the environment, can greatly help in tackling the challenges of developing future applications. However, in order to be effective, XaaS paradigm requires self-adaptive service assembly solutions able to cope with the unpredictable variability and scalability of the execution environment, the lack of global knowledge, and the QoS requirements of services to be built. We contribute in this direction by designing a fully decentralized and collective self-adaptive service assembly framework whose main features are: (i) self-assembly, i.e., the ability to operate autonomously, (ii) online-learning, i.e., the ability to dynamically learn from experience, (iii) QoS-awareness, i.e., the inclusion of QoS requirements as driving forces for self-assembly, (iv) scalability, i.e., the ability to cope with a large number of services, and (v) resilience, i.e., the ability to maintain the persistence of service delivery when facing unexpected changes (e.g., in the number and/or QoS of services). Simulation experiments show that our solution makes the system able to quickly converge to viable assemblies that improve and maintain over time the social welfare of the system, despite the local perspective of each participating service.
Keywords: Service assembly; Quality of service; Decentralized learning; Self-adaptive systems

Yerkezhan Sartayeva, Henry C. B. Chan,
A survey on indoor positioning security and privacy,
Computers & Security,
Volume 131,
2023,
103293,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103293.
(https://www.sciencedirect.com/science/article/pii/S0167404823002031)
Abstract: With rising demand for indoor location-based services (LBS) such as location-based marketing, mobile navigation, etc., there has been considerable interest in indoor positioning methods as well as their security and privacy. Current survey papers on indoor positioning methods mainly focus on positioning accuracy, whereas discussion on security and privacy considerations is limited. While there are survey papers on the security/privacy of LBS, they mainly focus on the services rather than the positioning methods. On the other hand, various survey papers on Internet of Things security/privacy mostly address device and system security. To fill the gap and complement the aforementioned survey papers, we conduct a systematic and comprehensive survey on indoor positioning security and privacy, focusing on the positioning methods. In particular, we provide the following contributions. First, based on general search (using the systematic PRISMA approach) and specific search, we study related papers published in recent years with the aim of addressing three research questions. Second, to facilitate the survey and study, we categorise the positioning methods into non-collaborative methods (i.e., proximity-based, geometric and fingerprinting methods), collaborative methods (i.e., mobile proximity-based and mobile geometric methods) and others (combining multiple technologies/methods). Third, for each method, we give an overview of the method and discuss its security and privacy issues. Last but not least, we highlight some future research directions and work on indoor positioning security and privacy. In particular, there is a need to conduct more research on collaborative positioning methods, including their security and privacy issues.
Keywords: Indoor positioning security; Indoor positioning privacy; Location-based services; Collaborative positioning; Non-collaborative positioning; Wireless networks

Samira Afzal, Vanessa Testoni, Christian Esteve Rothenberg, Prakash Kolan, Imed Bouazizi,
A holistic survey of multipath wireless video streaming,
Journal of Network and Computer Applications,
Volume 212,
2023,
103581,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103581.
(https://www.sciencedirect.com/science/article/pii/S1084804522002223)
Abstract: Demand for wireless video streaming services increases with users expecting to access high-quality video streaming experiences. Ensuring Quality of Experience (QoE) is quite challenging due to varying bandwidth and time constraints. Since most of today’s mobile devices are equipped with multiple network interfaces, one promising approach is to benefit from multipath communications. Multipathing leads to higher aggregate bandwidth and distributing video traffic over multiple network paths improves stability, seamless connectivity, and QoE. However, most of current transport protocols do not match the requirements of video streaming applications or are not designed to address relevant issues, such as networks heterogeneity, head-of-line blocking, and delay constraints. In this comprehensive survey, we first review video streaming standards and technology developments. We then discuss the benefits and challenges of multipath video transmission over wireless. We provide a holistic literature review of multipath wireless video streaming, shedding light on the different alternatives from an end-to-end layered stack perspective, reviewing key multipath wireless scheduling functions, unveiling trade-offs of each approach, and presenting a suitable taxonomy to classify the state-of-the-art. Finally, we discuss open issues and avenues for future work.
Keywords: Wireless video streaming; Multipath routing; Packet scheduling; Heterogeneous networks

German Peinado Gomez, Jordi Mongay Batalla, Yoan Miche, Silke Holtmanns, Constandinos X. Mavromoustakis, George Mastorakis, Noman Haider,
Security policies definition and enforcement utilizing policy control function framework in 5G,
Computer Communications,
Volume 172,
2021,
Pages 226-237,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2021.03.024.
(https://www.sciencedirect.com/science/article/pii/S0140366421001262)
Abstract: This research analyses new approaches to security enforcement in fifth generation (5G) architecture from end to end perspective. With the aim of finding a suitable and effective unified schema across the different network domains, it shows that policy control framework may become the cornerstone for the definition and enforcement of security policies in new 5G networks. The 5G core network architecture reference model is defined as a Service Based Architecture (SBA). The Policy Control Function (PCF) is a Network Function (NF) that constitutes, within the SBA architecture, a unique framework for defining any type of policies in the network and delivering those to other control plane NFs. In previous generations the policy control approach has been restricted to Quality of Service (QoS) and charging aspects. In contrast, the 5G system is now based on a unified policy control scheme that allows to build consistent policies covering the entire network. By utilizing the unified 5G policy framework we have found an effective security enforcement schema flexible to create new security policies, and agile to react to the constantly changing environment, across the end to end architecture. Within this schema we have defined mechanisms to apply the QoS principles to security use cases. We have also set up the user plane security enforcement within the session management and established security policies. Finally we have made proposals to extend the network analytics to security analytics. Our overall vision is to consider security as a quality element of the network.
Keywords: 5G; Security; Security enforcement; Quality of Service; Policy control; 3GPP; Service Based Architecture; Security policies; Security analytics; Security assurance; Slicing

Lewis Nkenyereye, S.M. Riazul Islam, Muhammad Bilal, M. Abdullah-Al-Wadud, Atif Alamri, Anand Nayyar,
Secure crowd-sensing protocol for fog-based vehicular cloud,
Future Generation Computer Systems,
Volume 120,
2021,
Pages 61-75,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2021.02.008.
(https://www.sciencedirect.com/science/article/pii/S0167739X21000601)
Abstract: The new paradigm of fog computing was extended from conventional cloud computing to provide computing and storage capabilities at the edge of the network. Applied to vehicular networks, fog-enabled vehicular computing is expected to become a core feature that can accelerate a multitude of services including crowd-sensing. Accordingly, the security and privacy of vehicles joining the crowd-sensing system have become important issues for cyber defense and smart policing. In addition, to satisfy the demand of crowd-sensing data users, fine-grained access control is required. In this paper, we propose a secure and privacy-preserving crowd-sensing scheme for fog-enabled vehicular computing. The proposed architecture is made by a double layer of fog nodes that is used to generate crowd-sensing tasks for vehicles, then collect, aggregate and analyze the data based on user specifications. To ensure data confidentiality and fined-grained access control, we make use of ciphertext-policy attribute-based encryption with access update policy (CP-ABE-UP), which is a well-known one-to-many encryption technique. The policy update algorithm allows the fog nodes to outsource the crowd-sensing data to other fog nodes or to data users directly. We also adopted the ID-based signature tied to pseudonymous techniques to guarantee the authentication and privacy-preservation of the entities in the system. From the upper fog layer to the data user, we show that an information-centric networking (ICN) approach can be applied to maximize the network resources and enhance the security by avoiding unauthorized and unauthenticated data owners. The security analysis confirms that our approach is secure against known attacks, whereas the simulation results show its efficiency in terms of communication with little computational overhead.
Keywords: Privacy preservation; Crowd-sensing; Fog enabled vehicular computing; Access control; ID-based signature; Attribute-based encryption; Information-centric network

Imran Makhdoom, Mehran Abolhasan, Daniel Franklin, Justin Lipman, Christian Zimmermann, Massimo Piccardi, Negin Shariati,
Detecting compromised IoT devices: Existing techniques, challenges, and a way forward,
Computers & Security,
Volume 132,
2023,
103384,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2023.103384.
(https://www.sciencedirect.com/science/article/pii/S0167404823002948)
Abstract: IoT devices, whether connected to the Internet or operating in a private network, are vulnerable to cyber attacks from external or internal attackers or insiders who may succeed in physically compromising an IoT device. Once compromised, the IoT device can join a botnet to participate in large-scale distributed attacks (potentially recruiting additional nodes), exfiltrating confidential data or injecting false data into critical data sets, corrupting subsequent data analytics. Although various device attestation techniques are available to detect malicious IoT devices, these methods do not fully address all aspects of a potentially compromised node. This study explores current state-of-the-art approaches for detecting a malicious/compromised node in the network, highlights related challenges, and proposes a way forward for developing secure and economical attestation protocols.
Keywords: Internet of things; IoT threats; IoT security; Device integrity; Device attestation; Code integrity; Memory attestation

Genghua Yu, Zhigang Chen, Jia Wu, Jian Wu,
Predicted encounter probability based on dynamic programming proposed probability algorithm in opportunistic social network,
Computer Networks,
Volume 181,
2020,
107465,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107465.
(https://www.sciencedirect.com/science/article/pii/S1389128620311440)
Abstract: With the development of mobile communication technology, the number of mobile terminal users keeps increasing. Human users mostly carry mobile devices. Users can use mobile devices for data transmission and data sharing across spaces. However, mobile users are highly mobile and have a certain degree of selfishness, and their encounters are random. Traditional probabilistic routing methods are difficult to adapt in the application of social network mobile opportunity transmission since they ignore the nodes' sociality and cooperation. To reduce the impact of uncertain factors on data transmission, we propose a predict the probability method of encounter and forwarding cooperation with node (PNECP), the method predicting mobility probabilities based on the social relationship and forwarding collaboration relationship between mobile users. This method uses mixed-relations matrix decomposition to predict the probability of the user's mobility encounter, taking into account the user's encounter and the ability to collaboratively forward information. Designing the way of information collection and transmission, when the data is sparse, can also give prediction results. Simulation results show that our proposed method has a better performance on the delivery rate and average delay performance compared with other probabilistic transmission methods.
Keywords: Opportunistic social network; Probabilistic routing; Collaboratively forward; Sociality

Maxime Compastié, Rémi Badonnel, Olivier Festor, Ruan He,
From virtualization security issues to cloud protection opportunities: An in-depth analysis of system virtualization models,
Computers & Security,
Volume 97,
2020,
101905,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2020.101905.
(https://www.sciencedirect.com/science/article/pii/S0167404820301814)
Abstract: Virtualization methods and techniques play an important role in the development of cloud infrastructures and their services. They enable the decoupling of virtualized resources from the underlying hardware, and facilitate their sharing amongst multiple users. They contribute to the building of elaborated cloud services that are based on the instantiation and composition of these resources. Different models may support such a virtualization, including virtualization based on type-I and type-II hypervisors, OS-level virtualization, and unikernel virtualization. These virtualization models pose a large variety of security issues, but also offer new opportunities for the protection of cloud services. In this article, we describe and compare these virtualization models, in order to establish a reference architecture of cloud infrastructure. We then analyze the security issues related to these models from the reference architecture, by considering related vulnerabilities and attacks. Finally, we point out different recommendations with respect to the exploitation of these models for supporting cloud protection.
Keywords: Security management; System virtualization; OS-Level virtualization; Cloud infrastructures; Unikernel

Domenico Cotroneo, Luigi De Simone, Roberto Natella,
ThorFI: a Novel Approach for Network Fault Injection as a Service,
Journal of Network and Computer Applications,
Volume 201,
2022,
103334,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103334.
(https://www.sciencedirect.com/science/article/pii/S1084804522000030)
Abstract: In this work, we present a novel fault injection solution (ThorFI) for virtual networks in cloud computing infrastructures. ThorFI is designed to provide non-intrusive fault injection capabilities for a cloud tenant, and to isolate injections from interfering with other tenants on the infrastructure. We present the solution in the context of the OpenStack cloud management platform, and release this implementation as open-source software. Finally, we present two relevant case studies of ThorFI, respectively in an NFV IMS and of a high-availability cloud application. The case studies show that ThorFI can enhance functional tests with fault injection, as in 4%–34% of the test cases the IMS is unable to handle faults; and that despite redundancy in virtual networks, faults in one virtual network segment can propagate to other segments, and can affect the throughput and response time of the cloud application as a whole, by about 3 times in the worst case.
Keywords: Network fault injection; Virtualization; Chaos engineering; OpenStack

Raihan ur Rasool, Hua Wang, Usman Ashraf, Khandakar Ahmed, Zahid Anwar, Wajid Rafique,
A survey of link flooding attacks in software defined network ecosystems,
Journal of Network and Computer Applications,
Volume 172,
2020,
102803,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102803.
(https://www.sciencedirect.com/science/article/pii/S1084804520302757)
Abstract: Link Flooding Attacks (LFA) are a devastating type of stealthy denial of service attack that congests critical network links and can completely isolate the victim's network. In this work, we present a systematic survey of LFA patterns on all the layers of the Software Defined Network (SDN) ecosystem, along with a comparative analysis of mitigation techniques. The paper starts by examining different LFA types, techniques, and behaviors in wired and wireless SDNs. Next, an in-depth analysis of mitigation techniques is presented along with their suitability for each of the SDN variants. Subsequently, the significance of a pattern matching and machine learning-based detection and mitigation approaches as a defense against these attacks is highlighted. The paper also contributes by discussing the vulnerabilities of in-band SDNs against LFA when the interface of the data/control plane is attacked by saturating shared strategic links through stealth flows.
Keywords: Link flooding attacks; SDN attacks; SDN security

Tamas Kiss, Peter Kacsuk, Jozsef Kovacs, Botond Rakoczi, Akos Hajnal, Attila Farkas, Gregoire Gesmier, Gabor Terstyanszky,
MiCADO—Microservice-based Cloud Application-level Dynamic Orchestrator,
Future Generation Computer Systems,
Volume 94,
2019,
Pages 937-946,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2017.09.050.
(https://www.sciencedirect.com/science/article/pii/S0167739X17310506)
Abstract: Various scientific and commercial applications require automated scalability and orchestration on cloud computing resources. However, extending applications with such automated scalability on an individual basis is not feasible. This paper investigates how such automated orchestration can be added to cloud applications without major reengineering of the application code. We suggest a generic architecture for an application level cloud orchestration framework, called MiCADO that supports various application scenarios on multiple heterogeneous federated clouds. Besides the generic architecture description, the paper also presents the first MiCADO reference implementation, and explains how the scalability of the Data Avenue service that is applied for data transfer in WS-PGRADE/gUSE based science gateways, can be improved. Performance evaluation of the implemented scalability based on up and downscaling experiments is presented.
Keywords: Cloud orchestration; Science gateway; Automated scalability; Data Avenue

Mohamed Ahzam Amanullah, Riyaz Ahamed Ariyaluran Habeeb, Fariza Hanum Nasaruddin, Abdullah Gani, Ejaz Ahmed, Abdul Salam Mohamed Nainar, Nazihah Md Akim, Muhammad Imran,
Deep learning and big data technologies for IoT security,
Computer Communications,
Volume 151,
2020,
Pages 495-517,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.01.016.
(https://www.sciencedirect.com/science/article/pii/S0140366419315361)
Abstract: Technology has become inevitable in human life, especially the growth of Internet of Things (IoT), which enables communication and interaction with various devices. However, IoT has been proven to be vulnerable to security breaches. Therefore, it is necessary to develop fool proof solutions by creating new technologies or combining existing technologies to address the security issues. Deep learning, a branch of machine learning has shown promising results in previous studies for detection of security breaches. Additionally, IoT devices generate large volumes, variety, and veracity of data. Thus, when big data technologies are incorporated, higher performance and better data handling can be achieved. Hence, we have conducted a comprehensive survey on state-of-the-art deep learning, IoT security, and big data technologies. Further, a comparative analysis and the relationship among deep learning, IoT security, and big data technologies have also been discussed. Further, we have derived a thematic taxonomy from the comparative analysis of technical studies of the three aforementioned domains. Finally, we have identified and discussed the challenges in incorporating deep learning for IoT security using big data technologies and have provided directions to future researchers on the IoT security aspects.
Keywords: Deep learning; Big data; IoT security

Penghao Sun, Zehua Guo, Julong Lan, Junfei Li, Yuxiang Hu, Thar Baker,
ScaleDRL: A Scalable Deep Reinforcement Learning Approach for Traffic Engineering in SDN with Pinning Control,
Computer Networks,
Volume 190,
2021,
107891,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.107891.
(https://www.sciencedirect.com/science/article/pii/S1389128621000554)
Abstract: As modern communication networks become more complicated and dynamic, designing a good Traffic Engineering (TE) policy becomes difficult due to the complexity of solving the optimal traffic scheduling problem. Traditional methods usually design a fixed model of the network traffic and solve an objective function to get a TE policy, which cannot ensure the solution efficiency. The emerging Deep Reinforcement Learning (DRL) together with the Software-Defined Networking (SDN) technologies provide us with a chance to design a model-free TE scheme through Machine Learning (ML). However, existing DRL-based TE solutions are all faced with a scalability problem, i.e., the solution cannot be applied to large networks. In this paper, we propose to combine the control theory and DRL technology to achieve an efficient network control scheme for TE. The proposed scheme ScaleDRL employs the idea from the pinning control theory to select a subset of links in the network and name them critical links. Based on the traffic distribution information collected by the SDN controller, we use a DRL algorithm to dynamically adjust a set of link weights for the critical links. Through a weighted shortest path algorithm, the forwarding paths of the network flows can be dynamically adjusted using the dynamic link weights. The packet-level simulation shows that ScaleDRL reduces the average end-to-end transmission delay by up to 39% compared to the state-of-the-art DRL-based TE scheme in different network topologies.
Keywords: Deep Reinforcement Learning; Pinning control; Software-Defined Networking; Traffic Engineering

Hassan Jalil Hadi, Yue Cao, Khaleeq Un Nisa, Abdul Majid Jamil, Qiang Ni,
A comprehensive survey on security, privacy issues and emerging defence technologies for UAVs,
Journal of Network and Computer Applications,
Volume 213,
2023,
103607,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2023.103607.
(https://www.sciencedirect.com/science/article/pii/S1084804523000267)
Abstract: In the past two decades, there has been a rapid development in the drone industry known as Unmanned Aerial Vehicles (UAVs). Currently, the use of commercial UAVs has increased a lot due to their affordability, but lack of security implementations has introduced many threats and vulnerabilities in UAVs. In addition, software, and hardware complexity in UAVs also triggers privacy and security issues as well as causes critical challenges for government, industry and academia. Firstly, in this research review, we broadly survey privacy and security issues of UAVs by dividing them into three classes: Software, Hardware and Communication. Particularly, for each class, we systematically survey the common vulnerabilities causing potential attacks to UAVs. Secondly, a review of prevailing threats that are threatening civilian UAVs’ applications is also a part of this survey. Thirdly, a comprehensive discussion of passive and active attacks from adversaries, for compromising privacy and security of UAVs is given as well. Fourthly, we provide detail description of existing mitigation techniques and countermeasures, to protect UAVs. Fifthly, the solution architecture part includes discussion about emerging technologies such as, blockchain usage, machine learning, intrusion detection systems and secure communication protocols. To store all data in transit, blockchain can be used cryptographically and protect it from eavesdropping and tampering. In this research review, key points that highlighted the lessons learned about security and privacy of UAVs are also summarized. Lastly, the survey is concluded by discussing important pitfalls as well as suggestions for future research directions, concerning privacy and security of UAVs.
Keywords: Cyber security and privacy; Vulnerabilities; Adversarial machine learning; Intrusion detection system; Digital forensic; Blockchain

Melchizedek Alipio, Nestor Michael Tiglao, Fawaz Bokhari, Salman Khalid,
TCP incast solutions in data center networks: A classification and survey,
Journal of Network and Computer Applications,
Volume 146,
2019,
102421,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2019.102421.
(https://www.sciencedirect.com/science/article/pii/S1084804519302553)
Abstract: In recent years, Data Centers Networks (DCNs) have been deployed to serve as the backbone to support the extensive variety of services offered through the Internet like social networking, web hosting, and e-commerce. The Transmission Control Protocol (TCP) is at present, the most widely used transmission protocol in DCNs and the Internet. When several senders simultaneously send data to a single receiver in a DCN, congestion occurs at the switches' buffer at the receiver's end. This phenomenon of throughput collapse, particularly in data centers, is termed as the TCP incast problem. In this work, we survey and develop a classification scheme of TCP incast solutions in DCNs. We classify the TCP incast solutions on a multi-level criteria based on the TCP/IP protocol stack either TCP or non-TCP approach from our comprehensive survey. Each level includes sublevel classification such as delay-based congestion control schemes, active queue management based solutions, or probabilistic schemes. We also discuss the performance of each existing solutions in terms of its ability to alleviate the network during incast conditions. Furthermore, we present the strengths and weaknesses of each TCP incast techniques. Finally, we outline the open challenges and issues in mitigating the TCP incast in DCNs. We believe that the result of our survey can serve as a design guide to researchers and engineers in designing future DCN protocols.
Keywords: Active queue management; Congestion control; Cloud computing; Data center networks; TCP incast

Luca Caviglione, Wojciech Mazurczyk, Matteo Repetto, Andreas Schaffhauser, Marco Zuppelli,
Kernel-level tracing for detecting stegomalware and covert channels in Linux environments,
Computer Networks,
Volume 191,
2021,
108010,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108010.
(https://www.sciencedirect.com/science/article/pii/S1389128621001249)
Abstract: Modern malware is becoming hard to spot since attackers are increasingly adopting new techniques to elude signature- and rule-based detection mechanisms. Among the others, steganography and information hiding can be used to bypass security frameworks searching for suspicious communications between processes or exfiltration attempts through covert channels. Since the array of potential carriers is very large (e.g., information can be hidden in hardware resources, various multimedia files or network flows), detecting this class of threats is a scarcely generalizable process and gathering multiple behavioral information is time-consuming, lacks scalability, and could lead to performance degradation. In this paper, we leverage the extended Berkeley Packet Filter (eBPF), which is a recent code augmentation feature provided by the Linux kernel, for programmatically tracing and monitoring the behavior of software processes in a very efficient way. To prove the flexibility of the approach, we investigate two realistic use cases implementing different attack mechanisms, i.e., two processes colluding via the alteration of the file system and hidden network communication attempts nested within IPv6 traffic flows. Our results show that even simple eBPF programs can provide useful data for the detection of anomalies, with a minimal overhead. Furthermore, the flexibility to develop and run such programs allows to extract relevant features that could be used for the creation of datasets for feeding security frameworks exploiting AI.
Keywords: eBPF; Syscall and network tracing; Stegomalware; Covert channels; Detection

Avik Bose, Prasun Ghosal,
A low latency energy efficient BFT based 3D NoC design with zone based routing strategy,
Journal of Systems Architecture,
Volume 108,
2020,
101738,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101738.
(https://www.sciencedirect.com/science/article/pii/S1383762120300321)
Abstract: NoC, along with 3D IC technology, successfully addresses communication needs in complex many-core systems today. Major challenges are scalability, network efficiency, power consumption, and energy dissipation. Topology along with an efficient routing strategy, can mitigate such issues. This work proposes a scalable 3D BFT based design along with a zone-based routing policy. 12–76% minimum latency improvement has been observed compared to the state-of-the-art across eight different traffic patterns under heavy traffic. An average gain of 22–88% in router power consumption prevents traffic hot spots achieving 4–32% throughput improvement.
Keywords: Network on chip; 3D NoC; Uniform hopping distance; Zone-based routing; Low network latency

Zohaib Latif, Kashif Sharif, Fan Li, Md Monjurul Karim, Sujit Biswas, Yu Wang,
A comprehensive survey of interface protocols for software defined networks,
Journal of Network and Computer Applications,
Volume 156,
2020,
102563,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102563.
(https://www.sciencedirect.com/science/article/pii/S1084804520300370)
Abstract: Software Defined Network implementation has seen tremendous growth and deployment in different types of networks. Compared to traditional networks it decouples the control logic from network layer devices and centralizes it for efficient traffic forwarding and flow management across the domain. This multi-layered architecture has data forwarding devices at the bottom in the data plane, which is programmed by controllers in the control plane. The high-level management plane interacts with the control plane to program the whole network and enforce different policies. The interaction among these planes is done through interfaces that work as communication/programming protocols. In this survey, we present a comprehensive study of these interface and programming protocols, which are primarily classified into southbound, northbound, and east/westbound interfaces. This work first classifies each of them into subcategories and then presents a comprehensive comparative analysis. As the different interfaces have different properties, hence, the sub-classification and their analysis are done using different properties. In addition, we also discuss the impact of different virtualization techniques, such as hypervisors, on interface protocols and inter-plane communication. More over specialized interfaces for emerging technologies such as the Internet of Things and wireless sensor networks are also presented. Finally, the paper highlights several short term and long term research challenges and open issues specific to the SDN interface protocols.
Keywords: Software defined networks; SDN interfaces; Southbound interface; Northbound interface; East/westbound interface

Everton de Matos, Ramão Tiago Tiburski, Carlos Roberto Moratelli, Sergio Johann Filho, Leonardo Albernaz Amaral, Gowri Ramachandran, Bhaskar Krishnamachari, Fabiano Hessel,
Context information sharing for the Internet of Things: A survey,
Computer Networks,
Volume 166,
2020,
106988,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2019.106988.
(https://www.sciencedirect.com/science/article/pii/S1389128619310400)
Abstract: Internet of Things (IoT) technology is starting to make an impact in a wide array of applications, including smart cities and industrial environments. Such real-world applications combine computation, communication, sensing, and in some cases, actuation, to monitor and remotely control the environment. Data is at the core of such real-world IoT applications. Analysis, modeling, and reasoning of data are necessary to gain valuable insights. Application developers employ context-aware systems to translate the data into contextual information, which then allows the applications to act cognitively. Context sharing platforms offer a solution to distribute context information to those who may be interested in it, thus enabling context interoperability among different entities. This survey first examines the requirements for sharing context information. It then reviews the relevant literature for context sharing and classifies them based on their requirements and characteristics. Challenges and future directions are presented to encourage the development of context sharing platforms.
Keywords: Context sharing; Context-awareness; Internet of Things; Context platform

Fotis Savva, Christos Anagnostopoulos, Peter Triantafillou,
Adaptive learning of aggregate analytics under dynamic workloads,
Future Generation Computer Systems,
Volume 109,
2020,
Pages 317-330,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2020.03.063.
(https://www.sciencedirect.com/science/article/pii/S0167739X19329504)
Abstract: Large organizations have seamlessly incorporated data-driven decision making in their operations. However, as data volumes increase, expensive big data infrastructures are called to rescue. In this setting, analytics tasks become very costly in terms of query response time, resource consumption, and money in cloud deployments, especially when base data are stored across geographically distributed data centers. Therefore, we introduce an adaptive, reciprocity-based Machine Learning mechanism which is light-weight, stored client-side, can estimate the answers of a variety of aggregate queries and can avoid the big data back-end. The estimations are performed in milliseconds are inexpensive and accurate as the mechanism learns from past analytical-query patterns. However, as analytic queries are ad hoc and analysts’ interests change over time we develop solutions that can swiftly and accurately detect such changes and adapt to new query patterns. The capabilities of our approach are demonstrated using extensive evaluation with real and synthetic datasets.
Keywords: Life-long learning; Approximate query processing; Machine learning; Concept drift detection; Change-point detection

Leena Mary Francis, N. Sreenath,
Live detection of text in the natural environment using Convolutional Neural Network,
Future Generation Computer Systems,
Volume 98,
2019,
Pages 444-455,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.03.054.
(https://www.sciencedirect.com/science/article/pii/S0167739X18322180)
Abstract: With the exponential growth in the quantity of born-digital images, the problem of comprehending text from natural scene images has acquired greater significance. This paper proposes a deep-learning based approach, to detect the presence of text in the natural scene images. The proposed approach is built with the capability to distinguish text and non-text images from the live-stream of the smart-phone camera, thereby eliminating the need for capturing the image, to locate the presence of text. A streamlined Convolutional Neural Network (CNN) MobileNet is harnessed for the process of distinguishing text images from non-text images. The proposed approach can be adopted as a filter, to decide whether to permit the image further down the processing pipeline for the text detection task, which in turn leads to the reduction in false-positives and false-negatives by not processing an image which does not have text. It was inferred from the experimental results that the width multiplier value of 0.75 and resolution multiplier of 224 yields the accuracy of 99.31% in classifying the text images from non-text images.
Keywords: Image classification; Scene-text image; Deep learning

Gonzalo Munilla Garrido, Johannes Sedlmeir, Ömer Uludağ, Ilias Soto Alaoui, Andre Luckow, Florian Matthes,
Revealing the landscape of privacy-enhancing technologies in the context of data markets for the IoT: A systematic literature review,
Journal of Network and Computer Applications,
Volume 207,
2022,
103465,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2022.103465.
(https://www.sciencedirect.com/science/article/pii/S1084804522001126)
Abstract: IoT data markets in public and private institutions have become increasingly relevant in recent years because of their potential to improve data availability and unlock new business models. However, exchanging data in markets bears considerable challenges related to disclosing sensitive information. Despite considerable research focused on different aspects of privacy-enhancing data markets for the IoT, none of the solutions proposed so far seems to find a practical adoption. Thus, this study aims to organize the state-of-the-art solutions, analyze and scope the technologies that have been suggested in this context, and structure the remaining challenges to determine areas where future research is required. To accomplish this goal, we conducted a systematic literature review on privacy enhancement in data markets for the IoT, covering 50 publications dated up to July 2020, and provided updates with 24 publications dated up to May 2022. Our results indicate that most research in this area has emerged only recently, and no IoT data market architecture has established itself as canonical. Existing solutions frequently lack the required combination of anonymization and secure computation technologies. Furthermore, there is no consensus on the appropriate use of blockchain technology for IoT data markets and a low degree of leveraging existing libraries or reusing generic data market architectures. We also identified significant challenges remaining, such as the copy problem and the recursive enforcement problem that – while solutions have been suggested to some extent – are often not sufficiently addressed in proposed designs. We conclude that privacy-enhancing technologies need further improvements to positively impact data markets so that, ultimately, the value of data is preserved through data scarcity and users’ privacy and businesses-critical information are protected.
Keywords: Anonymization; Big data; Copy problem; Data exchange; Marketplace; Platform; Secure computation

Yunhe Cui, Qing Qian, Chun Guo, Guowei Shen, Youliang Tian, Huanlai Xing, Lianshan Yan,
Towards DDoS detection mechanisms in Software-Defined Networking,
Journal of Network and Computer Applications,
Volume 190,
2021,
103156,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2021.103156.
(https://www.sciencedirect.com/science/article/pii/S1084804521001703)
Abstract: Software-Defined Networking (SDN) is widely considered as one of the next generation network architecture. However, SDN faces with a series of issues which restraint its development and application, where the security is one of the serious issues. The Distributed Denial of Service (DDoS) is such a devastating security problem. In this work, a comprehensive review of the DDoS detection mechanisms utilized in SDN is presented. DDoS attacks in SDN are classified into two types and five subtypes based on the features of DDoS and SDN. For each kind of DDoS, how the attackers can exploit the vulnerabilities of SDN to launch DDoS attacks is discussed. Subsequently, the DDoS detection mechanisms used in SDN are reviewed and categorized into five types and forty-six subtypes. These kinds of DDoS detection mechanisms are compared and analyzed, where we draw a conclusion that the machine learning-based DDoS detection mechanisms and threshold-based DDoS detection mechanisms are the two most popular technologies utilized to detect DDoS attacks in SDN. More importantly, for each kind of DDoS detection mechanism, the generational process, advantages, and disadvantages are discussed. Additionally, the open problems and future directions of DDoS detection in SDN are discussed. By presenting these review, discussion and analysis, we hope it can facilitate the understanding of DDoS detection in SDN.
Keywords: Distributed Denial of Service; Attack detection; Software-Defined Networking; OpenFlow

Rakesh Kumar, Rinkaj Goyal,
On cloud security requirements, threats, vulnerabilities and countermeasures: A survey,
Computer Science Review,
Volume 33,
2019,
Pages 1-48,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2019.05.002.
(https://www.sciencedirect.com/science/article/pii/S1574013718302065)
Abstract: The world is witnessing a phenomenal growth in the cloud enabled services and is expected to grow further with the improved technological innovations. However, the associated security and privacy challenges inhibit its widespread adoption, and therefore require further exploration. Researchers from academia, industry, and standards organizations have provided potential solutions to these challenges in the previously published studies. The narrative review presented in this survey, however, provides an integrationist end-to-end mapping of cloud security requirements, identified threats, known vulnerabilities, and recommended countermeasures, which seems to be not presented before at one place. Additionally, this study contributes towards identifying a unified taxonomy for security requirements, threats, vulnerabilities and countermeasures to carry out the proposed end-to-end mapping. Further, it highlights security challenges in other related areas like trust based security models, cloud-enabled applications of Big Data, Internet of Things (IoT), Software Defined Network (SDN) and Network Function Virtualization (NFV).
Keywords: Cloud computing; Security in cloud; Cloud security trust model; Cloud security challenges; Cloud security requirements threats vulnerabilities countermeasures

Aliyu Lawal Aliyu, Adel Aneiba, Mohammad Patwary, Peter Bull,
A trust management framework for Software Defined Network (SDN) controller and network applications,
Computer Networks,
Volume 181,
2020,
107421,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2020.107421.
(https://www.sciencedirect.com/science/article/pii/S1389128620311105)
Abstract: The use of network applications to manage network operations by the controller in SDN architecture introduces a threat that makes the controller to be susceptible to several network attacks. This is possible because the network applications operate without any access control mechanism that authenticates or dictates what operations they can execute in the network. This consequently makes the network applications to take advantage of their ability to manipulate, change or modify network state to compromise network operations and resources. In order to address this problem this paper introduces a token-based authentication method that enables the controller to authenticate the various network applications. The application of this method builds an access permission zone where only legitimate network applications with the correct token credentials can have access to the network prior to implementing any network changes. This paper contributes in providing an authorisation method Boolean Access Matrix that enforces permission constraints on what the network applications can access or execute within the network. The authorisation method helps limits the unprecedented access the network applications have over the control layer resources, core services and the network operations. The paper introduces a novel method of evaluating the trust between the controller and the network application based on Subjective Logic Reasoning (SLR) which is a belief learning model. SLR is an advanced learning algorithm that is derived from Probability Calculus and Statistics. Experiments demonstrate the efficiency and scalability of the proposed algorithms in a large scale test environment.
Keywords: SDN; Trust; Authentication; Authorisation; Security

Vaishnavi Moorthy, Revathi Venkataraman, T. Rama Rao,
Security and privacy attacks during data communication in Software Defined Mobile Clouds,
Computer Communications,
Volume 153,
2020,
Pages 515-526,
ISSN 0140-3664,
https://doi.org/10.1016/j.comcom.2020.02.030.
(https://www.sciencedirect.com/science/article/pii/S0140366419317268)
Abstract: There has been an enormous growth in networking and data communication between the wireless devices. Cloud computation has taken a giant leap into the mobile domain to cater to the on-going communication demands. The latest programming techniques have also paved the way for incorporating Software Defined Networking techniques in Mobile Cloud environment. The growth of networking infrastructure with many such technological innovations, has led to the parallel increase in security and privacy of data at different levels of communication. The communication links and interfaces have become targeted platform for various security attacks causing denial of services. This survey comprehends an in-depth analysis of the possible data privacy attacks and threats in the proposed Software Defined Mobile Cloud. The possibilities of the attacks which are enlisted focus on various ways of spoofing and flooding of data packets, mis-interpretation and aggregation of the flow rules for communication, packet dropping and traffic flow enrouting at different cross-layered architectural levels built over the heterogeneous networks. An overview of the mechanisms to encounter the various vulnerabilities in the suggested framework is recommended. This article further identifies research directions and challenges to incorporate the possible techniques and preventive solutions, to overcome the malicious attacks in the software defined mobile cloud.
Keywords: Data communication; Software Defined Mobile Cloud; Data privacy; Security; Malicious attacks; Preventive solutions

Shunmei Meng, Lianyong Qi, Qianmu Li, Wenmin Lin, Xiaolong Xu, Shaohua Wan,
Privacy-preserving and sparsity-aware location-based prediction method for collaborative recommender systems,
Future Generation Computer Systems,
Volume 96,
2019,
Pages 324-335,
ISSN 0167-739X,
https://doi.org/10.1016/j.future.2019.02.016.
(https://www.sciencedirect.com/science/article/pii/S0167739X18318053)
Abstract: With the rapid growth of public cloud offerings, how to design effective prediction models that provide appropriate recommendations for potential users has become more and more important. In dynamic cloud environment, both of user behaviors and service performance are sensitive to contextual information, such as geographic location information. In addition, the increasing number of attacks and security threats also brought the problem that how to protect critical information assets such as sensitive data, cloud resources and communication in a more effective and secure manner. In view of these challenges, we propose a privacy-preserving and sparsity-aware location-based prediction method for collaborative recommender systems. Specifically, our method is designed as a three-phase process: Firstly, two privacy-preserving mechanisms, i.e., a randomized data obfuscation technique and a region aggregation strategy are presented to protect the private information of users and deal with the data sparsity problem. Then a location-aware latent factor model based on tensor factorization is applied to explore the spatial similarity relationships between services. Finally, predictions are made based on both global and spatial nearest neighbors. Experiments are designed and conducted to validate the effectiveness of our proposal. The experimental results show that our method achieves decent prediction accuracy on the premise of privacy preservation.
Keywords: Location-aware recommendation; Privacy-preserving; Data sparsity; Tensor factorization

Shantanu Pal, Ambrose Hill, Tahiry Rabehaja, Michael Hitchens,
A blockchain-based trust management framework with verifiable interactions,
Computer Networks,
Volume 200,
2021,
108506,
ISSN 1389-1286,
https://doi.org/10.1016/j.comnet.2021.108506.
(https://www.sciencedirect.com/science/article/pii/S1389128621004436)
Abstract: There has been tremendous interest in the development of formal trust models and metrics through the use of analytics (e.g., Belief Theory and Bayesian models), logics (e.g., Epistemic and Subjective Logic) and other mathematical models. The choice of trust metric will depend on context, circumstance and user requirements and there is no single best metric for use in all circumstances. Where different users require different trust metrics to be employed the trust score calculations should still be based on all available trust evidence. Trust is normally computed using past experiences but, in practice (especially in centralised systems), the validity and accuracy of these experiences are taken for granted. In this paper, we provide a formal framework and practical blockchain-based implementation that allows independent trust providers to implement different trust metrics in a distributed manner while still allowing all trust providers to base their calculations on a common set of trust evidence. Further, our design allows experiences to be provably linked to interactions without the need for a central authority. This leads to the notion of evidence-based trust with provable interactions. Leveraging blockchain allows the trust providers to offer their services in a competitive manner, charging fees while users are provided with payments for recording experiences. Performance details of the blockchain implementation are provided.
Keywords: Trust management; Trust modelling; Evidence-based trust; Verifiable interaction; Blockchain; Security; Internet of things

Olivier van der Toorn, Moritz Müller, Sara Dickinson, Cristian Hesselman, Anna Sperotto, Roland van Rijswijk-Deij,
Addressing the challenges of modern DNS a comprehensive tutorial,
Computer Science Review,
Volume 45,
2022,
100469,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2022.100469.
(https://www.sciencedirect.com/science/article/pii/S1574013722000132)
Abstract: The Domain Name System (DNS) plays a crucial role in connecting services and users on the Internet. Since its first specification, DNS has been extended in numerous documents to keep it fit for today’s challenges and demands. And these challenges are many. Revelations of snooping on DNS traffic led to changes to guarantee confidentiality of DNS queries. Attacks to forge DNS traffic led to changes to shore up the integrity of the DNS. Finally, denial-of-service attack on DNS operations have led to new DNS operations architectures. All of these developments make DNS a highly interesting, but also highly challenging research topic. This tutorial – aimed at graduate students and early-career researchers – provides a overview of the modern DNS, its ongoing development and its open challenges. This tutorial has four major contributions. We first provide a comprehensive overview of the DNS protocol. Then, we explain how DNS is deployed in practice. This lays the foundation for the third contribution: a review of the biggest challenges the modern DNS faces today and how they can be addressed. These challenges are (i) protecting the confidentiality and (ii) guaranteeing the integrity of the information provided in the DNS, (iii) ensuring the availability of the DNS infrastructure, and (iv) detecting and preventing attacks that make use of the DNS. Last, we discuss which challenges remain open, pointing the reader towards new research areas.
Keywords: DNS; DNSSEC; Security; Availability; Internet abuse

Imran Makhdoom, Mehran Abolhasan, Justin Lipman,
A comprehensive survey of covert communication techniques, limitations and future challenges,
Computers & Security,
Volume 120,
2022,
102784,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2022.102784.
(https://www.sciencedirect.com/science/article/pii/S0167404822001791)
Abstract: Data encryption aims to protect the confidentiality of data at storage, during transmission, or while in processing. However, it is not always the optimum choice as attackers know the existence of the ciphertext. Hence, they can exploit various weaknesses in the implementation of encryption algorithms and can thus decrypt or guess the related cryptographic primitives. Moreover, in the case of proprietary applications such as online social networks, users are at the mercy of the vendor’s security measures. Therefore, users are vulnerable to various security and privacy threats. Contrary to this, covert communication techniques hide the existence of communication and thus achieve security through obscurity and hidden communication channels. Over the period, there has been a significant advancement in this field. However, existing literature fails to encompass all the aspects of covert communications in a single document. This survey thus endeavors to highlight the latest trends in covert communication techniques, related challenges, and future directions.
Keywords: Covert communication; Hidden messages; Data hiding; Data encapsulation

Alejandro G. Martín, Marta Beltrán, Alberto Fernández-Isabel, Isaac Martín de Diego,
An approach to detect user behaviour anomalies within identity federations,
Computers & Security,
Volume 108,
2021,
102356,
ISSN 0167-4048,
https://doi.org/10.1016/j.cose.2021.102356.
(https://www.sciencedirect.com/science/article/pii/S0167404821001802)
Abstract: User and Entity Behaviour Analytics (UEBA) mechanisms rely on statistical techniques and Machine Learning to determine when a significant deviation from patterns or trends established as a standard for users and entities is occurring. These mechanisms are beneficial within cybersecurity contexts because they allow managers and administrators to have early alerts warning about potential security incidents. This paper proposes the utilisation of UEBA to improve the security of Federated Identity Management (FIM) solutions. The proposed UEBA workflow allows Relying Parties within identity federations to build a session fingerprint characterising each user’s behaviour from available information. Furthermore, it enables anomaly detection based on this fingerprint, integrating raised alerts within current identity management specifications. The proposed workflow is validated and evaluated in a real use case based on a web chat application using OpenID Connect for identity management.
Keywords: Anomaly detection; Behavioural fingerprint; Federated identity management; Machine learning; User and entity behaviour analytics

Salah-Eddine Belouanas, Kim-Loan Thai, Prométhée Spathis, Marcelo Dias de Amorim,
Mobility-assisted offloading in centrally-coordinated cellular networks,
Journal of Network and Computer Applications,
Volume 128,
2019,
Pages 1-10,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2018.11.012.
(https://www.sciencedirect.com/science/article/pii/S1084804518303795)
Abstract: Cellular resources are expensive and should be saved whenever possible. In this paper, we address the problem of content dissemination in a capacity-constrained cellular network, a typical problem that future generations of networks will have to face. We investigate the benefits of device-to-device (D2D) communications with regard to resource savings in cellular networks and evaluate the cost of involving mobile users as relay nodes in the dissemination process. Thanks to their displacements, mobile nodes that already have a copy of the content opportunistically send it to neighboring nodes whenever they meet each other. To motivate the participation of mobile users, we consider a financial model where relay nodes are rewarded with a financial incentive. To conduct our study, we compare various relay node selection strategies including a random strategy and an Oracle strategy. Our results, obtained with a dataset of road traffic, show that the reward needed to secure the ratio of mobile users willing to act as relay nodes may exceed the cost of relying only on cellular transmissions. We evaluate the breaking point where this ratio comes with a higher cost for various network parameters including the range of D2D transmissions and the number of relay nodes.
Keywords: Device-to-device communications; Cellular resources; Delay-tolerant data; Mobile data offloading

Gilbert Tekli,
A survey on semi-structured web data manipulations by non-expert users,
Computer Science Review,
Volume 40,
2021,
100367,
ISSN 1574-0137,
https://doi.org/10.1016/j.cosrev.2021.100367.
(https://www.sciencedirect.com/science/article/pii/S1574013721000071)
Abstract: Since the emergence of web 2.0, data started floating all over the web, through online and offline applications, and across all application domains. Web data (semi-structured data loaded through web browsers and applications communicating via internet protocols such as HTTP), in particular XML-based data, is being used for simple commercial information display (i.e., XHTML/HTML in commercial websites), instant messaging (e.g., XMPP for messaging in Whatsapp, Skype, Gtalk etc.), financial transactions (i.e., CDF3 in ecommerce), medical record processing and storage (e.g., HL7 for electronic medical records), social media (e.g., XHTML/HTML in facebook, LinkedIn, Google Plus, etc.), and others. This phenomenon rendered web data manipulation (i.e., monitoring, modifying, controlling, etc.) by IT (information technology) experts, computer technicians and engineers utterly difficult seeing its exponential growth rate in volume and diversity. Not to mention the dynamicity of the data which is continuously changing on the clock and its heterogeneity (e.g., HTML/HTML5, XML, XHTML, RDF, OWL, etc.). Consequently, the manipulation of web data and in particular XML data (since XML has become one of the most essential data types used in computer communications) has shifted from the hands of computer scientists and programmers towards public computer users in all application domains. This has brought a new criterion into the web data manipulation research field, web data manipulation by non-experts. In this paper, we study and analyze existent techniques for manipulating semi-structured web data, particularly XML data, from a non-expert point of view while relating it to traditional manipulation techniques defined in the literature (i.e., filtering, adaptation, data extraction, transformation, access control, encryption, etc.). Web data manipulation techniques by non-experts were categorized under 3 major titles: (i) XML-oriented visual languages dealing with XML data extraction and transformations, (ii) Mashups tackling mainly XML restructuring with value manipulations, and (iii) Dataflow visual programming languages targeting non-expert manipulations and providing means to visually manipulate scientific data. A full analysis was conducted which allowed existent approaches/techniques to be compared and evaluated providing an overview of the current requirements on this subject.
Keywords: Web data; Semi-structured data; XML; XML manipulation; XML control; Visual languages; Dataflow; Mashups; Visual language

Norberto Garcia, Tomas Alcaniz, Aurora González-Vidal, Jorge Bernal Bernabe, Diego Rivera, Antonio Skarmeta,
Distributed real-time SlowDoS attacks detection over encrypted traffic using Artificial Intelligence,
Journal of Network and Computer Applications,
Volume 173,
2021,
102871,
ISSN 1084-8045,
https://doi.org/10.1016/j.jnca.2020.102871.
(https://www.sciencedirect.com/science/article/pii/S1084804520303362)
Abstract: SlowDoS attacks exploit slow transmissions on application-level protocols like HTTP to carry out denial of service against web-servers. These attacks are difficult to be detected with traditional signature-based intrusion detection approaches, even more when the HTTP traffic is encrypted. To cope with this challenge, this paper describes and AI-based anomaly detection system for real-time detection of SlowDoS attacks over application-level encrypted traffic. Our system monitors in real-time the network traffic, analyzing, processing and aggregating packets into conversation flows, getting valuable features and statistics that are dynamically analyzed in streaming for AI-based anomaly detection. The distributed AI model running in Apache Spark-streaming, combines clustering analysis for anomaly detection, along with deep learning techniques to increase detection accuracy in those cases where clustering obtains ambiguous probabilities. The proposal has been implemented and validated in a real testbed, showing its feasibility, performance and accuracy for detecting in real-time different kinds of SlowDoS attacks over encrypted traffic. The achieved results are close to the optimal precision value with a success rate 98%, while the false negative rate takes a value below 0.5%.
Keywords: Cybersecurity; Artificial intelligence; Cyberattacks; Machine learning

Jagsir Singh, Jaswinder Singh,
A survey on machine learning-based malware detection in executable files,
Journal of Systems Architecture,
Volume 112,
2021,
101861,
ISSN 1383-7621,
https://doi.org/10.1016/j.sysarc.2020.101861.
(https://www.sciencedirect.com/science/article/pii/S1383762120301442)
Abstract: In last decade, a proliferation growth in the development of computer malware has been done. Nowadays, cybercriminals (attacker) use malware as a weapon to carry out the attacks on the computer systems. Internet is the main media to execute the malware attack on the computer systems through emails, malicious websites and by drive and download software. Malicious software can be a virus, trojan horse, worms, rootkits, adware or ransomware. Malware and benign samples are analyzed using static or dynamic analysis techniques. After analysis unique features are extracted to distinguish the malware and benign files. The efficiency of the malware detection system depends on how effectively discriminative malware features are extracted through the analysis techniques. There are various methods to set up the analysis environments using various static and dynamic tools. The second phase is to train the malware classifiers. Earlier traditional methods were used but nowadays machine learning algorithms are used for malware classification which can cope with complexity and pace of malware development. In this paper detailed study of malware detection techniques using machine learning algorithms are presented. In addition, this paper discusses various challenges for developing malware classifiers. At last future directive is discussed to develop an effective malware detection system by handling various issues in malware detection.
Keywords: Dynamic analysis; Machine learning; Malicious software; Obfuscation techniques; Static analysis; API calls; Ransomware
